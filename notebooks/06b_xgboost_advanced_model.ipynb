{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ee6445",
   "metadata": {},
   "source": [
    "# Stage 2: XGBoost Advanced Model\n",
    "\n",
    "**Script**: `scripts\\05_stage2_model_training\\xgboost_models\\03_xgboost_advanced_WITH_AR_FILTER_OPTIMIZED.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "XGBoost model with all features including HMM and DMD.\n",
    "\n",
    "**KEY FEATURES**:\n",
    "- Basic features (ratio + zscore + location)\n",
    "- HMM features (latent states from ratio and zscore)\n",
    "- DMD features (temporal dynamics from ratio and zscore)\n",
    "- AR baseline predictions\n",
    "\n",
    "**HYPOTHESIS**: Advanced temporal features improve AR failure prediction.\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c3d34",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd974f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBoost Pipeline - Phase 3: Model Training WITH_AR_FILTER (HMM+DMD) - OPTIMIZED\n",
    "================================================================================\n",
    "Train XGBoost model with hyperparameter optimization using Stratified Spatial CV.\n",
    "\n",
    "KEY ENHANCEMENT:\n",
    "- GridSearchCV with Stratified Spatial CV for hyperparameter tuning\n",
    "- Same methodology as temporal holdout validation for consistency\n",
    "- Optimizes for AUC-ROC (area under ROC curve)\n",
    "\n",
    "FEATURES:\n",
    "- 9 ratio features (macrocategories)\n",
    "- 9 zscore features (macrocategories)\n",
    "- 6 HMM features (crisis_prob, transition_risk, entropy for ratio/zscore)\n",
    "- 8 DMD features (growth_rate, instability, frequency, amplitude for ratio/zscore)\n",
    "- 3 safe location features\n",
    "- Total: 35 features\n",
    "\n",
    "FILTER: WITH_AR_FILTER (IPC <= 2 AND AR == 0)\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "Date: December 2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                            roc_curve, confusion_matrix, brier_score_loss,\n",
    "                            log_loss, accuracy_score, precision_score,\n",
    "                            recall_score, f1_score)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Add parent directory to path for config import\n",
    "sys.path.append(str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "# Import from config\n",
    "from config import (\n",
    "    BASE_DIR,\n",
    "    STAGE1_DATA_DIR,\n",
    "    STAGE1_RESULTS_DIR,\n",
    "    STAGE2_FEATURES_DIR,\n",
    "    STAGE2_MODELS_DIR,\n",
    "    FIGURES_DIR,\n",
    "    RANDOM_STATE\n",
    ")\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Force unbuffered output\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2f48a",
   "metadata": {},
   "source": [
    "## Load Features and AR Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "N_SPATIAL_CLUSTERS = 20\n",
    "RANDOM_STATE = 42\n",
    "HIGH_RECALL_TARGET = 0.90\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_optimal_thresholds(y_true, y_pred_proba):\n",
    "    \"\"\"Compute optimal thresholds using multiple strategies.\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "\n",
    "    # Youden's J\n",
    "    youden_j = tpr - fpr\n",
    "    youden_idx = np.argmax(youden_j)\n",
    "    youden_threshold = thresholds[youden_idx]\n",
    "    youden_index = youden_j[youden_idx]\n",
    "\n",
    "    # F1-maximizing threshold\n",
    "    f1_scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        if cm.size == 4:\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    f1_idx = np.argmax(f1_scores)\n",
    "    f1_threshold = thresholds[f1_idx]\n",
    "\n",
    "    # High-recall threshold\n",
    "    high_recall_idx = np.where(tpr >= HIGH_RECALL_TARGET)[0]\n",
    "    if len(high_recall_idx) > 0:\n",
    "        high_recall_threshold = thresholds[high_recall_idx[-1]]\n",
    "    else:\n",
    "        high_recall_threshold = thresholds[np.argmax(tpr)]\n",
    "\n",
    "    return {\n",
    "        'youden_threshold': float(youden_threshold),\n",
    "        'youden_index': float(youden_index),\n",
    "        'f1_threshold': float(f1_threshold),\n",
    "        'high_recall_threshold': float(high_recall_threshold)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics_at_threshold(y_true, y_pred_proba, threshold):\n",
    "    \"\"\"Compute comprehensive metrics at a specific threshold.\"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.size == 4:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'tp': int(tp),\n",
    "        'tn': int(tn),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn)\n",
    "    }\n",
    "\n",
    "\n",
    "def assign_confusion_class(y_true, y_pred):\n",
    "    \"\"\"Assign confusion class labels for cartographic mapping.\"\"\"\n",
    "    classes = np.empty(len(y_true), dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd92b0",
   "metadata": {},
   "source": [
    "## Filter to AR Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    classes[(y_true == 0) & (y_pred == 0)] = 'TN'\n",
    "    classes[(y_true == 1) & (y_pred == 1)] = 'TP'\n",
    "    classes[(y_true == 0) & (y_pred == 1)] = 'FP'\n",
    "    classes[(y_true == 1) & (y_pred == 0)] = 'FN'\n",
    "    return classes\n",
    "\n",
    "\n",
    "class SpatialFoldCV:\n",
    "    \"\"\"Custom CV iterator using pre-computed spatial folds.\"\"\"\n",
    "\n",
    "    def __init__(self, fold_array, n_folds=5):\n",
    "        self.fold_array = fold_array\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_folds\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for fold in range(self.n_folds):\n",
    "            train_idx = np.where(self.fold_array != fold)[0]\n",
    "            test_idx = np.where(self.fold_array == fold)[0]\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS\n",
    "# =============================================================================\n",
    "\n",
    "INPUT_FILE = STAGE2_FEATURES_DIR / \"phase3_combined\" / \"combined_advanced_features_h8.csv\"\n",
    "OUTPUT_DIR = STAGE2_MODELS_DIR / \"xgboost\" / \"advanced_with_ar_optimized\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST PIPELINE - OPTIMIZED WITH HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel: xgboost_hmm_dmd_with_ar_optimized\")\n",
    "print(f\"Filter: WITH_AR_FILTER\")\n",
    "print(f\"Enhancement: GridSearchCV with Stratified Spatial CV\")\n",
    "print(f\"Start time: {datetime.now()}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 1: Loading Data\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loaded: {len(df):,} observations\")\n",
    "\n",
    "# =============================================================================\n",
    "# APPLY FILTER\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 2: Applying WITH_AR_FILTER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "initial_rows = len(df)\n",
    "df_filtered = df[(df['ipc_value_filled'] <= 2) & (df['ar_pred_optimal_filled'] == 0)].copy()\n",
    "filtered_out = initial_rows - len(df_filtered)\n",
    "\n",
    "print(f\"After IPC <= 2 filter: {df[df['ipc_value_filled'] <= 2].shape[0]:,} rows\")\n",
    "print(f\"After AR == 0 filter: {len(df_filtered):,} rows\")\n",
    "print(f\"Total filtered out: {filtered_out:,} rows\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE STRATIFIED SPATIAL CV FOLDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214c497",
   "metadata": {},
   "source": [
    "## Stratified Spatial CV Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec872cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 3: Creating Stratified Spatial CV Folds\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "sys.path.append(str(Path(__file__).parent.parent.parent / 'utils'))\n",
    "from stratified_spatial_cv import create_stratified_spatial_folds, create_safe_location_features\n",
    "\n",
    "df_filtered['fold'] = create_stratified_spatial_folds(\n",
    "    df=df_filtered,\n",
    "    district_col='ipc_geographic_unit_full',\n",
    "    country_col='ipc_country',\n",
    "    target_col='ipc_future_crisis',\n",
    "    lat_col='avg_latitude',\n",
    "    lon_col='avg_longitude',\n",
    "    n_folds=N_FOLDS,\n",
    "    n_spatial_clusters=N_SPATIAL_CLUSTERS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE SAFE LOCATION FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 4: Creating Safe Location Features\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_filtered = create_safe_location_features(\n",
    "    df=df_filtered,\n",
    "    district_col='ipc_geographic_unit_full',\n",
    "    country_col='ipc_country',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 5: Preparing Features\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "macro_categories = ['conflict', 'displacement', 'economic', 'food_security', 'governance',\n",
    "                    'health', 'humanitarian', 'other', 'weather']\n",
    "\n",
    "ratio_cols = [f'{cat}_ratio' for cat in macro_categories]\n",
    "zscore_cols = [f'{cat}_zscore' for cat in macro_categories]\n",
    "\n",
    "safe_location_cols = [\n",
    "    'country_baseline_conflict',\n",
    "    'country_baseline_food_security',\n",
    "    'country_data_density'\n",
    "]\n",
    "location_cols = [col for col in safe_location_cols if col in df_filtered.columns]\n",
    "\n",
    "hmm_ratio_cols = [f'hmm_ratio_{feat}' for feat in ['crisis_prob', 'transition_risk', 'entropy']]\n",
    "hmm_zscore_cols = [f'hmm_zscore_{feat}' for feat in ['crisis_prob', 'transition_risk', 'entropy']]\n",
    "\n",
    "dmd_ratio_cols = [f'dmd_ratio_{feat}' for feat in ['crisis_growth_rate', 'crisis_instability',\n",
    "                                                     'crisis_frequency', 'crisis_amplitude']]\n",
    "dmd_zscore_cols = [f'dmd_zscore_{feat}' for feat in ['crisis_growth_rate', 'crisis_instability',\n",
    "                                                       'crisis_frequency', 'crisis_amplitude']]\n",
    "\n",
    "feature_cols = ratio_cols + zscore_cols + location_cols + hmm_ratio_cols + hmm_zscore_cols + dmd_ratio_cols + dmd_zscore_cols\n",
    "feature_cols = [f for f in feature_cols if f in df_filtered.columns]\n",
    "\n",
    "print(f\"Ratio features: {len([f for f in feature_cols if f in ratio_cols])}\")\n",
    "print(f\"Zscore features: {len([f for f in feature_cols if f in zscore_cols])}\")\n",
    "print(f\"HMM features: {len([f for f in feature_cols if 'hmm' in f])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f970b6",
   "metadata": {},
   "source": [
    "## XGBoost Training with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DMD features: {len([f for f in feature_cols if 'dmd' in f])}\")\n",
    "print(f\"Location features: {len(location_cols)}\")\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "df_filtered = df_filtered[df_filtered['ipc_future_crisis'].notna()].copy()\n",
    "print(f\"After removing missing target: {len(df_filtered):,} rows\")\n",
    "\n",
    "n_positive = int(df_filtered['ipc_future_crisis'].sum())\n",
    "n_negative = len(df_filtered) - n_positive\n",
    "crisis_rate = 100 * n_positive / len(df_filtered)\n",
    "\n",
    "print(f\"Crisis events: {n_positive:,} ({crisis_rate:.1f}%)\")\n",
    "print(f\"Non-crisis events: {n_negative:,} ({100-crisis_rate:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {n_negative/n_positive:.2f}:1\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5A: COUNTRY-MEDIAN IMPUTATION FOR HMM/DMD FEATURES\n",
    "# =============================================================================\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 5A: Country-Median Imputation for HMM/DMD Features\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Country-median imputation for HMM and DMD features (instead of fillna(0))\n",
    "print(\"\\nImputing missing HMM/DMD values with country medians...\")\n",
    "for col in hmm_ratio_cols + hmm_zscore_cols + dmd_ratio_cols + dmd_zscore_cols:\n",
    "    if col in df_filtered.columns:\n",
    "        missing_count = df_filtered[col].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            # Compute country-level median\n",
    "            country_medians = df_filtered.groupby('ipc_country')[col].transform('median')\n",
    "            # Fill missing with country median, then global median if country has no data\n",
    "            df_filtered[col] = df_filtered[col].fillna(country_medians).fillna(df_filtered[col].median())\n",
    "            print(f\"  {col}: Imputed {missing_count} missing values with country median\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: HYPERPARAMETER OPTIMIZATION WITH SPATIAL CV\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 6: Hyperparameter Optimization with Stratified Spatial CV\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "X = df_filtered[feature_cols].values  # Convert to numpy array for indexing\n",
    "y = df_filtered['ipc_future_crisis'].values\n",
    "fold_array = df_filtered['fold'].values\n",
    "\n",
    "spatial_cv = SpatialFoldCV(fold_array, n_folds=N_FOLDS)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6B: HYPERPARAMETER TUNING WITH GRIDSEARCHCV\n",
    "# =============================================================================\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 6B: Hyperparameter Optimization with GridSearchCV\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Define hyperparameter grid - BEST CONFIGURATION (AUC 0.6849, PR-AUC 0.1913)\n",
    "# After testing multiple grid sizes, this configuration achieved best discrimination\n",
    "# Hyperparameter grid - FAIR COMPARISON (same as ablation models)\n",
    "# Key changes for fair comparison across feature set sizes:\n",
    "# - max_depth expanded upward (5-10) to allow complex models to utilize more features\n",
    "# - min_child_weight expanded downward (1-5) for finer splits with more features\n",
    "# - Total combinations: 2×3×3×3×2×2×3×2×2 = 2,592\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],           # 3 options\n",
    "    'max_depth': [5, 7, 10],                   # 3 options (expanded upward from 3,5,7)\n",
    "    'learning_rate': [0.01, 0.05, 0.1],        # 3 options\n",
    "    'min_child_weight': [1, 3, 5],             # 3 options (expanded downward from 3,5,10)\n",
    "    'subsample': [0.7, 0.8],                   # 2 options\n",
    "    'colsample_bytree': [0.6, 0.8],            # 2 options\n",
    "    'gamma': [0, 0.5, 1],                      # 3 options\n",
    "    'reg_alpha': [0, 0.1],                     # 2 options\n",
    "    'reg_lambda': [1, 2]                       # 2 options\n",
    "}\n",
    "\n",
    "n_combinations = (len(param_grid['n_estimators']) * len(param_grid['max_depth']) *\n",
    "                  len(param_grid['learning_rate']) * len(param_grid['min_child_weight']) *\n",
    "                  len(param_grid['subsample']) * len(param_grid['colsample_bytree']) *\n",
    "                  len(param_grid['gamma']) * len(param_grid['reg_alpha']) *\n",
    "                  len(param_grid['reg_lambda']))\n",
    "\n",
    "print(f\"Hyperparameter grid: {n_combinations} combinations\")\n",
    "print(\"Running GridSearchCV with Stratified Spatial CV...\")\n",
    "print(\"(This may take several minutes...)\")\n",
    "\n",
    "scale_pos_weight = n_negative / n_positive\n",
    "\n",
    "base_xgb = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=0,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=spatial_cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=8,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV recall: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Save grid search results\n",
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results_df = cv_results_df.sort_values('rank_test_score')\n",
    "cv_results_df.to_csv(OUTPUT_DIR / 'grid_search_results.csv', index=False)\n",
    "print(f\"Grid search results saved\")\n",
    "\n",
    "# =============================================================================\n",
    "# CROSS-VALIDATION WITH OPTIMIZED PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 7: Cross-Validation with Optimized Parameters\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nOptimized XGBoost parameters:\")\n",
    "for key, val in best_params.items():\n",
    "    print(f\"   {key}: {val}\")\n",
    "\n",
    "cv_results = []\n",
    "all_predictions = []\n",
    "feature_importance_list = []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    print(f\"\\nProcessing fold {fold}...\")\n",
    "\n",
    "    train_idx = df_filtered['fold'] != fold\n",
    "    test_idx = df_filtered['fold'] == fold\n",
    "\n",
    "    X_train = df_filtered.loc[train_idx, feature_cols]\n",
    "    y_train = df_filtered.loc[train_idx, 'ipc_future_crisis'].values\n",
    "    X_test = df_filtered.loc[test_idx, feature_cols]\n",
    "    y_test = df_filtered.loc[test_idx, 'ipc_future_crisis'].values\n",
    "\n",
    "    train_crisis = int(y_train.sum())\n",
    "    train_no_crisis = len(y_train) - train_crisis\n",
    "    test_crisis = int(y_test.sum())\n",
    "\n",
    "    print(f\"  Train: {len(X_train):,} ({train_crisis:,} crisis)\")\n",
    "    print(f\"  Test: {len(X_test):,} ({test_crisis:,} crisis)\")\n",
    "\n",
    "    # Calculate fold-specific scale_pos_weight\n",
    "    fold_scale_pos_weight = train_no_crisis / train_crisis if train_crisis > 0 else 1.0\n",
    "\n",
    "    # Create model with optimized params\n",
    "    model = XGBClassifier(\n",
    "        **best_params,\n",
    "        scale_pos_weight=fold_scale_pos_weight,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbosity=0,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "    tree_method='hist'\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save fold model\n",
    "    model_file = OUTPUT_DIR / f\"xgboost_optimized_fold_{fold}.pkl\"\n",
    "    joblib.dump(model, model_file)\n",
    "\n",
    "    # Feature importance\n",
    "    importance_dict = dict(zip(feature_cols, model.feature_importances_))\n",
    "    feature_importance_list.append({'fold': fold, **importance_dict})\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Compute optimal thresholds\n",
    "    threshold_results = compute_optimal_thresholds(y_test, y_pred_proba)\n",
    "    youden_threshold = threshold_results['youden_threshold']\n",
    "    f1_threshold = threshold_results['f1_threshold']\n",
    "    high_recall_threshold = threshold_results['high_recall_threshold']\n",
    "\n",
    "    # Compute metrics at each threshold\n",
    "    youden_metrics = compute_metrics_at_threshold(y_test, y_pred_proba, youden_threshold)\n",
    "    f1_metrics = compute_metrics_at_threshold(y_test, y_pred_proba, f1_threshold)\n",
    "    high_recall_metrics = compute_metrics_at_threshold(y_test, y_pred_proba, high_recall_threshold)\n",
    "\n",
    "    # Calibration metrics\n",
    "    brier_score = brier_score_loss(y_test, y_pred_proba)\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "    print(f\"  AUC: {auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
    "    print(f\"  Youden - P: {youden_metrics['precision']:.3f} | R: {youden_metrics['recall']:.3f} | F1: {youden_metrics['f1']:.3f}\")\n",
    "    print(f\"  F1-max - P: {f1_metrics['precision']:.3f} | R: {f1_metrics['recall']:.3f} | F1: {f1_metrics['f1']:.3f}\")\n",
    "    print(f\"  High-R - P: {high_recall_metrics['precision']:.3f} | R: {high_recall_metrics['recall']:.3f} | F1: {high_recall_metrics['f1']:.3f}\")\n",
    "\n",
    "    # Store CV results\n",
    "    cv_results.append({\n",
    "        'fold': fold,\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test),\n",
    "        'n_crisis_test': test_crisis,\n",
    "        'auc_roc': auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'brier_score': brier_score,\n",
    "        'log_loss': logloss,\n",
    "        'threshold_youden': youden_threshold,\n",
    "        'threshold_f1': f1_threshold,\n",
    "        'threshold_high_recall': high_recall_threshold,\n",
    "        'precision_youden': youden_metrics['precision'],\n",
    "        'recall_youden': youden_metrics['recall'],\n",
    "        'f1_youden': youden_metrics['f1'],\n",
    "        'precision_f1': f1_metrics['precision'],\n",
    "        'recall_f1': f1_metrics['recall'],\n",
    "        'f1_f1': f1_metrics['f1'],\n",
    "        'precision_high_recall': high_recall_metrics['precision'],\n",
    "        'recall_high_recall': high_recall_metrics['recall'],\n",
    "        'f1_high_recall': high_recall_metrics['f1'],\n",
    "        'tp_high_recall': high_recall_metrics['tp'],\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0982b",
   "metadata": {},
   "source": [
    "## Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e69908",
   "metadata": {},
   "outputs": [],
   "source": [
    "        'fn_high_recall': high_recall_metrics['fn'],\n",
    "        'fp_high_recall': high_recall_metrics['fp'],\n",
    "        'tn_high_recall': high_recall_metrics['tn']\n",
    "    })\n",
    "\n",
    "    # Store predictions with metadata\n",
    "    metadata_cols = [\n",
    "        'ipc_geographic_unit_full', 'ipc_district', 'ipc_region',\n",
    "        'ipc_country', 'ipc_country_code',\n",
    "        'avg_latitude', 'avg_longitude',\n",
    "        'year_month', 'ipc_period_start', 'ipc_period_end',\n",
    "        'ipc_value', 'ipc_value_filled', 'ipc_binary_crisis',\n",
    "        'ar_pred_optimal_filled', 'ar_prob_filled'\n",
    "    ]\n",
    "\n",
    "    available_metadata = [col for col in metadata_cols if col in df_filtered.columns]\n",
    "    fold_predictions = df_filtered.loc[test_idx, available_metadata + ['ipc_future_crisis']].copy()\n",
    "\n",
    "    fold_predictions['pred_prob'] = y_pred_proba\n",
    "    fold_predictions['y_pred_youden'] = (y_pred_proba >= youden_threshold).astype(int)\n",
    "    fold_predictions['y_pred_f1'] = (y_pred_proba >= f1_threshold).astype(int)\n",
    "    fold_predictions['y_pred_high_recall'] = (y_pred_proba >= high_recall_threshold).astype(int)\n",
    "    fold_predictions['threshold_youden'] = youden_threshold\n",
    "    fold_predictions['threshold_f1'] = f1_threshold\n",
    "    fold_predictions['threshold_high_recall'] = high_recall_threshold\n",
    "\n",
    "    fold_predictions['confusion_youden'] = assign_confusion_class(\n",
    "        fold_predictions['ipc_future_crisis'].values,\n",
    "        fold_predictions['y_pred_youden'].values\n",
    "    )\n",
    "    fold_predictions['confusion_high_recall'] = assign_confusion_class(\n",
    "        fold_predictions['ipc_future_crisis'].values,\n",
    "        fold_predictions['y_pred_high_recall'].values\n",
    "    )\n",
    "\n",
    "    fold_predictions['model'] = 'xgboost_optimized'\n",
    "    fold_predictions['filter_variant'] = 'WITH_AR_FILTER'\n",
    "    fold_predictions['fold'] = fold\n",
    "\n",
    "    all_predictions.append(fold_predictions)\n",
    "\n",
    "# =============================================================================\n",
    "# AGGREGATE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 8: Aggregate Results\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "overall_auc = cv_df['auc_roc'].mean()\n",
    "std_auc = cv_df['auc_roc'].std()\n",
    "overall_pr_auc = cv_df['pr_auc'].mean()\n",
    "std_pr_auc = cv_df['pr_auc'].std()\n",
    "\n",
    "print(f\"\\nOverall AUC-ROC: {overall_auc:.4f} +/- {std_auc:.4f}\")\n",
    "print(f\"Overall PR-AUC: {overall_pr_auc:.4f} +/- {std_pr_auc:.4f}\")\n",
    "print(f\"Mean Brier Score: {cv_df['brier_score'].mean():.4f}\")\n",
    "print(f\"Mean Log Loss: {cv_df['log_loss'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nYouden Threshold Metrics:\")\n",
    "print(f\"  Precision: {cv_df['precision_youden'].mean():.4f}\")\n",
    "print(f\"  Recall: {cv_df['recall_youden'].mean():.4f}\")\n",
    "print(f\"  F1: {cv_df['f1_youden'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nHigh-Recall Threshold Metrics:\")\n",
    "print(f\"  Precision: {cv_df['precision_high_recall'].mean():.4f}\")\n",
    "print(f\"  Recall: {cv_df['recall_high_recall'].mean():.4f}\")\n",
    "print(f\"  F1: {cv_df['f1_high_recall'].mean():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN FINAL MODEL ON ALL DATA\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 9: Train Final Model on All Data\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    **best_params,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=0,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(X)} samples with {len(feature_cols)} features\")\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Save final model\n",
    "final_model_file = OUTPUT_DIR / \"xgboost_optimized_final.pkl\"\n",
    "joblib.dump(final_model, final_model_file)\n",
    "print(f\"Final model saved: {final_model_file}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 10: Feature Importance\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "importance_df = pd.DataFrame(feature_importance_list).fillna(0)\n",
    "avg_importance = importance_df.drop('fold', axis=1).mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "for i, (feat, score) in enumerate(avg_importance.head(10).items(), 1):\n",
    "    print(f\"  {i}. {feat}: {score:.4f}\")\n",
    "\n",
    "importance_file = OUTPUT_DIR / \"feature_importance.csv\"\n",
    "avg_importance.to_csv(importance_file, header=['importance'])\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 11: Saving Results\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Predictions\n",
    "predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "predictions_file = OUTPUT_DIR / \"xgboost_optimized_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"Predictions saved: {predictions_file.name} ({len(predictions_df):,} rows)\")\n",
    "\n",
    "# CV results\n",
    "cv_file = OUTPUT_DIR / \"xgboost_optimized_cv_results.csv\"\n",
    "cv_df.to_csv(cv_file, index=False)\n",
    "print(f\"CV results saved: {cv_file.name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE COUNTRY-LEVEL METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 12: Computing Country-Level Metrics\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "country_metrics = []\n",
    "\n",
    "for country in predictions_df['ipc_country'].unique():\n",
    "    country_df = predictions_df[predictions_df['ipc_country'] == country].copy()\n",
    "\n",
    "    y_true = country_df['ipc_future_crisis'].values\n",
    "    y_pred_proba = country_df['pred_prob'].values\n",
    "\n",
    "    country_row = {\n",
    "        'country': country,\n",
    "        'n_observations': len(country_df),\n",
    "        'n_crisis': int(y_true.sum()),\n",
    "        'crisis_rate': float(y_true.mean()),\n",
    "        'model': 'xgboost_optimized',\n",
    "        'filter_variant': 'WITH_AR_FILTER'\n",
    "    }\n",
    "\n",
    "    for threshold_type in ['youden', 'f1', 'high_recall']:\n",
    "        pred_col = f'y_pred_{threshold_type}'\n",
    "\n",
    "        if pred_col in country_df.columns:\n",
    "            y_pred = country_df[pred_col].values\n",
    "\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03502b5",
   "metadata": {},
   "source": [
    "## Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93424b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            try:\n",
    "                auc = roc_auc_score(y_true, y_pred_proba) if len(np.unique(y_true)) > 1 else np.nan\n",
    "            except:\n",
    "                auc = np.nan\n",
    "\n",
    "            country_row[f'auc_{threshold_type}'] = auc\n",
    "            country_row[f'precision_{threshold_type}'] = precision\n",
    "            country_row[f'recall_{threshold_type}'] = recall\n",
    "            country_row[f'f1_{threshold_type}'] = f1\n",
    "            country_row[f'tp_{threshold_type}'] = int(tp)\n",
    "            country_row[f'fn_{threshold_type}'] = int(fn)\n",
    "            country_row[f'fp_{threshold_type}'] = int(fp)\n",
    "            country_row[f'tn_{threshold_type}'] = int(tn)\n",
    "\n",
    "    country_metrics.append(country_row)\n",
    "\n",
    "country_metrics_df = pd.DataFrame(country_metrics)\n",
    "country_metrics_file = OUTPUT_DIR / \"country_metrics.csv\"\n",
    "country_metrics_df.to_csv(country_metrics_file, index=False)\n",
    "print(f\"Country metrics saved: {country_metrics_file.name} ({len(country_metrics_df)} countries)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY JSON\n",
    "# =============================================================================\n",
    "\n",
    "summary = {\n",
    "    'model': 'xgboost_advanced_optimized',\n",
    "    'model_type': 'ADVANCED (HMM+DMD) - All Features',\n",
    "    'filter': 'WITH_AR_FILTER',\n",
    "    'enhancement': 'GridSearchCV with Stratified Spatial CV',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "\n",
    "\n",
    "    'hyperparameter_tuning': {\n",
    "        'method': 'GridSearchCV with Stratified Spatial CV',\n",
    "        'scoring': 'recall',\n",
    "        'n_combinations': n_combinations,\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': float(grid_search.best_score_)\n",
    "    },\n",
    "\n",
    "    'data': {\n",
    "        'total_observations': int(len(df_filtered)),\n",
    "        'crisis_events': int(n_positive),\n",
    "        'non_crisis_events': int(n_negative),\n",
    "        'crisis_rate': float(crisis_rate / 100),\n",
    "        'imbalance_ratio': float(n_negative / n_positive),\n",
    "        'countries': int(df_filtered['ipc_country'].nunique()),\n",
    "        'districts': int(df_filtered['ipc_geographic_unit_full'].nunique())\n",
    "    },\n",
    "\n",
    "    'features': {\n",
    "        'ratio': len([f for f in feature_cols if f in ratio_cols]),\n",
    "        'zscore': len([f for f in feature_cols if f in zscore_cols]),\n",
    "        'hmm': len([f for f in feature_cols if 'hmm' in f]),\n",
    "        'dmd': len([f for f in feature_cols if 'dmd' in f]),\n",
    "        'location': len([f for f in feature_cols if f in location_cols]),\n",
    "        'total': len(feature_cols)\n",
    "    },\n",
    "\n",
    "    'cv_performance': {\n",
    "        'auc_roc_mean': float(overall_auc),\n",
    "        'auc_roc_std': float(std_auc),\n",
    "        'pr_auc_mean': float(overall_pr_auc),\n",
    "        'pr_auc_std': float(std_pr_auc),\n",
    "        'brier_score_mean': float(cv_df['brier_score'].mean()),\n",
    "        'log_loss_mean': float(cv_df['log_loss'].mean()),\n",
    "        'youden': {\n",
    "            'precision': float(cv_df['precision_youden'].mean()),\n",
    "            'recall': float(cv_df['recall_youden'].mean()),\n",
    "            'f1': float(cv_df['f1_youden'].mean())\n",
    "        },\n",
    "        'high_recall': {\n",
    "            'precision': float(cv_df['precision_high_recall'].mean()),\n",
    "            'recall': float(cv_df['recall_high_recall'].mean()),\n",
    "            'f1': float(cv_df['f1_high_recall'].mean())\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'top_features': avg_importance.head(10).to_dict()\n",
    "}\n",
    "\n",
    "summary_file = OUTPUT_DIR / \"xgboost_optimized_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Summary saved: {summary_file.name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST ADVANCED OPTIMIZED TRAINING COMPLETE \")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"FEATURES:\")\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "print(f\"  - Ratio: {len([f for f in feature_cols if f in ratio_cols])}, Zscore: {len([f for f in feature_cols if f in zscore_cols])}\")\n",
    "print(f\"  - HMM: {len([f for f in feature_cols if 'hmm' in f])}, DMD: {len([f for f in feature_cols if 'dmd' in f])}\")\n",
    "print(f\"  - Location: {len([f for f in feature_cols if f in location_cols])}\")\n",
    "print()\n",
    "print(\"HYPERPARAMETER OPTIMIZATION:\")\n",
    "print(f\"  Method: GridSearchCV with Stratified Spatial CV\")\n",
    "print(f\"  Combinations tested: {n_combinations}\")\n",
    "print(f\"  Best CV Recall: {grid_search.best_score_:.4f}\")\n",
    "print()\n",
    "print(\"BEST PARAMETERS:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print()\n",
    "print(\"CV PERFORMANCE:\")\n",
    "print(f\"  AUC-ROC: {overall_auc:.4f} +/- {std_auc:.4f}\")\n",
    "print(f\"  PR-AUC: {overall_pr_auc:.4f} +/- {std_pr_auc:.4f}\")\n",
    "print(f\"  High-Recall - P: {cv_df['precision_high_recall'].mean():.3f}, R: {cv_df['recall_high_recall'].mean():.3f}, F1: {cv_df['f1_high_recall'].mean():.3f}\")\n",
    "print()\n",
    "print(\"OUTPUT FILES:\")\n",
    "print(f\"  - xgboost_optimized_predictions.csv\")\n",
    "print(f\"  - xgboost_optimized_cv_results.csv\")\n",
    "print(f\"  - country_metrics.csv\")\n",
    "print(f\"  - feature_importance.csv\")\n",
    "print(f\"  - grid_search_results.csv\")\n",
    "print(f\"  - xgboost_optimized_final.pkl\")\n",
    "print()\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"End time: {datetime.now()}\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
