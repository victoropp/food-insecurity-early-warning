{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# Deduplicate IPC Assessments - District Level\n",
    "\n",
    "**Script**: `scripts/02_data_processing/05_deduplicate.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Aggregates to unique `(ipc_geographic_unit_full, period)` level.\n",
    "\n",
    "**KEY POINT**: Each unique `(geographic_unit_full, period)` is ONE observation. This script handles any duplicates from the GDELT matching process.\n",
    "\n",
    "**Aggregation strategy**:\n",
    "- **Count columns**: Sum (article_count, location_mention_count, etc.)\n",
    "- **Geographic columns**: Weighted mean by location_mention_count\n",
    "- **IPC metadata**: Keep first (should be identical)\n",
    "- **Categorical**: Keep first\n",
    "- **Score**: Max\n",
    "- **Numeric features**: Sum\n",
    "\n",
    "**Runtime**: ~10 minutes\n",
    "\n",
    "**Input**: `data/district_level/ml_dataset_complete.parquet` (from 02d)\n",
    "\n",
    "**Output**: `data/district_level/ml_dataset_deduplicated.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from config import BASE_DIR\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(str(BASE_DIR.parent.parent.parent))\n",
    "\n",
    "# District pipeline I/O (district_level subfolder)\n",
    "DISTRICT_DATA_DIR = BASE_DIR / 'data' / 'district_level'\n",
    "INPUT_FILE = DISTRICT_DATA_DIR / 'ml_dataset_complete.parquet'\n",
    "OUTPUT_FILE = DISTRICT_DATA_DIR / 'ml_dataset_deduplicated.parquet'\n",
    "OUTPUT_CSV = DISTRICT_DATA_DIR / 'ml_dataset_deduplicated.csv'\n",
    "\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-mean-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(group, value_col, weight_col='location_mention_count'):\n",
    "    \"\"\"Compute weighted mean using location_mention_count as weights\"\"\"\n",
    "    if weight_col not in group.columns:\n",
    "        return group[value_col].mean()\n",
    "\n",
    "    weights = group[weight_col]\n",
    "    values = group[value_col]\n",
    "\n",
    "    mask = values.notna() & weights.notna() & (weights > 0)\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return (values[mask] * weights[mask]).sum() / weights[mask].sum()\n",
    "\n",
    "print(\"weighted_mean defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-header",
   "metadata": {},
   "source": [
    "## Main Processing\n",
    "\n",
    "Load data, detect duplicates, and aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"IPC Assessment Deduplication - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Start time: {datetime.now()}\\n\")\n",
    "\n",
    "# Load data\n",
    "print(\"1. Loading ML complete dataset...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "print(f\"   Input rows: {len(df):,}\")\n",
    "print(f\"   Unique ipc_id: {df['ipc_id'].nunique():,}\")\n",
    "print(f\"   Unique ipc_geographic_unit_full: {df['ipc_geographic_unit_full'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "# The unique observation key is (ipc_geographic_unit_full, ipc_period_start)\n",
    "df['observation_key'] = (\n",
    "    df['ipc_geographic_unit_full'].astype(str) + '_' +\n",
    "    df['ipc_period_start'].astype(str)\n",
    ")\n",
    "\n",
    "n_unique = df['observation_key'].nunique()\n",
    "n_duplicates = len(df) - n_unique\n",
    "\n",
    "print(f\"   Unique observations: {n_unique:,}\")\n",
    "print(f\"   Duplicate rows: {n_duplicates:,}\")\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    print(\"\\n   No duplicates found! Saving dataset as-is...\")\n",
    "    df = df.drop('observation_key', axis=1)\n",
    "    df.to_parquet(OUTPUT_FILE, index=False)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"   Saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    print(f\"\\n2. Analyzing duplicates...\")\n",
    "\n",
    "    # Show sample duplicates\n",
    "    dup_counts = df.groupby('observation_key').size()\n",
    "    dup_keys = dup_counts[dup_counts > 1].head(3).index\n",
    "\n",
    "    for key in dup_keys:\n",
    "        rows = df[df['observation_key'] == key]\n",
    "        print(f\"\\n   Duplicate: {key[:60]}...\")\n",
    "        print(f\"   Rows: {len(rows)}\")\n",
    "        print(f\"   IPC values: {rows['ipc_value'].unique()}\")\n",
    "        print(f\"   Match levels: {rows['match_level'].unique() if 'match_level' in rows.columns else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorize-columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    # Categorize columns for aggregation\n",
    "    print(\"\\n3. Categorizing columns for aggregation...\")\n",
    "\n",
    "    all_cols = df.columns.tolist()\n",
    "\n",
    "    # IPC metadata columns (keep first - should be identical)\n",
    "    ipc_meta_cols = [col for col in all_cols if col.startswith('ipc_')]\n",
    "    print(f\"   IPC metadata columns: {len(ipc_meta_cols)}\")\n",
    "\n",
    "    # Geographic columns (weighted mean)\n",
    "    geo_cols = ['avg_latitude', 'avg_longitude', 'latitude_std', 'longitude_std']\n",
    "    geo_cols = [c for c in geo_cols if c in all_cols]\n",
    "    print(f\"   Geographic columns (weighted mean): {len(geo_cols)}\")\n",
    "\n",
    "    # Count columns (sum)\n",
    "    count_cols = [\n",
    "        'article_count', 'location_mention_count', 'unique_location_names',\n",
    "        'unique_cities', 'unique_days', 'unique_sources'\n",
    "    ]\n",
    "    count_cols = [c for c in count_cols if c in all_cols]\n",
    "    print(f\"   Count columns (sum): {len(count_cols)}\")\n",
    "\n",
    "    # Categorical columns (first)\n",
    "    cat_cols = ['primary_gadm2', 'primary_gadm3', 'match_level', 'data_source',\n",
    "                'has_articles', 'has_locations']\n",
    "    cat_cols = [c for c in cat_cols if c in all_cols]\n",
    "    print(f\"   Categorical columns (first): {len(cat_cols)}\")\n",
    "\n",
    "    # Score columns (max)\n",
    "    score_cols = ['match_score']\n",
    "    score_cols = [c for c in score_cols if c in all_cols]\n",
    "\n",
    "    # Date component columns (first)\n",
    "    date_cols = ['year', 'month', 'quarter', 'day']\n",
    "    date_cols = [c for c in date_cols if c in all_cols]\n",
    "\n",
    "    # Numeric feature columns (sum) - everything else numeric\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feature_cols = [\n",
    "        c for c in numeric_cols\n",
    "        if c not in ipc_meta_cols\n",
    "        and c not in geo_cols\n",
    "        and c not in count_cols\n",
    "        and c not in score_cols\n",
    "        and c not in date_cols\n",
    "        and c not in ['observation_key']\n",
    "    ]\n",
    "    print(f\"   Numeric feature columns (sum): {len(numeric_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-agg-dict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    # Build aggregation dictionary\n",
    "    print(\"\\n4. Building aggregation dictionary...\")\n",
    "\n",
    "    agg_dict = {}\n",
    "\n",
    "    # IPC metadata - keep first\n",
    "    for col in ipc_meta_cols:\n",
    "        if col not in ['observation_key']:\n",
    "            agg_dict[col] = 'first'\n",
    "\n",
    "    # Count columns - sum\n",
    "    for col in count_cols:\n",
    "        agg_dict[col] = 'sum'\n",
    "\n",
    "    # Categorical - first\n",
    "    for col in cat_cols:\n",
    "        agg_dict[col] = 'first'\n",
    "\n",
    "    # Score - max\n",
    "    for col in score_cols:\n",
    "        agg_dict[col] = 'max'\n",
    "\n",
    "    # Date components - first\n",
    "    for col in date_cols:\n",
    "        agg_dict[col] = 'first'\n",
    "\n",
    "    # Numeric features - sum\n",
    "    for col in numeric_feature_cols:\n",
    "        agg_dict[col] = 'sum'\n",
    "\n",
    "    print(f\"   Total columns to aggregate: {len(agg_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-weighted",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    # Compute weighted averages for geographic columns\n",
    "    print(\"\\n5. Computing weighted averages for geographic columns...\")\n",
    "\n",
    "    weighted_avg_data = {}\n",
    "\n",
    "    for col in geo_cols:\n",
    "        print(f\"   Computing weighted mean for {col}...\", end='\\r')\n",
    "        weighted_avg_data[col] = df.groupby('observation_key').apply(\n",
    "            lambda g: weighted_mean(g, col),\n",
    "            include_groups=False\n",
    "        )\n",
    "\n",
    "    print(f\"   Computed {len(weighted_avg_data)} weighted averages\" + \" \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    # Perform standard aggregation\n",
    "    print(\"\\n6. Aggregating by observation_key...\")\n",
    "    df_agg = df.groupby('observation_key').agg(agg_dict).reset_index()\n",
    "\n",
    "    # Add weighted averages\n",
    "    print(\"\\n7. Adding weighted averages...\")\n",
    "    for col, values in weighted_avg_data.items():\n",
    "        df_agg[col] = df_agg['observation_key'].map(values)\n",
    "        print(f\"   Added {col}\")\n",
    "\n",
    "    # Drop observation_key\n",
    "    df_agg = df_agg.drop('observation_key', axis=1)\n",
    "\n",
    "    print(\"\\n   Aggregation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "## Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    # Validation\n",
    "    print(\"\\n8. Validation checks...\")\n",
    "\n",
    "    print(f\"\\n   Data reduction:\")\n",
    "    print(f\"      Before: {len(df):,} rows\")\n",
    "    print(f\"      After: {len(df_agg):,} rows\")\n",
    "    print(f\"      Reduction: {len(df) - len(df_agg):,} rows ({(1 - len(df_agg)/len(df))*100:.1f}%)\")\n",
    "\n",
    "    # Verify uniqueness\n",
    "    df_agg['check_key'] = (\n",
    "        df_agg['ipc_geographic_unit_full'].astype(str) + '_' +\n",
    "        df_agg['ipc_period_start'].astype(str)\n",
    "    )\n",
    "    is_unique = df_agg['check_key'].nunique() == len(df_agg)\n",
    "    df_agg = df_agg.drop('check_key', axis=1)\n",
    "\n",
    "    print(f\"\\n   Uniqueness check: {'PASSED' if is_unique else 'FAILED'}\")\n",
    "\n",
    "    # Check IPC values\n",
    "    print(f\"\\n   IPC value summary:\")\n",
    "    print(f\"      Unique ipc_id: {df_agg['ipc_id'].nunique():,}\")\n",
    "    print(f\"      Missing ipc_value: {df_agg['ipc_value'].isna().sum()}\")\n",
    "\n",
    "    print(f\"\\n   Geographic coverage:\")\n",
    "    print(f\"      Unique districts: {df_agg['ipc_district'].nunique():,}\")\n",
    "    print(f\"      Unique geographic_unit_full: {df_agg['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"      Countries: {df_agg['ipc_country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if duplicates exist\n",
    "if n_duplicates > 0:\n",
    "    # Save\n",
    "    print(f\"\\n9. Saving deduplicated dataset...\")\n",
    "    print(f\"   Parquet: {OUTPUT_FILE}\")\n",
    "    df_agg.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(\"   [OK] Parquet saved\")\n",
    "\n",
    "    print(f\"\\n   CSV: {OUTPUT_CSV}\")\n",
    "    df_agg.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(\"   [OK] CSV saved\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Deduplication Complete - DISTRICT LEVEL\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFinal dataset: {len(df_agg):,} unique district-period observations\")\n",
    "    print(f\"\\nMethodology note for dissertation:\")\n",
    "    print(\"'GDELT features were aggregated to unique IPC district-period observations\")\n",
    "    print(\"by summing event counts and computing weighted averages for geographic\")\n",
    "    print(f\"coordinates, reducing from {len(df):,} matched records to {len(df_agg):,}\")\n",
    "    print(\"unique district-period observations.'\")\n",
    "    print(f\"\\nEnd time: {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
