{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49bdb142",
   "metadata": {},
   "source": [
    "# Stage 2: Monthly Location Aggregation\n",
    "\n",
    "**Script**: `scripts\\02_data_processing\\03a_stage2_aggregate_locations_monthly.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Aggregates GDELT location mentions by (district, year-month).\n",
    "\n",
    "Aligns with Stage 2 article aggregation for monthly feature computation.\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41f515",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stage 2: Monthly Location Aggregation for Dynamic News Features\n",
    "Aggregates GDELT location mentions by (district, year-month) using Stage 1 matching logic.\n",
    "\n",
    "KEY DIFFERENCE FROM STAGE 1 (Script 03):\n",
    "- Stage 1: Aggregates within IPC assessment periods (Feb, Jun, Oct - ~3/year)\n",
    "- Stage 2: Aggregates by calendar month (all 12 months)\n",
    "\n",
    "CRITICAL: Uses the SAME district matching logic as Script 03 to ensure alignment\n",
    "with Stage 1 predictions and AR failures.\n",
    "\n",
    "Matching Priority: GADM3 → GADM2 → GADM1 → Country-level\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "import gc\n",
    "import pyarrow.parquet as pq\n",
    "from config import BASE_DIR\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(str(BASE_DIR.parent.parent.parent))\n",
    "\n",
    "# Input files (same as Stage 1)\n",
    "LOCATIONS_FILE = BASE_DIR / 'data' / 'african_gkg_locations_aligned.parquet'\n",
    "\n",
    "# District pipeline I/O\n",
    "DISTRICT_DATA_DIR = BASE_DIR / 'data' / 'district_level'\n",
    "IPC_REF_FILE = DISTRICT_DATA_DIR / 'ipc_reference.parquet'\n",
    "\n",
    "# Stage 2 output\n",
    "STAGE2_DATA_DIR = DISTRICT_DATA_DIR / 'stage2'\n",
    "OUTPUT_PARQUET = STAGE2_DATA_DIR / 'locations_aggregated_monthly.parquet'\n",
    "OUTPUT_CSV = STAGE2_DATA_DIR / 'locations_aggregated_monthly.csv'\n",
    "\n",
    "# Processing parameters\n",
    "CHUNK_SIZE = 500000  # Reduced for 8GB RAM systems\n",
    "FUZZY_THRESHOLD = 80  # Same as Stage 1\n",
    "CONSOLIDATE_EVERY = 20  # Consolidate aggregations every N batches for memory safety\n",
    "\n",
    "# Countries with ONLY national-level IPC data (same as Stage 1)\n",
    "COUNTRY_LEVEL_ONLY = {'AO', 'CG', 'CT', 'LT', 'MR', 'RW', 'TO'}\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for matching with accent removal. (Same as Stage 1)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = str(text).lower().strip()\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    text = ''.join(c if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "    text = ' '.join(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5690b9d",
   "metadata": {},
   "source": [
    "## Matching Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "    return text\n",
    "\n",
    "\n",
    "def find_fuzzy_match(loc_name_norm, ipc_candidates, country_code):\n",
    "    \"\"\"\n",
    "    Find best fuzzy match for location name among IPC district candidates.\n",
    "    Returns: (district_info_list, match_score) or (None, 0)\n",
    "    (Same as Stage 1)\n",
    "    \"\"\"\n",
    "    if not loc_name_norm or not ipc_candidates:\n",
    "        return None, 0\n",
    "\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "\n",
    "    for (fips, district_norm), district_list in ipc_candidates.items():\n",
    "        if fips != country_code:\n",
    "            continue\n",
    "\n",
    "        score = fuzz.ratio(loc_name_norm, district_norm)\n",
    "\n",
    "        if score >= FUZZY_THRESHOLD and score > best_score:\n",
    "            best_score = score\n",
    "            best_match = district_list\n",
    "\n",
    "    return best_match, best_score\n",
    "\n",
    "\n",
    "def build_district_lookup(ipc_ref):\n",
    "    \"\"\"\n",
    "    Build lookup dictionary for district matching.\n",
    "\n",
    "    Returns: combined_lookup (dict) with district metadata (not period-specific)\n",
    "    \"\"\"\n",
    "    print(\"\\n2. Building district lookup dictionary...\", flush=True)\n",
    "\n",
    "    # Get unique districts from IPC reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e8c7f",
   "metadata": {},
   "source": [
    "## District Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    unique_districts = ipc_ref.drop_duplicates(subset=['geographic_unit_full_name'])[\n",
    "        ['country', 'country_code', 'fips_code', 'district', 'region',\n",
    "         'geographic_unit_name', 'geographic_unit_full_name', 'district_normalized',\n",
    "         'full_name_normalized', 'fewsnet_region', 'geographic_group']\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"   Unique districts: {len(unique_districts):,}\", flush=True)\n",
    "\n",
    "    # Build primary lookup: (country_fips, district_normalized) -> district_info\n",
    "    ipc_lookup = defaultdict(list)\n",
    "\n",
    "    for idx, row in unique_districts.iterrows():\n",
    "        if pd.notna(row['fips_code']) and pd.notna(row['district_normalized']):\n",
    "            key = (row['fips_code'], row['district_normalized'])\n",
    "            district_info = {\n",
    "                'ipc_country': row['country'],\n",
    "                'ipc_country_code': row['country_code'],\n",
    "                'ipc_fips_code': row['fips_code'],\n",
    "                'ipc_district': row['district'],\n",
    "                'ipc_region': row['region'],\n",
    "                'ipc_geographic_unit': row['geographic_unit_name'],\n",
    "                'ipc_geographic_unit_full': row['geographic_unit_full_name'],\n",
    "                'ipc_fewsnet_region': row['fewsnet_region'],\n",
    "                'ipc_geographic_group': row['geographic_group'],\n",
    "            }\n",
    "            ipc_lookup[key].append(district_info)\n",
    "\n",
    "    print(f\"   Primary lookup: {len(ipc_lookup):,} (country, district) keys\", flush=True)\n",
    "\n",
    "    # Build word-based lookup from full_name_normalized\n",
    "    print(\"   Building word-based lookup from full_name_normalized...\", flush=True)\n",
    "    word_lookup = defaultdict(list)\n",
    "\n",
    "    for idx, row in unique_districts.iterrows():\n",
    "        if pd.notna(row['fips_code']) and pd.notna(row.get('full_name_normalized')):\n",
    "            country = row['fips_code']\n",
    "            full_name_norm = normalize_text(row['full_name_normalized'])\n",
    "            words = full_name_norm.split()\n",
    "\n",
    "            district_info = {\n",
    "                'ipc_country': row['country'],\n",
    "                'ipc_country_code': row['country_code'],\n",
    "                'ipc_fips_code': row['fips_code'],\n",
    "                'ipc_district': row['district'],\n",
    "                'ipc_region': row['region'],\n",
    "                'ipc_geographic_unit': row['geographic_unit_name'],\n",
    "                'ipc_geographic_unit_full': row['geographic_unit_full_name'],\n",
    "                'ipc_fewsnet_region': row['fewsnet_region'],\n",
    "                'ipc_geographic_group': row['geographic_group'],\n",
    "            }\n",
    "\n",
    "            for word in words:\n",
    "                if len(word) > 2:\n",
    "                    key = (country, word)\n",
    "                    word_lookup[key].append(district_info)\n",
    "\n",
    "    print(f\"   Word-based lookup: {len(word_lookup):,} keys\", flush=True)\n",
    "\n",
    "    # Combine lookups\n",
    "    combined_lookup = defaultdict(list)\n",
    "    for k, v in word_lookup.items():\n",
    "        combined_lookup[k].extend(v)\n",
    "    for k, v in ipc_lookup.items():\n",
    "        combined_lookup[k].extend(v)\n",
    "\n",
    "    print(f\"   Combined lookup: {len(combined_lookup):,} keys\", flush=True)\n",
    "\n",
    "    return combined_lookup, unique_districts\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Stage 2: Monthly Location Aggregation\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Start time: {datetime.now()}\")\n",
    "    print(f\"Fuzzy matching threshold: {FUZZY_THRESHOLD}\")\n",
    "    print(\"\\nKEY DIFFERENCE from Stage 1:\")\n",
    "    print(\"  - Stage 1: Aggregates within IPC assessment periods (Feb, Jun, Oct)\")\n",
    "    print(\"  - Stage 2: Aggregates by calendar month (all 12 months)\")\n",
    "    print(\"  - Uses SAME district matching logic as Stage 1 for alignment\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    STAGE2_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load IPC reference (same as Stage 1)\n",
    "    print(\"\\n1. Loading IPC district reference data...\")\n",
    "    ipc_ref = pd.read_parquet(IPC_REF_FILE)\n",
    "    print(f\"   Loaded {len(ipc_ref):,} IPC periods\")\n",
    "    print(f\"   Unique districts: {ipc_ref['district'].nunique():,}\")\n",
    "    print(f\"   Unique full_names: {ipc_ref['geographic_unit_full_name'].nunique():,}\")\n",
    "\n",
    "    # Build district lookup (same matching logic as Stage 1)\n",
    "    combined_lookup, unique_districts = build_district_lookup(ipc_ref)\n",
    "\n",
    "    # Process locations in chunks\n",
    "    print(\"\\n3. Processing locations with DISTRICT-level matching...\", flush=True)\n",
    "    print(\"   Priority: GADM3 -> GADM2 -> GADM1 -> Country (aligned with Stage 1)\", flush=True)\n",
    "    print(\"   Aggregating by MONTH instead of IPC period\", flush=True)\n",
    "\n",
    "    parquet_file = pq.ParquetFile(LOCATIONS_FILE)\n",
    "\n",
    "    all_aggregations = []\n",
    "    total_processed = 0\n",
    "    match_stats = {\n",
    "        'GADM3_exact': 0, 'GADM3_fuzzy': 0,\n",
    "        'GADM2_exact': 0, 'GADM2_fuzzy': 0,\n",
    "        'GADM1_exact': 0, 'GADM1_fuzzy': 0,\n",
    "        'Country_level': 0,\n",
    "        'no_match': 0\n",
    "    }\n",
    "\n",
    "    for batch_num, batch in enumerate(parquet_file.iter_batches(batch_size=CHUNK_SIZE)):\n",
    "        chunk_start = datetime.now()\n",
    "        locations_chunk = batch.to_pandas()\n",
    "\n",
    "        print(f\"\\n   Batch {batch_num + 1}: Processing {len(locations_chunk):,} locations...\", flush=True)\n",
    "\n",
    "        locations_chunk['date_extracted'] = pd.to_datetime(locations_chunk['date_extracted'])\n",
    "\n",
    "        # KEY CHANGE: Extract year-month from location date\n",
    "        locations_chunk['year_month'] = locations_chunk['date_extracted'].dt.to_period('M')\n",
    "\n",
    "        # Normalize geographic fields (same as Stage 1)\n",
    "        locations_chunk['gadm1_norm'] = locations_chunk['gadm1_name'].apply(normalize_text)\n",
    "        locations_chunk['gadm2_norm'] = locations_chunk['gadm2_name'].apply(normalize_text)\n",
    "        locations_chunk['gadm3_norm'] = locations_chunk['gadm3_name'].apply(normalize_text)\n",
    "\n",
    "        matched_records = []\n",
    "        # KEY CHANGE: Track unique (district, month) combinations\n",
    "        matched_district_months = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf630a",
   "metadata": {},
   "source": [
    "## Monthly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee914562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Group by country for efficiency\n",
    "        for country_code, country_locs in locations_chunk.groupby('african_country_code'):\n",
    "\n",
    "            # ================================================================\n",
    "            # PRIORITY 1: GADM3 Matching (Exact → Fuzzy)\n",
    "            # ================================================================\n",
    "            for gadm3_norm, gadm3_locs in country_locs.groupby('gadm3_norm'):\n",
    "                if not gadm3_norm:\n",
    "                    continue\n",
    "\n",
    "                key = (country_code, gadm3_norm)\n",
    "                matched_district_list = None\n",
    "                match_type = None\n",
    "                match_score = 0\n",
    "\n",
    "                if key in combined_lookup:\n",
    "                    matched_district_list = combined_lookup[key]\n",
    "                    match_type = 'GADM3_exact'\n",
    "                    match_score = 100\n",
    "                else:\n",
    "                    matched_district_list, match_score = find_fuzzy_match(gadm3_norm, combined_lookup, country_code)\n",
    "                    if matched_district_list:\n",
    "                        match_type = 'GADM3_fuzzy'\n",
    "\n",
    "                if matched_district_list:\n",
    "                    for district_info in matched_district_list:\n",
    "                        # KEY CHANGE: Group by year_month instead of IPC period\n",
    "                        for year_month, month_locs in gadm3_locs.groupby('year_month'):\n",
    "                            district_month_key = (district_info['ipc_geographic_unit_full'], str(year_month))\n",
    "                            if district_month_key in matched_district_months:\n",
    "                                continue\n",
    "\n",
    "                            if len(month_locs) > 0:\n",
    "                                agg_data = {\n",
    "                                    'year_month': str(year_month),\n",
    "                                    'location_mention_count': len(month_locs),\n",
    "                                    'unique_location_names': month_locs['location_fullname'].nunique(),\n",
    "                                    'unique_cities': month_locs['city_name'].nunique() if 'city_name' in month_locs.columns else 0,\n",
    "                                    'unique_days': month_locs['date_extracted'].nunique(),\n",
    "                                    'avg_latitude': month_locs['latitude'].mean(),\n",
    "                                    'avg_longitude': month_locs['longitude'].mean(),\n",
    "                                    'latitude_std': month_locs['latitude'].std(),\n",
    "                                    'longitude_std': month_locs['longitude'].std(),\n",
    "                                    'primary_gadm2': month_locs['gadm2_name'].mode()[0] if not month_locs['gadm2_name'].mode().empty else None,\n",
    "                                    'primary_gadm3': month_locs['gadm3_name'].mode()[0] if not month_locs['gadm3_name'].mode().empty else None,\n",
    "                                    'match_level': match_type,\n",
    "                                    'match_score': match_score,\n",
    "                                    **district_info\n",
    "                                }\n",
    "                                matched_records.append(agg_data)\n",
    "                                matched_district_months.add(district_month_key)\n",
    "                                match_stats[match_type] += 1\n",
    "\n",
    "            # ================================================================\n",
    "            # PRIORITY 2: GADM2 Matching (Exact → Fuzzy)\n",
    "            # ================================================================\n",
    "            for gadm2_norm, gadm2_locs in country_locs.groupby('gadm2_norm'):\n",
    "                if not gadm2_norm:\n",
    "                    continue\n",
    "\n",
    "                key = (country_code, gadm2_norm)\n",
    "                matched_district_list = None\n",
    "                match_type = None\n",
    "                match_score = 0\n",
    "\n",
    "                if key in combined_lookup:\n",
    "                    matched_district_list = combined_lookup[key]\n",
    "                    match_type = 'GADM2_exact'\n",
    "                    match_score = 100\n",
    "                else:\n",
    "                    matched_district_list, match_score = find_fuzzy_match(gadm2_norm, combined_lookup, country_code)\n",
    "                    if matched_district_list:\n",
    "                        match_type = 'GADM2_fuzzy'\n",
    "\n",
    "                if matched_district_list:\n",
    "                    for district_info in matched_district_list:\n",
    "                        for year_month, month_locs in gadm2_locs.groupby('year_month'):\n",
    "                            district_month_key = (district_info['ipc_geographic_unit_full'], str(year_month))\n",
    "                            if district_month_key in matched_district_months:\n",
    "                                continue\n",
    "\n",
    "                            if len(month_locs) > 0:\n",
    "                                agg_data = {\n",
    "                                    'year_month': str(year_month),\n",
    "                                    'location_mention_count': len(month_locs),\n",
    "                                    'unique_location_names': month_locs['location_fullname'].nunique(),\n",
    "                                    'unique_cities': month_locs['city_name'].nunique() if 'city_name' in month_locs.columns else 0,\n",
    "                                    'unique_days': month_locs['date_extracted'].nunique(),\n",
    "                                    'avg_latitude': month_locs['latitude'].mean(),\n",
    "                                    'avg_longitude': month_locs['longitude'].mean(),\n",
    "                                    'latitude_std': month_locs['latitude'].std(),\n",
    "                                    'longitude_std': month_locs['longitude'].std(),\n",
    "                                    'primary_gadm2': month_locs['gadm2_name'].mode()[0] if not month_locs['gadm2_name'].mode().empty else None,\n",
    "                                    'primary_gadm3': month_locs['gadm3_name'].mode()[0] if not month_locs['gadm3_name'].mode().empty else None,\n",
    "                                    'match_level': match_type,\n",
    "                                    'match_score': match_score,\n",
    "                                    **district_info\n",
    "                                }\n",
    "                                matched_records.append(agg_data)\n",
    "                                matched_district_months.add(district_month_key)\n",
    "                                match_stats[match_type] += 1\n",
    "\n",
    "            # ================================================================\n",
    "            # PRIORITY 3: GADM1 Matching (State/Region level)\n",
    "            # ================================================================\n",
    "            for gadm1_norm, gadm1_locs in country_locs.groupby('gadm1_norm'):\n",
    "                if not gadm1_norm:\n",
    "                    continue\n",
    "\n",
    "                key = (country_code, gadm1_norm)\n",
    "                matched_district_list = None\n",
    "                match_type = None\n",
    "                match_score = 0\n",
    "\n",
    "                if key in combined_lookup:\n",
    "                    matched_district_list = combined_lookup[key]\n",
    "                    match_type = 'GADM1_exact'\n",
    "                    match_score = 100\n",
    "                else:\n",
    "                    matched_district_list, match_score = find_fuzzy_match(gadm1_norm, combined_lookup, country_code)\n",
    "                    if matched_district_list:\n",
    "                        match_type = 'GADM1_fuzzy'\n",
    "\n",
    "                if matched_district_list:\n",
    "                    for district_info in matched_district_list:\n",
    "                        for year_month, month_locs in gadm1_locs.groupby('year_month'):\n",
    "                            district_month_key = (district_info['ipc_geographic_unit_full'], str(year_month))\n",
    "                            if district_month_key in matched_district_months:\n",
    "                                continue\n",
    "\n",
    "                            if len(month_locs) > 0:\n",
    "                                agg_data = {\n",
    "                                    'year_month': str(year_month),\n",
    "                                    'location_mention_count': len(month_locs),\n",
    "                                    'unique_location_names': month_locs['location_fullname'].nunique(),\n",
    "                                    'unique_cities': month_locs['city_name'].nunique() if 'city_name' in month_locs.columns else 0,\n",
    "                                    'unique_days': month_locs['date_extracted'].nunique(),\n",
    "                                    'avg_latitude': month_locs['latitude'].mean(),\n",
    "                                    'avg_longitude': month_locs['longitude'].mean(),\n",
    "                                    'latitude_std': month_locs['latitude'].std(),\n",
    "                                    'longitude_std': month_locs['longitude'].std(),\n",
    "                                    'primary_gadm2': month_locs['gadm2_name'].mode()[0] if not month_locs['gadm2_name'].mode().empty else None,\n",
    "                                    'primary_gadm3': month_locs['gadm3_name'].mode()[0] if not month_locs['gadm3_name'].mode().empty else None,\n",
    "                                    'match_level': match_type,\n",
    "                                    'match_score': match_score,\n",
    "                                    **district_info\n",
    "                                }\n",
    "                                matched_records.append(agg_data)\n",
    "                                matched_district_months.add(district_month_key)\n",
    "                                match_stats[match_type] += 1\n",
    "\n",
    "            # ================================================================\n",
    "            # PRIORITY 4: Country-level matching\n",
    "            # ================================================================\n",
    "            if country_code in COUNTRY_LEVEL_ONLY:\n",
    "                for (fips, district), district_list in combined_lookup.items():\n",
    "                    if fips == country_code:\n",
    "                        for district_info in district_list:\n",
    "                            for year_month, month_locs in country_locs.groupby('year_month'):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd1e6d",
   "metadata": {},
   "source": [
    "## Main Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                district_month_key = (district_info['ipc_geographic_unit_full'], str(year_month))\n",
    "                                if district_month_key in matched_district_months:\n",
    "                                    continue\n",
    "\n",
    "                                if len(month_locs) > 0:\n",
    "                                    agg_data = {\n",
    "                                        'year_month': str(year_month),\n",
    "                                        'location_mention_count': len(month_locs),\n",
    "                                        'unique_location_names': month_locs['location_fullname'].nunique(),\n",
    "                                        'unique_cities': month_locs['city_name'].nunique() if 'city_name' in month_locs.columns else 0,\n",
    "                                        'unique_days': month_locs['date_extracted'].nunique(),\n",
    "                                        'avg_latitude': month_locs['latitude'].mean(),\n",
    "                                        'avg_longitude': month_locs['longitude'].mean(),\n",
    "                                        'latitude_std': month_locs['latitude'].std(),\n",
    "                                        'longitude_std': month_locs['longitude'].std(),\n",
    "                                        'primary_gadm2': month_locs['gadm2_name'].mode()[0] if not month_locs['gadm2_name'].mode().empty else None,\n",
    "                                        'primary_gadm3': month_locs['gadm3_name'].mode()[0] if not month_locs['gadm3_name'].mode().empty else None,\n",
    "                                        'match_level': 'Country_level',\n",
    "                                        'match_score': 100,\n",
    "                                        **district_info\n",
    "                                    }\n",
    "                                    matched_records.append(agg_data)\n",
    "                                    matched_district_months.add(district_month_key)\n",
    "                                    match_stats['Country_level'] += 1\n",
    "\n",
    "        if matched_records:\n",
    "            chunk_df = pd.DataFrame(matched_records)\n",
    "            all_aggregations.append(chunk_df)\n",
    "\n",
    "        total_processed += len(locations_chunk)\n",
    "        chunk_time = (datetime.now() - chunk_start).total_seconds()\n",
    "\n",
    "        print(f\"      Matched: {len(matched_records):,} district-month aggregations\", flush=True)\n",
    "        print(f\"      Time: {chunk_time:.1f}s\", flush=True)\n",
    "\n",
    "        del locations_chunk\n",
    "        gc.collect()\n",
    "\n",
    "        # PERIODIC CONSOLIDATION: Prevent memory accumulation (memory-safe for 8GB)\n",
    "        if len(all_aggregations) >= CONSOLIDATE_EVERY:\n",
    "            print(f\"      Consolidating {len(all_aggregations)} partial aggregations...\", flush=True)\n",
    "            combined = pd.concat(all_aggregations, ignore_index=True)\n",
    "\n",
    "            # Re-aggregate to reduce memory footprint\n",
    "            consol_group_cols = [\n",
    "                'ipc_country', 'ipc_country_code', 'ipc_fips_code',\n",
    "                'ipc_district', 'ipc_region',\n",
    "                'ipc_geographic_unit', 'ipc_geographic_unit_full',\n",
    "                'ipc_fewsnet_region', 'ipc_geographic_group',\n",
    "                'year_month'\n",
    "            ]\n",
    "            consol_agg_dict = {\n",
    "                'location_mention_count': 'sum',\n",
    "                'unique_location_names': 'sum',\n",
    "                'unique_cities': 'sum',\n",
    "                'unique_days': 'max',\n",
    "                'avg_latitude': 'mean',\n",
    "                'avg_longitude': 'mean',\n",
    "                'latitude_std': 'mean',\n",
    "                'longitude_std': 'mean',\n",
    "                'primary_gadm2': 'first',\n",
    "                'primary_gadm3': 'first',\n",
    "                'match_level': 'first',\n",
    "                'match_score': 'mean'\n",
    "            }\n",
    "            consolidated = combined.groupby(consol_group_cols).agg(consol_agg_dict).reset_index()\n",
    "            all_aggregations = [consolidated]\n",
    "            del combined, consolidated\n",
    "            gc.collect()\n",
    "            print(f\"      Consolidated to {len(all_aggregations[0]):,} aggregated rows\", flush=True)\n",
    "\n",
    "    # Combine all aggregations\n",
    "    print(f\"\\n4. Combining {len(all_aggregations)} chunks...\")\n",
    "    if all_aggregations:\n",
    "        final_df = pd.concat(all_aggregations, ignore_index=True)\n",
    "\n",
    "        # Final aggregation by (district, month) to combine across chunks\n",
    "        group_cols = [\n",
    "            'ipc_country', 'ipc_country_code', 'ipc_fips_code',\n",
    "            'ipc_district', 'ipc_region',\n",
    "            'ipc_geographic_unit', 'ipc_geographic_unit_full',\n",
    "            'ipc_fewsnet_region', 'ipc_geographic_group',\n",
    "            'year_month'\n",
    "        ]\n",
    "\n",
    "        agg_dict = {\n",
    "            'location_mention_count': 'sum',\n",
    "            'unique_location_names': 'sum',\n",
    "            'unique_cities': 'sum',\n",
    "            'unique_days': 'max',\n",
    "            'avg_latitude': 'mean',\n",
    "            'avg_longitude': 'mean',\n",
    "            'latitude_std': 'mean',\n",
    "            'longitude_std': 'mean',\n",
    "            'primary_gadm2': 'first',\n",
    "            'primary_gadm3': 'first',\n",
    "            'match_level': 'first',\n",
    "            'match_score': 'mean'\n",
    "        }\n",
    "\n",
    "        final_agg = final_df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Monthly Aggregation Summary - Stage 2\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nTotal locations processed: {total_processed:,}\")\n",
    "        print(f\"Total monthly aggregations: {len(final_agg):,}\")\n",
    "        print(f\"Unique districts (ipc_geographic_unit_full): {final_agg['ipc_geographic_unit_full'].nunique():,}\")\n",
    "        print(f\"Unique months: {final_agg['year_month'].nunique()}\")\n",
    "        print(f\"Month range: {final_agg['year_month'].min()} to {final_agg['year_month'].max()}\")\n",
    "        print(f\"Countries: {final_agg['ipc_country'].nunique()}\")\n",
    "\n",
    "        print(f\"\\nMatch level distribution:\")\n",
    "        print(final_agg['match_level'].value_counts())\n",
    "\n",
    "        print(f\"\\nRecords by country:\")\n",
    "        print(final_agg['ipc_country'].value_counts().head(10))\n",
    "\n",
    "        # Verify alignment with Stage 1 districts\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"District Alignment Check\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        stage1_locations = pd.read_parquet(DISTRICT_DATA_DIR / 'locations_aggregated.parquet')\n",
    "        stage1_districts = set(stage1_locations['ipc_geographic_unit_full'].unique())\n",
    "        stage2_districts = set(final_agg['ipc_geographic_unit_full'].unique())\n",
    "\n",
    "        overlap = stage1_districts & stage2_districts\n",
    "        only_stage1 = stage1_districts - stage2_districts\n",
    "        only_stage2 = stage2_districts - stage1_districts\n",
    "\n",
    "        print(f\"\\nStage 1 districts: {len(stage1_districts):,}\")\n",
    "        print(f\"Stage 2 districts: {len(stage2_districts):,}\")\n",
    "        print(f\"Overlap: {len(overlap):,} ({100*len(overlap)/len(stage1_districts):.1f}% of Stage 1)\")\n",
    "        print(f\"Only in Stage 1: {len(only_stage1):,}\")\n",
    "        print(f\"Only in Stage 2: {len(only_stage2):,}\")\n",
    "\n",
    "        # Save\n",
    "        print(f\"\\n5. Saving to {OUTPUT_PARQUET}...\")\n",
    "        final_agg.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "        print(\"   [OK] Parquet saved\")\n",
    "\n",
    "        print(f\"\\n6. Saving to {OUTPUT_CSV}...\")\n",
    "        final_agg.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(\"   [OK] CSV saved\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Stage 2 Monthly Location Aggregation Complete\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"\\n   WARNING: No matched records found!\")\n",
    "\n",
    "    print(f\"\\nEnd time: {datetime.now()}\")\n",
    "    print(f\"\\nOutput: {OUTPUT_PARQUET}\")\n",
    "    print(f\"Next step: Run 04a_stage2_create_ml_dataset.py\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
