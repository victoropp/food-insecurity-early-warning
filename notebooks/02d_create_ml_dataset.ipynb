{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# Create ML-Ready Dataset - District Level\n",
    "\n",
    "**Script**: `scripts/02_data_processing/04_create_ml_dataset.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Combines articles and locations aggregations into unified ML-ready dataset.\n",
    "\n",
    "**KEY OPERATIONS**:\n",
    "- Inner join on `ipc_geographic_unit_full` (unique district-period identifier)\n",
    "- Preserve all IPC metadata columns\n",
    "- Handle match_level from both sources\n",
    "- Extract date components (year, month, quarter)\n",
    "- Verify uniqueness and detect duplicates\n",
    "\n",
    "**Runtime**: ~15 minutes\n",
    "\n",
    "**Input**: \n",
    "- `data/district_level/articles_aggregated.parquet` (from 02b)\n",
    "- `data/district_level/locations_aggregated.parquet` (from 02c)\n",
    "\n",
    "**Output**: `data/district_level/ml_dataset_complete.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from config import BASE_DIR\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(str(BASE_DIR.parent.parent.parent))\n",
    "\n",
    "# District pipeline I/O (district_level subfolder)\n",
    "DISTRICT_DATA_DIR = BASE_DIR / 'data' / 'district_level'\n",
    "ARTICLES_FILE = DISTRICT_DATA_DIR / 'articles_aggregated.parquet'\n",
    "LOCATIONS_FILE = DISTRICT_DATA_DIR / 'locations_aggregated.parquet'\n",
    "OUTPUT_COMPLETE = DISTRICT_DATA_DIR / 'ml_dataset_complete.parquet'\n",
    "OUTPUT_COMPLETE_CSV = DISTRICT_DATA_DIR / 'ml_dataset_complete.csv'\n",
    "\n",
    "print(f\"Articles file: {ARTICLES_FILE}\")\n",
    "print(f\"Locations file: {LOCATIONS_FILE}\")\n",
    "print(f\"Output: {OUTPUT_COMPLETE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-header",
   "metadata": {},
   "source": [
    "## Main Processing\n",
    "\n",
    "Load aggregated data, merge, and create ML-ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Creating ML-Ready Dataset - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Start time: {datetime.now()}\\n\")\n",
    "\n",
    "# Load aggregated data\n",
    "print(\"1. Loading district-level aggregated articles...\")\n",
    "articles = pd.read_parquet(ARTICLES_FILE)\n",
    "print(f\"   Loaded {len(articles):,} article aggregations\")\n",
    "print(f\"   Unique districts: {articles['ipc_district'].nunique():,}\")\n",
    "print(f\"   Columns: {len(articles.columns)}\")\n",
    "\n",
    "print(\"\\n2. Loading district-level aggregated locations...\")\n",
    "locations = pd.read_parquet(LOCATIONS_FILE)\n",
    "print(f\"   Loaded {len(locations):,} location aggregations\")\n",
    "print(f\"   Unique districts: {locations['ipc_district'].nunique():,}\")\n",
    "print(f\"   Columns: {len(locations.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-merge-keys",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define merge keys - KEY CHANGE: Include district and region\n",
    "merge_keys = [\n",
    "    'ipc_id',\n",
    "    'ipc_country',\n",
    "    'ipc_country_code',\n",
    "    'ipc_fips_code',\n",
    "    'ipc_district',  # NEW: Extracted district\n",
    "    'ipc_region',  # NEW: Extracted region\n",
    "    'ipc_geographic_unit',\n",
    "    'ipc_geographic_unit_full',  # KEY: This is the unique identifier\n",
    "    'ipc_period_start',\n",
    "    'ipc_period_end',\n",
    "    'ipc_period_length_days',\n",
    "    'ipc_value',\n",
    "    'ipc_description',\n",
    "    'ipc_binary_crisis',\n",
    "    'ipc_is_allowing_assistance',\n",
    "    'ipc_fewsnet_region',\n",
    "    'ipc_geographic_group',\n",
    "    'ipc_scenario',\n",
    "    'ipc_classification_scale',\n",
    "    'ipc_reporting_date'\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(merge_keys)} merge keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-merge-keys",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify merge keys exist in both datasets\n",
    "print(\"\\n3. Verifying merge keys...\")\n",
    "articles_keys = set(merge_keys) & set(articles.columns)\n",
    "locations_keys = set(merge_keys) & set(locations.columns)\n",
    "\n",
    "print(f\"   Articles has {len(articles_keys)}/{len(merge_keys)} keys\")\n",
    "print(f\"   Locations has {len(locations_keys)}/{len(merge_keys)} keys\")\n",
    "\n",
    "missing_from_articles = set(merge_keys) - articles_keys\n",
    "missing_from_locations = set(merge_keys) - locations_keys\n",
    "\n",
    "if missing_from_articles:\n",
    "    print(f\"   [WARNING] Missing from articles: {missing_from_articles}\")\n",
    "if missing_from_locations:\n",
    "    print(f\"   [WARNING] Missing from locations: {missing_from_locations}\")\n",
    "\n",
    "# Use common keys\n",
    "actual_merge_keys = list(articles_keys & locations_keys)\n",
    "print(f\"\\n   Using {len(actual_merge_keys)} merge keys\")\n",
    "\n",
    "# Critical key validation\n",
    "critical_keys = ['ipc_id', 'ipc_geographic_unit_full', 'ipc_period_start', 'ipc_value']\n",
    "missing_critical = set(critical_keys) - set(actual_merge_keys)\n",
    "if missing_critical:\n",
    "    raise ValueError(f\"CRITICAL ERROR: Missing essential merge keys: {missing_critical}. \"\n",
    "                     \"Pipeline cannot continue. Check upstream scripts.\")\n",
    "\n",
    "print(\"   All critical keys present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert-types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure date columns have consistent types before merge\n",
    "print(\"\\n   Converting date columns to consistent types...\")\n",
    "date_cols = ['ipc_period_start', 'ipc_period_end', 'ipc_reporting_date']\n",
    "for col in date_cols:\n",
    "    if col in articles.columns:\n",
    "        articles[col] = pd.to_datetime(articles[col])\n",
    "    if col in locations.columns:\n",
    "        locations[col] = pd.to_datetime(locations[col])\n",
    "\n",
    "# Ensure ipc_id has consistent type (convert to string)\n",
    "print(\"   Converting ipc_id to consistent string type...\")\n",
    "if 'ipc_id' in articles.columns:\n",
    "    articles['ipc_id'] = articles['ipc_id'].astype(str)\n",
    "if 'ipc_id' in locations.columns:\n",
    "    locations['ipc_id'] = locations['ipc_id'].astype(str)\n",
    "\n",
    "print(\"   Type conversions complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create COMPLETE dataset (inner join)\n",
    "print(\"\\n4. Creating COMPLETE dataset (inner join)...\")\n",
    "df_complete = articles.merge(\n",
    "    locations,\n",
    "    on=actual_merge_keys,\n",
    "    how='inner',\n",
    "    suffixes=('_articles', '_locations')\n",
    ")\n",
    "\n",
    "# Add data source flags\n",
    "df_complete['has_articles'] = True\n",
    "df_complete['has_locations'] = True\n",
    "df_complete['data_source'] = 'complete'\n",
    "\n",
    "print(f\"   Result: {len(df_complete):,} rows\")\n",
    "print(f\"   Unique ipc_geographic_unit_full: {df_complete['ipc_geographic_unit_full'].nunique():,}\")\n",
    "print(f\"   Unique districts: {df_complete['ipc_district'].nunique():,}\")\n",
    "print(f\"   Columns: {len(df_complete.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handle-match-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle match_level columns from both sources\n",
    "if 'match_level_articles' in df_complete.columns and 'match_level_locations' in df_complete.columns:\n",
    "    # Prefer locations match level, fallback to articles\n",
    "    df_complete['match_level'] = df_complete['match_level_locations'].fillna(\n",
    "        df_complete['match_level_articles']\n",
    "    )\n",
    "    # Drop the separate columns\n",
    "    df_complete = df_complete.drop(['match_level_articles', 'match_level_locations'], axis=1)\n",
    "    print(\"   Merged match_level columns\")\n",
    "else:\n",
    "    print(\"   match_level columns not found or already merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-date-components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components for analysis\n",
    "print(\"\\n5. Adding date components...\")\n",
    "df_complete['ipc_period_start'] = pd.to_datetime(df_complete['ipc_period_start'])\n",
    "df_complete['ipc_period_end'] = pd.to_datetime(df_complete['ipc_period_end'])\n",
    "df_complete['year'] = df_complete['ipc_period_start'].dt.year\n",
    "df_complete['month'] = df_complete['ipc_period_start'].dt.month\n",
    "df_complete['quarter'] = df_complete['ipc_period_start'].dt.quarter\n",
    "\n",
    "print(\"   Added year, month, quarter columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset Summary - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal records: {len(df_complete):,}\")\n",
    "print(f\"Unique districts (ipc_district): {df_complete['ipc_district'].nunique():,}\")\n",
    "print(f\"Unique geographic_unit_full: {df_complete['ipc_geographic_unit_full'].nunique():,}\")\n",
    "print(f\"Unique IPC assessments (ipc_id): {df_complete['ipc_id'].nunique():,}\")\n",
    "print(f\"Date range: {df_complete['ipc_period_start'].min()} to {df_complete['ipc_period_end'].max()}\")\n",
    "print(f\"Countries: {df_complete['ipc_country'].nunique()}\")\n",
    "\n",
    "print(\"\\nDistricts per country:\")\n",
    "district_counts = df_complete.groupby('ipc_country')['ipc_district'].nunique().sort_values(ascending=False)\n",
    "for country, count in district_counts.head(15).items():\n",
    "    records = len(df_complete[df_complete['ipc_country'] == country])\n",
    "    print(f\"   {country}: {count} districts, {records:,} records\")\n",
    "\n",
    "print(\"\\nMatch level distribution:\")\n",
    "if 'match_level' in df_complete.columns:\n",
    "    print(df_complete['match_level'].value_counts())\n",
    "\n",
    "print(\"\\nIPC Binary Crisis distribution:\")\n",
    "print(df_complete['ipc_binary_crisis'].value_counts())\n",
    "\n",
    "print(\"\\nIPC Value distribution:\")\n",
    "print(df_complete['ipc_value'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-header",
   "metadata": {},
   "source": [
    "## District-Level Verification\n",
    "\n",
    "Verify uniqueness and detect duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify district-level granularity\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"District-Level Verification\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if same (district, period) can have different IPC values\n",
    "# (This would indicate we're at the right level)\n",
    "grouped = df_complete.groupby(['ipc_district', 'ipc_period_start'])['ipc_value'].nunique()\n",
    "multi_value = (grouped > 1).sum()\n",
    "print(f\"\\n(district, period) combinations with varying IPC values: {multi_value}\")\n",
    "print(\"(Expected: 0 if each district-period has one IPC value)\")\n",
    "\n",
    "# Check uniqueness of (geographic_unit_full, period)\n",
    "unique_check = df_complete.groupby(['ipc_geographic_unit_full', 'ipc_period_start']).size()\n",
    "duplicates = (unique_check > 1).sum()\n",
    "print(f\"(geographic_unit_full, period) duplicates: {duplicates}\")\n",
    "print(\"(Expected: 0 - each observation should be unique)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nWARNING: Found duplicate observations. Deduplication may be needed.\")\n",
    "    # Show sample duplicates\n",
    "    dup_idx = unique_check[unique_check > 1].head(3).index\n",
    "    for idx in dup_idx:\n",
    "        dup_rows = df_complete[\n",
    "            (df_complete['ipc_geographic_unit_full'] == idx[0]) &\n",
    "            (df_complete['ipc_period_start'] == idx[1])\n",
    "        ]\n",
    "        print(f\"\\n   Duplicate: {idx[0][:50]}..., {idx[1]}\")\n",
    "        print(f\"   Rows: {len(dup_rows)}, IPC values: {dup_rows['ipc_value'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "print(f\"\\n6. Saving COMPLETE dataset...\")\n",
    "print(f\"   Parquet: {OUTPUT_COMPLETE}\")\n",
    "df_complete.to_parquet(OUTPUT_COMPLETE, index=False)\n",
    "print(\"   [OK] Parquet saved\")\n",
    "\n",
    "print(f\"   CSV: {OUTPUT_COMPLETE_CSV}\")\n",
    "df_complete.to_csv(OUTPUT_COMPLETE_CSV, index=False)\n",
    "print(\"   [OK] CSV saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "column-summary-header",
   "metadata": {},
   "source": [
    "## Column Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "column-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Column Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Categorize columns\n",
    "ipc_cols = [c for c in df_complete.columns if c.startswith('ipc_')]\n",
    "geo_cols = ['ipc_district', 'ipc_region', 'primary_gadm2', 'primary_gadm3',\n",
    "            'avg_latitude', 'avg_longitude', 'latitude_std', 'longitude_std']\n",
    "geo_cols = [c for c in geo_cols if c in df_complete.columns]\n",
    "article_cols = [c for c in df_complete.columns if 'article' in c.lower() or c.endswith('_articles')]\n",
    "location_cols = [c for c in df_complete.columns if 'location' in c.lower() or c.endswith('_locations')]\n",
    "time_cols = ['year', 'month', 'quarter']\n",
    "\n",
    "print(f\"\\nIPC metadata columns: {len(ipc_cols)}\")\n",
    "print(f\"Geographic columns: {len(geo_cols)}\")\n",
    "print(f\"Article-derived columns: {len(article_cols)}\")\n",
    "print(f\"Location-derived columns: {len(location_cols)}\")\n",
    "print(f\"Time columns: {len(time_cols)}\")\n",
    "print(f\"Total columns: {len(df_complete.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ML Dataset Creation Complete - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput: {OUTPUT_COMPLETE}\")\n",
    "print(f\"\\nNext step: Run 05_deduplicate_district.py if duplicates found\")\n",
    "print(f\"Then: Run 06_stage1_feature_engineering_district.py\")\n",
    "print(f\"\\nEnd time: {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
