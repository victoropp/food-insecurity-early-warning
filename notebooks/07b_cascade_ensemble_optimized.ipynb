{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a39c0fe",
   "metadata": {},
   "source": [
    "# Cascade Analysis: Optimized Ensemble\n",
    "\n",
    "**Script**: `scripts\\06_cascade_analysis\\05_cascade_ensemble_optimized_production.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Implements two-stage cascade decision system.\n",
    "\n",
    "**CASCADE LOGIC**:\n",
    "1. Trust AR baseline if AR Predicts Crises\n",
    "2. Otherwise use XGBoost advanced if AR predicts no crises\n",
    "**OPTIMIZATION**: Tunes confidence threshold to maximize F1 score.\n",
    "\n",
    "**KEY ADVANTAGE**: Uses News Features to resolve AR failures\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371f5d3",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stage 3: Optimized Cascade Ensemble - PRODUCTION VERSION\n",
    "=========================================================\n",
    "Production-ready cascade ensemble using optimized Stage 2 XGBoost model.\n",
    "\n",
    "This script creates the final cascade ensemble with:\n",
    "- Optimized Stage 2 XGBoost (hyperparameter-tuned)\n",
    "- Simple binary override logic\n",
    "- Full metadata for district and country level analysis\n",
    "- Comprehensive metrics for dissertation\n",
    "\n",
    "CASCADE STRATEGY (Simple Binary Logic):\n",
    "1. Use AR baseline predictions as primary\n",
    "2. If AR = 1: Keep as 1 (trust AR's crisis prediction)\n",
    "3. If AR = 0: Use Stage 2's binary prediction\n",
    "   - If Stage 2 = 1: Override to 1 (Stage 2 detected crisis)\n",
    "   - If Stage 2 = 0: Keep as 0 (both agree: no crisis)\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "Date: December 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    average_precision_score, brier_score_loss, log_loss,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Force unbuffered output\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# Add parent directory to path for config import\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "from config import BASE_DIR, RESULTS_DIR, STAGE1_RESULTS_DIR, STAGE2_MODELS_DIR\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT DIRECTORIES\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR = RESULTS_DIR / 'cascade_optimized_production'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02581544",
   "metadata": {},
   "source": [
    "## Load Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FIGURES_DIR = BASE_DIR / 'FIGURES' / 'cascade_optimized'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OPTIMIZED CASCADE ENSEMBLE - PRODUCTION VERSION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "print(\"Strategy: AR Baseline + Optimized Stage 2 XGBoost Override\")\n",
    "print(\"Logic: Simple binary override (AR=0 cases use Stage 2 prediction)\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def cost_sensitive_threshold_search(y_true, y_pred_prob, cost_fn=10, cost_fp=1,\n",
    "                                    thresholds=None):\n",
    "    \"\"\"\n",
    "    Find optimal threshold that minimizes weighted cost.\n",
    "\n",
    "    Cost function: 10*FN + 1*FP (missing a crisis is 10x worse than false alarm)\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 91)\n",
    "\n",
    "    results = []\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_pred_prob >= thresh).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        if cm.size == 4:\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "        total_cost = cost_fn * fn + cost_fp * fp\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            'threshold': thresh,\n",
    "            'cost': total_cost,\n",
    "            'fn': fn, 'fp': fp, 'tp': tp, 'tn': tn,\n",
    "            'precision': precision, 'recall': recall, 'f1': f1\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    optimal_idx = results_df['cost'].idxmin()\n",
    "    optimal = results_df.iloc[optimal_idx]\n",
    "\n",
    "    return {\n",
    "        'optimal_threshold': optimal['threshold'],\n",
    "        'optimal_cost': optimal['cost'],\n",
    "        'optimal_metrics': optimal.to_dict(),\n",
    "        'all_results': results_df\n",
    "    }\n",
    "\n",
    "\n",
    "def assign_confusion_class(y_true, y_pred):\n",
    "    \"\"\"Assign confusion class labels for cartographic mapping.\"\"\"\n",
    "    classes = np.empty(len(y_true), dtype=object)\n",
    "    classes[(y_true == 0) & (y_pred == 0)] = 'TN'\n",
    "    classes[(y_true == 1) & (y_pred == 1)] = 'TP'\n",
    "    classes[(y_true == 0) & (y_pred == 1)] = 'FP'\n",
    "    classes[(y_true == 1) & (y_pred == 0)] = 'FN'\n",
    "    return classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a3867",
   "metadata": {},
   "source": [
    "## Define Cascade Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_comprehensive_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Compute comprehensive metrics for a prediction.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.size == 4:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "    # Positive Predictive Value (same as precision)\n",
    "    ppv = precision\n",
    "    # Negative Predictive Value\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    # False Positive Rate\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    # False Negative Rate\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "    # Cost (10:1 ratio for FN:FP)\n",
    "    cost_10_1 = 10 * fn + 1 * fp\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'ppv': ppv,\n",
    "        'npv': npv,\n",
    "        'fpr': fpr,\n",
    "        'fnr': fnr,\n",
    "        'tp': int(tp),\n",
    "        'tn': int(tn),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn),\n",
    "        'total': int(tp + tn + fp + fn),\n",
    "        'cost_10_1': int(cost_10_1)\n",
    "    }\n",
    "\n",
    "    # Add AUC metrics if probabilities provided\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                metrics['auc_roc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "                metrics['pr_auc'] = average_precision_score(y_true, y_pred_proba)\n",
    "                metrics['brier_score'] = brier_score_loss(y_true, y_pred_proba)\n",
    "            else:\n",
    "                metrics['auc_roc'] = np.nan\n",
    "                metrics['pr_auc'] = np.nan\n",
    "                metrics['brier_score'] = np.nan\n",
    "        except:\n",
    "            metrics['auc_roc'] = np.nan\n",
    "            metrics['pr_auc'] = np.nan\n",
    "            metrics['brier_score'] = np.nan\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 1: Loading Data\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Stage 1: AR Baseline predictions\n",
    "stage1_file = STAGE1_RESULTS_DIR / 'predictions_h8_averaged.csv'\n",
    "print(f\"Loading Stage 1 (AR Baseline): {stage1_file.name}\")\n",
    "stage1_df = pd.read_csv(stage1_file)\n",
    "print(f\"  Total observations: {len(stage1_df):,}\")\n",
    "\n",
    "# Extract relevant columns\n",
    "stage1_cols = [\n",
    "    # Geographic identifiers\n",
    "    'ipc_geographic_unit_full', 'ipc_district', 'ipc_region',\n",
    "    'ipc_country', 'ipc_country_code',\n",
    "    # Coordinates\n",
    "    'avg_latitude', 'avg_longitude',\n",
    "    # Temporal identifiers\n",
    "    'year_month', 'ipc_period_start', 'ipc_period_end',\n",
    "    # IPC data\n",
    "    'ipc_value', 'ipc_future_crisis',\n",
    "    # AR predictions\n",
    "    'pred_prob', 'y_pred_optimal',\n",
    "    # Fold\n",
    "    'fold'\n",
    "]\n",
    "\n",
    "available_cols = [c for c in stage1_cols if c in stage1_df.columns]\n",
    "stage1_data = stage1_df[available_cols].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "stage1_data = stage1_data.rename(columns={\n",
    "    'pred_prob': 'ar_prob',\n",
    "    'y_pred_optimal': 'ar_pred',\n",
    "    'ipc_future_crisis': 'y_true'\n",
    "})\n",
    "\n",
    "stage1_data['date'] = pd.to_datetime(stage1_data['ipc_period_start'])\n",
    "stage1_data = stage1_data[stage1_data['y_true'].notna()].copy()\n",
    "\n",
    "n_total = len(stage1_data)\n",
    "n_crisis = int(stage1_data['y_true'].sum())\n",
    "crisis_rate = 100 * n_crisis / n_total\n",
    "\n",
    "print(f\"  Valid observations: {n_total:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6ae20",
   "metadata": {},
   "source": [
    "## Optimize Confidence Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Crisis events: {n_crisis:,} ({crisis_rate:.1f}%)\")\n",
    "print(f\"  Non-crisis events: {n_total - n_crisis:,} ({100-crisis_rate:.1f}%)\")\n",
    "print(f\"  Countries: {stage1_data['ipc_country'].nunique()}\")\n",
    "print(f\"  Districts: {stage1_data['ipc_geographic_unit_full'].nunique()}\")\n",
    "\n",
    "# Stage 2: Optimized XGBoost predictions\n",
    "stage2_file = STAGE2_MODELS_DIR / 'xgboost' / 'advanced_with_ar_optimized' / 'xgboost_optimized_predictions.csv'\n",
    "print(f\"\\nLoading Stage 2 (Optimized XGBoost): {stage2_file.name}\")\n",
    "stage2_df = pd.read_csv(stage2_file)\n",
    "print(f\"  Total observations: {len(stage2_df):,}\")\n",
    "\n",
    "# Extract Stage 2 predictions (using Youden optimal threshold)\n",
    "stage2_cols = ['ipc_geographic_unit_full', 'ipc_period_start', 'y_pred_youden', 'ipc_future_crisis']\n",
    "stage2_data = stage2_df[stage2_cols].copy()\n",
    "stage2_data.columns = ['geographic_unit', 'date', 'stage2_pred', 'y_true_s2']\n",
    "stage2_data['date'] = pd.to_datetime(stage2_data['date'])\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: MERGE DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 2: Merging Datasets\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Merge Stage 1 and Stage 2\n",
    "full_df = stage1_data.merge(\n",
    "    stage2_data[['geographic_unit', 'date', 'stage2_pred']],\n",
    "    left_on=['ipc_geographic_unit_full', 'date'],\n",
    "    right_on=['geographic_unit', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "full_df = full_df.drop(columns=['geographic_unit'], errors='ignore')\n",
    "\n",
    "n_with_s2 = full_df['stage2_pred'].notna().sum()\n",
    "n_without_s2 = full_df['stage2_pred'].isna().sum()\n",
    "\n",
    "print(f\"Full dataset: {len(full_df):,} observations\")\n",
    "print(f\"  With Stage 2 predictions: {n_with_s2:,} ({100*n_with_s2/len(full_df):.1f}%)\")\n",
    "print(f\"  Without Stage 2: {n_without_s2:,} ({100*n_without_s2/len(full_df):.1f}%)\")\n",
    "print()\n",
    "print(\"Stage 2 coverage explanation:\")\n",
    "print(\"  Stage 2 only covers IPC <= 2 AND AR == 0 (cases where AR predicts no crisis)\")\n",
    "print(\"  These are the cases where we want to detect crises AR might miss\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: SIMPLE CASCADE LOGIC (BINARY PREDICTIONS ONLY)\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 3: Applying Simple Cascade Logic\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"CASCADE RULE:\")\n",
    "print(\"  - If AR prediction = 1: Keep as 1 (trust AR's crisis prediction)\")\n",
    "print(\"  - If AR prediction = 0: Use Stage 2's binary prediction\")\n",
    "print(\"    * If Stage 2 prediction = 1: Override to 1 (Stage 2 detected crisis)\")\n",
    "print(\"    * If Stage 2 prediction = 0: Keep as 0 (both agree: no crisis)\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: BUILD SIMPLE CASCADE ENSEMBLE\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 4: Building Simple Cascade Ensemble\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get arrays\n",
    "y_true = full_df['y_true'].values\n",
    "ar_pred = full_df['ar_pred'].astype(int).values\n",
    "ar_prob = full_df['ar_prob'].values  # Keep for AUC calculations\n",
    "\n",
    "# Identify where Stage 2 can contribute\n",
    "has_s2 = full_df['stage2_pred'].notna()\n",
    "ar_predicts_no_crisis = ar_pred == 0\n",
    "can_use_s2 = ar_predicts_no_crisis & has_s2\n",
    "\n",
    "print(f\"\\nCases where AR predicted 0: {ar_predicts_no_crisis.sum():,}\")\n",
    "print(f\"Cases with Stage 2 prediction: {can_use_s2.sum():,}\")\n",
    "\n",
    "# Simple cascade logic\n",
    "cascade_pred = ar_pred.copy()  # Start with AR predictions\n",
    "\n",
    "# Where AR=0 and Stage 2 available, use Stage 2's prediction\n",
    "override_mask = np.zeros(len(full_df), dtype=bool)\n",
    "for idx in full_df[can_use_s2].index:\n",
    "    row_loc = full_df.index.get_loc(idx)\n",
    "    s2_pred = int(full_df.loc[idx, 'stage2_pred'])\n",
    "\n",
    "    if s2_pred == 1:\n",
    "        cascade_pred[row_loc] = 1  # Stage 2 says crisis\n",
    "        override_mask[row_loc] = True\n",
    "\n",
    "full_df['cascade_pred'] = cascade_pred\n",
    "full_df['was_overridden'] = override_mask.astype(int)\n",
    "\n",
    "n_overrides = override_mask.sum()\n",
    "print(f\"\\nStage 2 overrides (AR=0 -> 1): {n_overrides:,}\")\n",
    "print(f\"Stage 2 confirms AR=0: {(can_use_s2 & ~override_mask).sum():,}\")\n",
    "print(f\"Override rate: {100*n_overrides/can_use_s2.sum():.1f}% of AR=0 cases with Stage 2\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: COMPUTE KEY SAVES\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 5: Computing Key Saves\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Key saves: crises that AR missed but cascade caught\n",
    "ar_missed_crisis = (ar_pred == 0) & (y_true == 1)\n",
    "cascade_caught = cascade_pred == 1\n",
    "key_saves_mask = ar_missed_crisis & cascade_caught\n",
    "\n",
    "full_df['ar_missed'] = ar_missed_crisis.astype(int)\n",
    "full_df['is_key_save'] = key_saves_mask.astype(int)\n",
    "\n",
    "n_key_saves = key_saves_mask.sum()\n",
    "n_ar_missed = ar_missed_crisis.sum()\n",
    "\n",
    "print(f\"AR baseline missed: {n_ar_missed:,} crises\")\n",
    "print(f\"Cascade caught (key saves): {n_key_saves:,} crises\")\n",
    "print(f\"Key save rate: {100*n_key_saves/n_ar_missed:.1f}% of AR misses\")\n",
    "\n",
    "# Key saves by country\n",
    "print(\"\\nKey saves by country:\")\n",
    "key_saves_by_country = full_df[full_df['is_key_save'] == 1].groupby('ipc_country').size().sort_values(ascending=False)\n",
    "for country, count in key_saves_by_country.head(10).items():\n",
    "    print(f\"  {country:<30}: {count:>4}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: COMPUTE COMPREHENSIVE METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 6: Computing Comprehensive Metrics\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# AR Baseline metrics\n",
    "ar_metrics = compute_comprehensive_metrics(y_true, ar_pred, ar_prob)\n",
    "print(\"\\nAR BASELINE METRICS:\")\n",
    "print(f\"  Precision: {ar_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {ar_metrics['recall']:.4f}\")\n",
    "print(f\"  F1: {ar_metrics['f1']:.4f}\")\n",
    "print(f\"  Specificity: {ar_metrics['specificity']:.4f}\")\n",
    "print(f\"  Balanced Accuracy: {ar_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"  AUC-ROC: {ar_metrics.get('auc_roc', 'N/A')}\")\n",
    "print(f\"  FN (missed crises): {ar_metrics['fn']:,}\")\n",
    "\n",
    "# Cascade metrics\n",
    "cascade_metrics = compute_comprehensive_metrics(y_true, cascade_pred)\n",
    "print(\"\\nCASCADE ENSEMBLE METRICS:\")\n",
    "print(f\"  Precision: {cascade_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {cascade_metrics['recall']:.4f}\")\n",
    "print(f\"  F1: {cascade_metrics['f1']:.4f}\")\n",
    "print(f\"  Specificity: {cascade_metrics['specificity']:.4f}\")\n",
    "print(f\"  Balanced Accuracy: {cascade_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"  FN (missed crises): {cascade_metrics['fn']:,}\")\n",
    "\n",
    "# Improvement\n",
    "print(\"\\nIMPROVEMENT OVER AR BASELINE:\")\n",
    "print(f\"  Recall change: {cascade_metrics['recall'] - ar_metrics['recall']:+.4f}\")\n",
    "print(f\"  Precision change: {cascade_metrics['precision'] - ar_metrics['precision']:+.4f}\")\n",
    "print(f\"  F1 change: {cascade_metrics['f1'] - ar_metrics['f1']:+.4f}\")\n",
    "print(f\"  FN reduction: {ar_metrics['fn'] - cascade_metrics['fn']:,} fewer missed crises\")\n",
    "print(f\"  Key saves: {n_key_saves:,} crises saved\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: ASSIGN CONFUSION CLASSES FOR MAPPING\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 7: Assigning Confusion Classes for Mapping\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "full_df['confusion_ar'] = assign_confusion_class(y_true, ar_pred)\n",
    "full_df['confusion_cascade'] = assign_confusion_class(y_true, cascade_pred)\n",
    "\n",
    "# Summary\n",
    "for model_name, col in [('AR Baseline', 'confusion_ar'), ('Cascade', 'confusion_cascade')]:\n",
    "    print(f\"\\n{model_name} Confusion Distribution:\")\n",
    "    for cls in ['TP', 'TN', 'FP', 'FN']:\n",
    "        count = (full_df[col] == cls).sum()\n",
    "        pct = 100 * count / len(full_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b84234",
   "metadata": {},
   "source": [
    "## Apply Optimized Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424654b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        print(f\"  {cls}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: COMPUTE COUNTRY-LEVEL METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 8: Computing Country-Level Metrics\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "country_metrics = []\n",
    "\n",
    "for country in full_df['ipc_country'].unique():\n",
    "    country_df = full_df[full_df['ipc_country'] == country].copy()\n",
    "\n",
    "    y_true_c = country_df['y_true'].values\n",
    "    ar_pred_c = country_df['ar_pred'].values\n",
    "    cascade_pred_c = country_df['cascade_pred'].values\n",
    "    ar_prob_c = country_df['ar_prob'].values\n",
    "\n",
    "    # AR metrics\n",
    "    ar_m = compute_comprehensive_metrics(y_true_c, ar_pred_c, ar_prob_c)\n",
    "\n",
    "    # Cascade metrics\n",
    "    cas_m = compute_comprehensive_metrics(y_true_c, cascade_pred_c)\n",
    "\n",
    "    # Key saves for this country\n",
    "    country_key_saves = country_df['is_key_save'].sum()\n",
    "    country_ar_missed = country_df['ar_missed'].sum()\n",
    "\n",
    "    country_row = {\n",
    "        'country': country,\n",
    "        'n_observations': len(country_df),\n",
    "        'n_crisis': int(y_true_c.sum()),\n",
    "        'crisis_rate': float(y_true_c.mean()),\n",
    "        'n_districts': country_df['ipc_geographic_unit_full'].nunique(),\n",
    "\n",
    "        # AR Baseline\n",
    "        'ar_precision': ar_m['precision'],\n",
    "        'ar_recall': ar_m['recall'],\n",
    "        'ar_f1': ar_m['f1'],\n",
    "        'ar_specificity': ar_m['specificity'],\n",
    "        'ar_balanced_accuracy': ar_m['balanced_accuracy'],\n",
    "        'ar_auc_roc': ar_m.get('auc_roc', np.nan),\n",
    "        'ar_tp': ar_m['tp'],\n",
    "        'ar_tn': ar_m['tn'],\n",
    "        'ar_fp': ar_m['fp'],\n",
    "        'ar_fn': ar_m['fn'],\n",
    "\n",
    "        # Cascade\n",
    "        'cascade_precision': cas_m['precision'],\n",
    "        'cascade_recall': cas_m['recall'],\n",
    "        'cascade_f1': cas_m['f1'],\n",
    "        'cascade_specificity': cas_m['specificity'],\n",
    "        'cascade_balanced_accuracy': cas_m['balanced_accuracy'],\n",
    "        'cascade_tp': cas_m['tp'],\n",
    "        'cascade_tn': cas_m['tn'],\n",
    "        'cascade_fp': cas_m['fp'],\n",
    "        'cascade_fn': cas_m['fn'],\n",
    "\n",
    "        # Key saves\n",
    "        'key_saves': int(country_key_saves),\n",
    "        'ar_missed_crises': int(country_ar_missed),\n",
    "        'key_save_rate': float(country_key_saves / country_ar_missed) if country_ar_missed > 0 else 0,\n",
    "\n",
    "        # Improvement\n",
    "        'recall_improvement': cas_m['recall'] - ar_m['recall'],\n",
    "        'precision_change': cas_m['precision'] - ar_m['precision'],\n",
    "        'f1_change': cas_m['f1'] - ar_m['f1'],\n",
    "        'fn_reduction': ar_m['fn'] - cas_m['fn']\n",
    "    }\n",
    "\n",
    "    country_metrics.append(country_row)\n",
    "\n",
    "country_metrics_df = pd.DataFrame(country_metrics)\n",
    "country_metrics_df = country_metrics_df.sort_values('key_saves', ascending=False)\n",
    "\n",
    "print(f\"\\nCountry-level metrics computed for {len(country_metrics_df)} countries\")\n",
    "print(\"\\nTop countries by key saves:\")\n",
    "for _, row in country_metrics_df.head(5).iterrows():\n",
    "    print(f\"  {row['country']:<25}: {row['key_saves']:>3} key saves, \"\n",
    "          f\"recall +{row['recall_improvement']:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 9: COMPUTE DISTRICT-LEVEL METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 9: Computing District-Level Metrics\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "district_metrics = []\n",
    "\n",
    "for district in full_df['ipc_geographic_unit_full'].unique():\n",
    "    district_df = full_df[full_df['ipc_geographic_unit_full'] == district].copy()\n",
    "\n",
    "    if len(district_df) < 2:\n",
    "        continue\n",
    "\n",
    "    y_true_d = district_df['y_true'].values\n",
    "    ar_pred_d = district_df['ar_pred'].values\n",
    "    cascade_pred_d = district_df['cascade_pred'].values\n",
    "\n",
    "    # Simple metrics (may not have enough data for all metrics)\n",
    "    ar_m = compute_comprehensive_metrics(y_true_d, ar_pred_d)\n",
    "    cas_m = compute_comprehensive_metrics(y_true_d, cascade_pred_d)\n",
    "\n",
    "    district_row = {\n",
    "        'district': district,\n",
    "        'country': district_df['ipc_country'].iloc[0],\n",
    "        'avg_latitude': district_df['avg_latitude'].iloc[0] if 'avg_latitude' in district_df.columns else np.nan,\n",
    "        'avg_longitude': district_df['avg_longitude'].iloc[0] if 'avg_longitude' in district_df.columns else np.nan,\n",
    "        'n_observations': len(district_df),\n",
    "        'n_crisis': int(y_true_d.sum()),\n",
    "        'crisis_rate': float(y_true_d.mean()),\n",
    "\n",
    "        # AR Baseline\n",
    "        'ar_recall': ar_m['recall'],\n",
    "        'ar_precision': ar_m['precision'],\n",
    "        'ar_f1': ar_m['f1'],\n",
    "        'ar_fn': ar_m['fn'],\n",
    "\n",
    "        # Cascade\n",
    "        'cascade_recall': cas_m['recall'],\n",
    "        'cascade_precision': cas_m['precision'],\n",
    "        'cascade_f1': cas_m['f1'],\n",
    "        'cascade_fn': cas_m['fn'],\n",
    "\n",
    "        # Key saves\n",
    "        'key_saves': int(district_df['is_key_save'].sum()),\n",
    "        'ar_missed_crises': int(district_df['ar_missed'].sum()),\n",
    "\n",
    "        # Improvement\n",
    "        'recall_improvement': cas_m['recall'] - ar_m['recall'],\n",
    "        'fn_reduction': ar_m['fn'] - cas_m['fn']\n",
    "    }\n",
    "\n",
    "    district_metrics.append(district_row)\n",
    "\n",
    "district_metrics_df = pd.DataFrame(district_metrics)\n",
    "print(f\"District-level metrics computed for {len(district_metrics_df)} districts\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 10: SAVE ALL OUTPUTS\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "print(\"STEP 10: Saving All Outputs\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 1. Full predictions with all metadata\n",
    "predictions_file = OUTPUT_DIR / 'cascade_optimized_predictions.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91efd0",
   "metadata": {},
   "source": [
    "## Evaluation and Country Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a612a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(predictions_file, index=False)\n",
    "print(f\"[OK] Predictions: {predictions_file.name} ({len(full_df):,} rows)\")\n",
    "\n",
    "# 2. Key saves dataset\n",
    "key_saves_df = full_df[full_df['is_key_save'] == 1].copy()\n",
    "key_saves_file = OUTPUT_DIR / 'key_saves.csv'\n",
    "key_saves_df.to_csv(key_saves_file, index=False)\n",
    "print(f\"[OK] Key saves: {key_saves_file.name} ({len(key_saves_df)} rows)\")\n",
    "\n",
    "# 3. Country-level metrics\n",
    "country_file = OUTPUT_DIR / 'country_metrics.csv'\n",
    "country_metrics_df.to_csv(country_file, index=False)\n",
    "print(f\"[OK] Country metrics: {country_file.name} ({len(country_metrics_df)} countries)\")\n",
    "\n",
    "# 4. District-level metrics\n",
    "district_file = OUTPUT_DIR / 'district_metrics.csv'\n",
    "district_metrics_df.to_csv(district_file, index=False)\n",
    "print(f\"[OK] District metrics: {district_file.name} ({len(district_metrics_df)} districts)\")\n",
    "\n",
    "# 5. Simple summary (no threshold analysis needed for binary logic)\n",
    "print(\"[OK] Simple binary cascade - no threshold tuning required\")\n",
    "\n",
    "# 6. Comprehensive summary JSON\n",
    "summary = {\n",
    "    'model': 'Optimized Cascade Ensemble',\n",
    "    'strategy': 'AR Baseline + Optimized Stage 2 XGBoost with Simple Binary Override',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "\n",
    "    'data': {\n",
    "        'total_observations': int(len(full_df)),\n",
    "        'total_crises': int(n_crisis),\n",
    "        'crisis_rate': float(n_crisis / len(full_df)),\n",
    "        'countries': int(full_df['ipc_country'].nunique()),\n",
    "        'districts': int(full_df['ipc_geographic_unit_full'].nunique()),\n",
    "        'with_stage2_predictions': int(n_with_s2),\n",
    "        'override_candidates': int(can_use_s2.sum())\n",
    "    },\n",
    "\n",
    "    'cascade_strategy': {\n",
    "        'method': 'Simple Binary Logic',\n",
    "        'rule': 'If AR=1 keep, if AR=0 use Stage2 prediction',\n",
    "        'total_overrides': int(n_overrides),\n",
    "        'override_rate': float(n_overrides / can_use_s2.sum()) if can_use_s2.sum() > 0 else 0\n",
    "    },\n",
    "\n",
    "    'ar_baseline_performance': {\n",
    "        'precision': float(ar_metrics['precision']),\n",
    "        'recall': float(ar_metrics['recall']),\n",
    "        'f1': float(ar_metrics['f1']),\n",
    "        'specificity': float(ar_metrics['specificity']),\n",
    "        'balanced_accuracy': float(ar_metrics['balanced_accuracy']),\n",
    "        'auc_roc': float(ar_metrics.get('auc_roc', 0)),\n",
    "        'confusion_matrix': {\n",
    "            'tp': ar_metrics['tp'],\n",
    "            'tn': ar_metrics['tn'],\n",
    "            'fp': ar_metrics['fp'],\n",
    "            'fn': ar_metrics['fn']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'cascade_performance': {\n",
    "        'precision': float(cascade_metrics['precision']),\n",
    "        'recall': float(cascade_metrics['recall']),\n",
    "        'f1': float(cascade_metrics['f1']),\n",
    "        'specificity': float(cascade_metrics['specificity']),\n",
    "        'balanced_accuracy': float(cascade_metrics['balanced_accuracy']),\n",
    "        'confusion_matrix': {\n",
    "            'tp': cascade_metrics['tp'],\n",
    "            'tn': cascade_metrics['tn'],\n",
    "            'fp': cascade_metrics['fp'],\n",
    "            'fn': cascade_metrics['fn']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'improvement': {\n",
    "        'recall_change': float(cascade_metrics['recall'] - ar_metrics['recall']),\n",
    "        'precision_change': float(cascade_metrics['precision'] - ar_metrics['precision']),\n",
    "        'f1_change': float(cascade_metrics['f1'] - ar_metrics['f1']),\n",
    "        'fn_reduction': ar_metrics['fn'] - cascade_metrics['fn'],\n",
    "        'key_saves': int(n_key_saves),\n",
    "        'ar_missed_crises': int(n_ar_missed),\n",
    "        'key_save_rate': float(n_key_saves / n_ar_missed) if n_ar_missed > 0 else 0\n",
    "    },\n",
    "\n",
    "    'key_saves_by_country': key_saves_by_country.to_dict(),\n",
    "\n",
    "    'output_files': {\n",
    "        'predictions': str(predictions_file.name),\n",
    "        'key_saves': str(key_saves_file.name),\n",
    "        'country_metrics': str(country_file.name),\n",
    "        'district_metrics': str(district_file.name),\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_file = OUTPUT_DIR / 'cascade_optimized_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"[OK] Summary: {summary_file.name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"OPTIMIZED CASCADE ENSEMBLE - COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"STRATEGY:\")\n",
    "print(\"  - AR Baseline as primary predictor (using optimal threshold)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541f636",
   "metadata": {},
   "source": [
    "## Save Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72560b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  - Stage 2 (Optimized XGBoost Advanced) refines AR=0 cases\")\n",
    "print(\"  - Simple binary logic:\")\n",
    "print(\"    * If AR = 1: Keep as 1 (trust AR)\")\n",
    "print(\"    * If AR = 0 and Stage 2 = 1: Override to 1 (Stage 2 detects crisis)\")\n",
    "print(\"    * If AR = 0 and Stage 2 = 0: Keep as 0 (both agree)\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFORMANCE: AR BASELINE vs ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"RECALL (Most Important - Catching Crises):\")\n",
    "print(f\"  AR Baseline:  {ar_metrics['recall']:.4f} ({ar_metrics['recall']*100:.2f}%)\")\n",
    "print(f\"  Ensemble:     {cascade_metrics['recall']:.4f} ({cascade_metrics['recall']*100:.2f}%)\")\n",
    "print(f\"  Improvement:  {cascade_metrics['recall']-ar_metrics['recall']:+.4f} ({(cascade_metrics['recall']-ar_metrics['recall'])*100:+.2f} percentage points)\")\n",
    "print(f\"  --> THE ENSEMBLE CATCHES {n_key_saves} MORE CRISES THAT AR MISSED\")\n",
    "print()\n",
    "print(\"PRECISION:\")\n",
    "print(f\"  AR Baseline:  {ar_metrics['precision']:.4f}\")\n",
    "print(f\"  Ensemble:     {cascade_metrics['precision']:.4f}\")\n",
    "print(f\"  Change:       {cascade_metrics['precision']-ar_metrics['precision']:+.4f}\")\n",
    "print()\n",
    "print(\"F1 SCORE:\")\n",
    "print(f\"  AR Baseline:  {ar_metrics['f1']:.4f}\")\n",
    "print(f\"  Ensemble:     {cascade_metrics['f1']:.4f}\")\n",
    "print(f\"  Change:       {cascade_metrics['f1']-ar_metrics['f1']:+.4f}\")\n",
    "print()\n",
    "print(\"MISSED CRISES (False Negatives - The Critical Metric):\")\n",
    "print(f\"  AR Baseline:  {ar_metrics['fn']:,} crises missed\")\n",
    "print(f\"  Ensemble:     {cascade_metrics['fn']:,} crises missed\")\n",
    "print(f\"  Reduction:    {ar_metrics['fn']-cascade_metrics['fn']:,} fewer missed crises ({100*(ar_metrics['fn']-cascade_metrics['fn'])/ar_metrics['fn']:.1f}% reduction)\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2 INCREMENTAL CONTRIBUTION (On AR=0 Cases Only)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Cases where AR predicted 'no crisis': {can_use_s2.sum():,}\")\n",
    "print(f\"  Stage 2 overrides (predicted crisis):  {n_overrides:,} ({100*n_overrides/can_use_s2.sum():.1f}%)\")\n",
    "print(f\"  Stage 2 confirms AR (no crisis):       {(can_use_s2 & ~override_mask).sum():,}\")\n",
    "print()\n",
    "print(f\"  Stage 2 Performance on Overrides:\")\n",
    "print(f\"    - True crises caught: {n_key_saves:,}\")\n",
    "print(f\"    - False alarms: {n_overrides - n_key_saves:,}\")\n",
    "print(f\"    - Precision on overrides: {n_key_saves/n_overrides:.4f}\")\n",
    "print(f\"    - Key save rate: {100*n_key_saves/n_ar_missed:.1f}% of AR's misses\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"The ensemble improves recall from {ar_metrics['recall']:.4f} to {cascade_metrics['recall']:.4f}.\")\n",
    "print(f\"This is not just an improved measurement - it means {n_key_saves} REAL CRISES\")\n",
    "print(f\"that would have been MISSED are now PREDICTED and can trigger early warning.\")\n",
    "print(f\"These are the cases that matter most: vulnerable populations facing food insecurity.\")\n",
    "print()\n",
    "print(\"OUTPUT FILES:\")\n",
    "for name, path in [\n",
    "    ('Predictions', predictions_file),\n",
    "    ('Key Saves', key_saves_file),\n",
    "    ('Country Metrics', country_file),\n",
    "    ('District Metrics', district_file),\n",
    "    ('Summary', summary_file)\n",
    "]:\n",
    "    print(f\"  - {name}: {path.name}\")\n",
    "print()\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
