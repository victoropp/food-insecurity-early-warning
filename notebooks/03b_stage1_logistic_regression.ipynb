{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# Stage 1: Logistic Regression Baseline with Spatial CV - District Level\n",
    "\n",
    "**Script**: `scripts/03_stage1_baseline/07_stage1_logistic_regression.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Predicts binary crisis onset using spatio-temporal autoregressive features (Lt, Ls):\n",
    "- **Spatial CV**: K-means clustering on district coordinates\n",
    "- **Threshold tuning**: Balanced precision/recall with minimum constraint\n",
    "- **Multiple strategies**: F1-optimal, G-mean, Youden's J, balanced P=R\n",
    "- **District-level predictions**: Saved for cartographic analysis\n",
    "\n",
    "**Model**: Logistic Regression with L2 regularization, class balancing\n",
    "\n",
    "**Runtime**: ~20 minutes (5-fold CV Ã— 3 horizons)\n",
    "\n",
    "**Input**: `data/district_level/stage1_features.parquet` (from 03a)\n",
    "\n",
    "**Output**: \n",
    "- `results/stage1_baseline/predictions_h{4,8,12}_averaged.parquet`\n",
    "- `results/stage1_baseline/performance_metrics_district.csv`\n",
    "- `results/stage1_baseline/threshold_tuning_summary.json`\n",
    "- `figures/stage1/threshold_analysis_h{4,8,12}.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_auc_score,\n",
    "    average_precision_score, accuracy_score,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for config import\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "\n",
    "# Import from config\n",
    "from config import (\n",
    "    BASE_DIR,\n",
    "    STAGE1_DATA_DIR,\n",
    "    STAGE1_RESULTS_DIR,\n",
    "    FIGURES_DIR,\n",
    "    STAGE1_CONFIG,\n",
    "    RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Extract configuration\n",
    "HORIZONS = STAGE1_CONFIG['prediction_horizons']\n",
    "N_FOLDS = STAGE1_CONFIG['cv_folds']\n",
    "THRESHOLD_RANGE = np.arange(0.1, 0.95, 0.05)  # Thresholds to evaluate\n",
    "\n",
    "# Input/output paths\n",
    "INPUT_FILE = STAGE1_DATA_DIR / 'stage1_features.parquet'\n",
    "OUTPUT_DIR = STAGE1_RESULTS_DIR\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Figures subdirectory\n",
    "STAGE1_FIGURES_DIR = FIGURES_DIR / 'stage1'\n",
    "STAGE1_FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {INPUT_FILE}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Figures: {STAGE1_FIGURES_DIR}\")\n",
    "print(f\"Horizons: {HORIZONS}\")\n",
    "print(f\"CV Folds: {N_FOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-functions-header",
   "metadata": {},
   "source": [
    "## Threshold Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-optimal-thresholds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_thresholds(y_true, y_proba, min_metric=0.6):\n",
    "    \"\"\"\n",
    "    Find optimal classification thresholds using multiple strategies.\n",
    "\n",
    "    Returns dict with:\n",
    "    - f1_optimal: threshold maximizing F1 score\n",
    "    - gmean_optimal: threshold maximizing geometric mean of precision & recall\n",
    "    - youden_optimal: threshold maximizing Youden's J (sensitivity + specificity - 1)\n",
    "    - balanced: threshold where precision = recall\n",
    "    - balanced_constrained: threshold where P=R with minimum constraint (default 0.6)\n",
    "    \"\"\"\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_proba)\n",
    "\n",
    "    # F1-optimal threshold\n",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    f1_threshold = pr_thresholds[best_f1_idx]\n",
    "\n",
    "    # Geometric mean optimal\n",
    "    gmean = np.sqrt(precision[:-1] * recall[:-1])\n",
    "    best_gmean_idx = np.argmax(gmean)\n",
    "    gmean_threshold = pr_thresholds[best_gmean_idx]\n",
    "\n",
    "    # Youden's J statistic (from ROC)\n",
    "    youdens_j = tpr - fpr\n",
    "    best_youden_idx = np.argmax(youdens_j)\n",
    "    youden_threshold = roc_thresholds[best_youden_idx]\n",
    "\n",
    "    # Balanced threshold (where precision = recall, unconstrained)\n",
    "    diff = np.abs(precision[:-1] - recall[:-1])\n",
    "    balanced_idx = np.argmin(diff)\n",
    "    balanced_threshold = pr_thresholds[balanced_idx]\n",
    "\n",
    "    # Balanced threshold with minimum constraint (P=R >= min_metric)\n",
    "    # Find indices where both precision and recall >= min_metric\n",
    "    valid_mask = (precision[:-1] >= min_metric) & (recall[:-1] >= min_metric)\n",
    "\n",
    "    if valid_mask.any():\n",
    "        # Among valid points, find where P and R are closest (P = R)\n",
    "        valid_diff = np.where(valid_mask, diff, np.inf)\n",
    "        constrained_idx = np.argmin(valid_diff)\n",
    "        constrained_threshold = pr_thresholds[constrained_idx]\n",
    "        constrained_precision = precision[constrained_idx]\n",
    "        constrained_recall = recall[constrained_idx]\n",
    "        constraint_met = True\n",
    "    else:\n",
    "        # If no threshold meets the constraint, find the best we can do\n",
    "        # Find the threshold that maximizes min(precision, recall)\n",
    "        min_pr = np.minimum(precision[:-1], recall[:-1])\n",
    "        best_min_idx = np.argmax(min_pr)\n",
    "        constrained_threshold = pr_thresholds[best_min_idx]\n",
    "        constrained_precision = precision[best_min_idx]\n",
    "        constrained_recall = recall[best_min_idx]\n",
    "        constraint_met = False\n",
    "\n",
    "    return {\n",
    "        'f1_optimal': {\n",
    "            'threshold': float(f1_threshold),\n",
    "            'f1': float(f1_scores[best_f1_idx]),\n",
    "            'precision': float(precision[best_f1_idx]),\n",
    "            'recall': float(recall[best_f1_idx])\n",
    "        },\n",
    "        'gmean_optimal': {\n",
    "            'threshold': float(gmean_threshold),\n",
    "            'gmean': float(gmean[best_gmean_idx]),\n",
    "            'precision': float(precision[best_gmean_idx]),\n",
    "            'recall': float(recall[best_gmean_idx])\n",
    "        },\n",
    "        'youden_optimal': {\n",
    "            'threshold': float(youden_threshold),\n",
    "            'youden_j': float(youdens_j[best_youden_idx]),\n",
    "            'tpr': float(tpr[best_youden_idx]),\n",
    "            'fpr': float(fpr[best_youden_idx])\n",
    "        },\n",
    "        'balanced': {\n",
    "            'threshold': float(balanced_threshold),\n",
    "            'precision': float(precision[balanced_idx]),\n",
    "            'recall': float(recall[balanced_idx])\n",
    "        },\n",
    "        'balanced_constrained': {\n",
    "            'threshold': float(constrained_threshold),\n",
    "            'precision': float(constrained_precision),\n",
    "            'recall': float(constrained_recall),\n",
    "            'min_constraint': min_metric,\n",
    "            'constraint_met': constraint_met\n",
    "        },\n",
    "        'curves': {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'pr_thresholds': pr_thresholds,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'roc_thresholds': roc_thresholds\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"find_optimal_thresholds defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-at-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_at_threshold(y_true, y_proba, threshold):\n",
    "    \"\"\"Evaluate predictions at a specific threshold.\"\"\"\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    gmean = np.sqrt(precision * recall)\n",
    "\n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1': f1,\n",
    "        'gmean': gmean,\n",
    "        'true_positives': int(tp),\n",
    "        'false_positives': int(fp),\n",
    "        'true_negatives': int(tn),\n",
    "        'false_negatives': int(fn),\n",
    "        'n_predicted_positive': int(tp + fp),\n",
    "        'n_predicted_negative': int(tn + fn)\n",
    "    }\n",
    "\n",
    "print(\"evaluate_at_threshold defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-threshold-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold_analysis(y_true, y_proba, optimal_thresholds, horizon, save_path):\n",
    "    \"\"\"Create comprehensive threshold analysis visualization.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "    precision = optimal_thresholds['curves']['precision']\n",
    "    recall = optimal_thresholds['curves']['recall']\n",
    "    pr_thresholds = optimal_thresholds['curves']['pr_thresholds']\n",
    "    fpr = optimal_thresholds['curves']['fpr']\n",
    "    tpr = optimal_thresholds['curves']['tpr']\n",
    "\n",
    "    # 1. Precision-Recall Curve\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(recall, precision, 'b-', linewidth=2, label='PR Curve')\n",
    "\n",
    "    # Mark optimal thresholds\n",
    "    f1_opt = optimal_thresholds['f1_optimal']\n",
    "    ax1.scatter([f1_opt['recall']], [f1_opt['precision']],\n",
    "                c='orange', s=100, marker='o', zorder=5,\n",
    "                label=f\"F1 Optimal (t={f1_opt['threshold']:.3f})\")\n",
    "\n",
    "    balanced = optimal_thresholds['balanced']\n",
    "    ax1.scatter([balanced['recall']], [balanced['precision']],\n",
    "                c='blue', s=100, marker='s', zorder=5,\n",
    "                label=f\"Balanced (t={balanced['threshold']:.3f})\")\n",
    "\n",
    "    # Balanced constrained (P=R >= 0.6) - PRIMARY\n",
    "    bal_const = optimal_thresholds['balanced_constrained']\n",
    "    marker_label = f\"P=R>={bal_const['min_constraint']:.1f} (t={bal_const['threshold']:.3f})\"\n",
    "    if not bal_const['constraint_met']:\n",
    "        marker_label += \" [!]\"\n",
    "    ax1.scatter([bal_const['recall']], [bal_const['precision']],\n",
    "                c='red', s=200, marker='*', zorder=6,\n",
    "                label=marker_label)\n",
    "\n",
    "    # Draw horizontal/vertical lines at 0.6 constraint\n",
    "    ax1.axhline(y=0.6, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax1.axvline(x=0.6, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "    ax1.set_xlabel('Recall', fontsize=12)\n",
    "    ax1.set_ylabel('Precision', fontsize=12)\n",
    "    ax1.set_title(f'Precision-Recall Curve (h={horizon})', fontsize=14)\n",
    "    ax1.legend(loc='lower left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim([0, 1.02])\n",
    "    ax1.set_ylim([0, 1.02])\n",
    "\n",
    "    # 2. ROC Curve\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(fpr, tpr, 'b-', linewidth=2, label='ROC Curve')\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "\n",
    "    youden = optimal_thresholds['youden_optimal']\n",
    "    ax2.scatter([youden['fpr']], [youden['tpr']],\n",
    "                c='purple', s=150, marker='*', zorder=5,\n",
    "                label=f\"Youden (t={youden['threshold']:.3f})\")\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_proba)\n",
    "    ax2.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax2.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax2.set_title(f'ROC Curve (AUC={auc_roc:.3f})', fontsize=14)\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Metrics vs Threshold\n",
    "    ax3 = axes[1, 0]\n",
    "    thresholds = np.arange(0.05, 0.95, 0.01)\n",
    "    metrics_at_thresh = [evaluate_at_threshold(y_true, y_proba, t) for t in thresholds]\n",
    "\n",
    "    prec_vals = [m['precision'] for m in metrics_at_thresh]\n",
    "    rec_vals = [m['recall'] for m in metrics_at_thresh]\n",
    "    f1_vals = [m['f1'] for m in metrics_at_thresh]\n",
    "    spec_vals = [m['specificity'] for m in metrics_at_thresh]\n",
    "\n",
    "    ax3.plot(thresholds, prec_vals, 'b-', linewidth=2, label='Precision')\n",
    "    ax3.plot(thresholds, rec_vals, 'r-', linewidth=2, label='Recall')\n",
    "    ax3.plot(thresholds, f1_vals, 'g-', linewidth=2, label='F1 Score')\n",
    "    ax3.plot(thresholds, spec_vals, 'm--', linewidth=1.5, label='Specificity')\n",
    "\n",
    "    # Mark optimal threshold (balanced constrained)\n",
    "    ax3.axvline(x=bal_const['threshold'], color='red', linestyle=':', linewidth=2, alpha=0.8,\n",
    "                label=f\"Balanced P=R (t={bal_const['threshold']:.3f})\")\n",
    "    ax3.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Default (0.5)')\n",
    "    ax3.axhline(y=0.6, color='gray', linestyle='--', alpha=0.3)  # Min constraint line\n",
    "\n",
    "    ax3.set_xlabel('Threshold', fontsize=12)\n",
    "    ax3.set_ylabel('Score', fontsize=12)\n",
    "    ax3.set_title('Metrics vs Classification Threshold', fontsize=14)\n",
    "    ax3.legend(loc='lower left')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xlim([0, 1])\n",
    "    ax3.set_ylim([0, 1.02])\n",
    "\n",
    "    # 4. Confusion Matrix Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "\n",
    "    # Compare default (0.5) vs balanced constrained threshold\n",
    "    metrics_default = evaluate_at_threshold(y_true, y_proba, 0.5)\n",
    "    metrics_optimal = evaluate_at_threshold(y_true, y_proba, bal_const['threshold'])\n",
    "\n",
    "    labels = ['Threshold', 'Precision', 'Recall', 'F1', 'FP (x1000)', 'FN (x1000)']\n",
    "    default_vals = [0.5, metrics_default['precision'], metrics_default['recall'],\n",
    "                    metrics_default['f1'], metrics_default['false_positives']/1000,\n",
    "                    metrics_default['false_negatives']/1000]\n",
    "    optimal_vals = [bal_const['threshold'], metrics_optimal['precision'], metrics_optimal['recall'],\n",
    "                    metrics_optimal['f1'], metrics_optimal['false_positives']/1000,\n",
    "                    metrics_optimal['false_negatives']/1000]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax4.bar(x - width/2, default_vals, width, label='Default (0.5)', color='lightcoral')\n",
    "    bars2 = ax4.bar(x + width/2, optimal_vals, width, label='Balanced P=R', color='lightgreen')\n",
    "\n",
    "    ax4.set_ylabel('Value', fontsize=12)\n",
    "    ax4.set_title('Default vs Optimal Threshold Comparison', fontsize=14)\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(labels, rotation=15, ha='right')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax4.annotate(f'{height:.2f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3), textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return fig\n",
    "\n",
    "print(\"plot_threshold_analysis defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv-functions-header",
   "metadata": {},
   "source": [
    "## Cross-Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-spatial-folds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spatial_folds(df, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create spatially separated folds using K-means clustering on DISTRICT coordinates.\n",
    "    Prevents spatial autocorrelation in train/test splits.\n",
    "\n",
    "    KEY CHANGE: Uses ipc_geographic_unit_full as district identifier\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating {n_folds} spatial folds at DISTRICT level...\")\n",
    "\n",
    "    # Get unique districts with their coordinates\n",
    "    districts = df[['ipc_geographic_unit_full', 'ipc_district', 'ipc_country',\n",
    "                    'avg_latitude', 'avg_longitude']].drop_duplicates()\n",
    "    districts = districts.dropna(subset=['avg_latitude', 'avg_longitude'])\n",
    "\n",
    "    print(f\"   Unique districts with coordinates: {len(districts)}\")\n",
    "\n",
    "    # Cluster districts into spatially separated groups\n",
    "    coords = districts[['avg_latitude', 'avg_longitude']].values\n",
    "    kmeans = KMeans(n_clusters=n_folds, random_state=random_state, n_init=10)\n",
    "    districts['fold'] = kmeans.fit_predict(coords)\n",
    "\n",
    "    # Map folds back to full dataset\n",
    "    fold_map = dict(zip(districts['ipc_geographic_unit_full'], districts['fold']))\n",
    "    df['fold'] = df['ipc_geographic_unit_full'].map(fold_map)\n",
    "\n",
    "    # Print fold statistics\n",
    "    print(f\"   Fold distribution:\")\n",
    "    for fold in range(n_folds):\n",
    "        n_districts = (districts['fold'] == fold).sum()\n",
    "        n_obs = (df['fold'] == fold).sum()\n",
    "        print(f\"      Fold {fold}: {n_districts} districts, {n_obs:,} observations\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"create_spatial_folds defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-and-predict-fold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_fold(X_train, y_train, X_test, fold_idx, horizon):\n",
    "    \"\"\"Train logistic regression and predict on test fold\"\"\"\n",
    "    model = LogisticRegression(\n",
    "        penalty='l2',\n",
    "        C=1.0,\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    coef_dict = {\n",
    "        'intercept': model.intercept_[0],\n",
    "        'coef_Lt': model.coef_[0][0],\n",
    "        'coef_Ls': model.coef_[0][1]\n",
    "    }\n",
    "\n",
    "    return y_pred_proba, y_pred, coef_dict, model\n",
    "\n",
    "print(\"train_and_predict_fold defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred, y_pred_proba, horizon, fold_idx=None):\n",
    "    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    auc_pr = average_precision_score(y_true, y_pred_proba)\n",
    "\n",
    "    metrics = {\n",
    "        'horizon': horizon,\n",
    "        'fold': fold_idx,\n",
    "        'n_samples': len(y_true),\n",
    "        'n_crisis': int(y_true.sum()),\n",
    "        'n_no_crisis': int((1 - y_true).sum()),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc_roc': auc_roc,\n",
    "        'auc_pr': auc_pr,\n",
    "        'true_negatives': int(tn),\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'true_positives': int(tp)\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"evaluate_predictions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process-horizon-header",
   "metadata": {},
   "source": [
    "## Horizon Processing Function\n",
    "\n",
    "Main workflow for processing one prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-horizon-part1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_horizon(df, horizon):\n",
    "    \"\"\"Process one prediction horizon with spatial CV\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing Horizon h={horizon} months - DISTRICT LEVEL\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    target_col = f'y_h{horizon}'\n",
    "\n",
    "    # Filter to complete cases\n",
    "    required_cols = ['ipc_id', 'ipc_country', 'ipc_district', 'ipc_geographic_unit_full',\n",
    "                     'ipc_period_start', 'ipc_period_end', 'ipc_value',\n",
    "                     'avg_latitude', 'avg_longitude',\n",
    "                     'Lt', 'Ls', target_col, 'fold']\n",
    "\n",
    "    df_complete = df[required_cols].copy()\n",
    "    df_complete = df_complete.dropna(subset=['Lt', 'Ls', target_col])\n",
    "\n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"   Total observations: {len(df_complete):,}\")\n",
    "    print(f\"   Unique districts: {df_complete['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"   Crisis cases (y=1): {df_complete[target_col].sum():,} ({df_complete[target_col].mean()*100:.1f}%)\")\n",
    "\n",
    "    # Prepare features\n",
    "    feature_cols = ['Lt', 'Ls']\n",
    "    X = df_complete[feature_cols].values\n",
    "    y = df_complete[target_col].values\n",
    "\n",
    "    predictions_list = []\n",
    "    metrics_list = []\n",
    "    coefficients_list = []\n",
    "\n",
    "    # Spatial Cross-Validation\n",
    "    print(f\"\\nRunning {N_FOLDS}-fold Spatial Cross-Validation...\")\n",
    "\n",
    "    for fold in range(N_FOLDS):\n",
    "        print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
    "\n",
    "        train_mask = df_complete['fold'] != fold\n",
    "        test_mask = df_complete['fold'] == fold\n",
    "\n",
    "        X_train, X_test = X[train_mask], X[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "        print(f\"   Training: {len(X_train):,} samples (crisis: {y_train.sum():,})\")\n",
    "        print(f\"   Test: {len(X_test):,} samples (crisis: {y_test.sum():,})\")\n",
    "\n",
    "        # Train and predict\n",
    "        y_pred_proba, y_pred, coef_dict, model = train_and_predict_fold(\n",
    "            X_train, y_train, X_test, fold, horizon\n",
    "        )\n",
    "\n",
    "        # Store coefficients\n",
    "        coef_dict['fold'] = fold\n",
    "        coef_dict['horizon'] = horizon\n",
    "        coefficients_list.append(coef_dict)\n",
    "\n",
    "        # Evaluate\n",
    "        fold_metrics = evaluate_predictions(y_test, y_pred, y_pred_proba, horizon, fold)\n",
    "        metrics_list.append(fold_metrics)\n",
    "\n",
    "        print(f\"   Accuracy: {fold_metrics['accuracy']:.3f}\")\n",
    "        print(f\"   Precision: {fold_metrics['precision']:.3f}\")\n",
    "        print(f\"   Recall: {fold_metrics['recall']:.3f}\")\n",
    "        print(f\"   AUC-ROC: {fold_metrics['auc_roc']:.3f}\")\n",
    "\n",
    "        # Store predictions\n",
    "        test_df = df_complete[test_mask].copy()\n",
    "        test_df['ipc_future_crisis'] = y_test  # Target column (standardized name)\n",
    "        test_df['pred_prob'] = y_pred_proba  # STANDARDIZED: prediction probability\n",
    "        test_df['y_pred'] = y_pred\n",
    "        test_df['fold'] = fold\n",
    "        test_df['horizon'] = horizon\n",
    "        test_df['correct'] = (y_test == y_pred).astype(int)\n",
    "        test_df['ar_failure'] = ((y_pred == 0) & (y_test == 1)).astype(int)\n",
    "        test_df['false_alarm'] = ((y_pred == 1) & (y_test == 0)).astype(int)\n",
    "\n",
    "        predictions_list.append(test_df)\n",
    "\n",
    "    # Combine predictions\n",
    "    all_predictions = pd.concat(predictions_list, ignore_index=True)\n",
    "\n",
    "    # Average predictions per district-period\n",
    "    avg_predictions = all_predictions.groupby('ipc_id').agg({\n",
    "        'ipc_country': 'first',\n",
    "        'ipc_district': 'first',\n",
    "        'ipc_geographic_unit_full': 'first',\n",
    "        'ipc_period_start': 'first',\n",
    "        'ipc_period_end': 'first',\n",
    "        'ipc_value': 'first',\n",
    "        'avg_latitude': 'first',\n",
    "        'avg_longitude': 'first',\n",
    "        'Lt': 'first',\n",
    "        'Ls': 'first',\n",
    "        'ipc_future_crisis': 'first',  # Target (standardized name)\n",
    "        'pred_prob': 'mean',  # STANDARDIZED: prediction probability\n",
    "        'y_pred': lambda x: (x.mean() >= 0.5).astype(int),\n",
    "        'fold': 'first',\n",
    "        'horizon': 'first',\n",
    "        'correct': lambda x: (x.mean() >= 0.5).astype(int),\n",
    "        'ar_failure': lambda x: (x.mean() >= 0.5).astype(int),\n",
    "        'false_alarm': lambda x: (x.mean() >= 0.5).astype(int)\n",
    "    }).reset_index()\n",
    "\n",
    "    return all_predictions, avg_predictions, metrics_list, coefficients_list\n",
    "\n",
    "print(\"process_horizon part 1 defined (CV loop)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-horizon-part2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_horizon_threshold_tuning(avg_predictions, all_predictions, metrics_list, coefficients_list, horizon):\n",
    "    \"\"\"Part 2: Threshold tuning and saving results\"\"\"\n",
    "    \n",
    "    # THRESHOLD TUNING\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"THRESHOLD TUNING (h={horizon} months)\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    y_true = avg_predictions['ipc_future_crisis'].values\n",
    "    y_proba = avg_predictions['pred_prob'].values\n",
    "\n",
    "    # Find optimal thresholds\n",
    "    optimal_thresholds = find_optimal_thresholds(y_true, y_proba)\n",
    "\n",
    "    print(f\"\\n--- Optimal Threshold Strategies ---\")\n",
    "    print(f\"\\n1. F1-Optimal Threshold: {optimal_thresholds['f1_optimal']['threshold']:.3f}\")\n",
    "    print(f\"   Precision: {optimal_thresholds['f1_optimal']['precision']:.3f}\")\n",
    "    print(f\"   Recall: {optimal_thresholds['f1_optimal']['recall']:.3f}\")\n",
    "    print(f\"   F1: {optimal_thresholds['f1_optimal']['f1']:.3f}\")\n",
    "\n",
    "    print(f\"\\n2. Geometric Mean Threshold: {optimal_thresholds['gmean_optimal']['threshold']:.3f}\")\n",
    "    print(f\"   Precision: {optimal_thresholds['gmean_optimal']['precision']:.3f}\")\n",
    "    print(f\"   Recall: {optimal_thresholds['gmean_optimal']['recall']:.3f}\")\n",
    "\n",
    "    print(f\"\\n3. Youden's J Threshold: {optimal_thresholds['youden_optimal']['threshold']:.3f}\")\n",
    "    print(f\"   TPR: {optimal_thresholds['youden_optimal']['tpr']:.3f}\")\n",
    "    print(f\"   FPR: {optimal_thresholds['youden_optimal']['fpr']:.3f}\")\n",
    "\n",
    "    print(f\"\\n4. Balanced (P=R) Threshold: {optimal_thresholds['balanced']['threshold']:.3f}\")\n",
    "    print(f\"   Precision: {optimal_thresholds['balanced']['precision']:.3f}\")\n",
    "    print(f\"   Recall: {optimal_thresholds['balanced']['recall']:.3f}\")\n",
    "\n",
    "    bal_const = optimal_thresholds['balanced_constrained']\n",
    "    constraint_status = \"MET\" if bal_const['constraint_met'] else \"NOT MET\"\n",
    "    print(f\"\\n5. BALANCED CONSTRAINED (P=R >= {bal_const['min_constraint']:.1f}) - [{constraint_status}]\")\n",
    "    print(f\"   Threshold: {bal_const['threshold']:.3f}\")\n",
    "    print(f\"   Precision: {bal_const['precision']:.3f}\")\n",
    "    print(f\"   Recall: {bal_const['recall']:.3f}\")\n",
    "\n",
    "    # Evaluate at multiple thresholds\n",
    "    print(f\"\\n--- Performance at Various Thresholds ---\")\n",
    "    threshold_results = []\n",
    "    for thresh in THRESHOLD_RANGE:\n",
    "        metrics = evaluate_at_threshold(y_true, y_proba, thresh)\n",
    "        metrics['horizon'] = horizon\n",
    "        threshold_results.append(metrics)\n",
    "        if thresh in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "            print(f\"   t={thresh:.1f}: Prec={metrics['precision']:.3f}, \"\n",
    "                  f\"Rec={metrics['recall']:.3f}, F1={metrics['f1']:.3f}, \"\n",
    "                  f\"FP={metrics['false_positives']}, FN={metrics['false_negatives']}\")\n",
    "\n",
    "    # Save threshold analysis\n",
    "    thresh_df = pd.DataFrame(threshold_results)\n",
    "    thresh_file = OUTPUT_DIR / f'threshold_analysis_h{horizon}.csv'\n",
    "    thresh_df.to_csv(thresh_file, index=False)\n",
    "    print(f\"\\n   Saved: {thresh_file}\")\n",
    "\n",
    "    # Generate threshold analysis visualization\n",
    "    fig_path = STAGE1_FIGURES_DIR / f'threshold_analysis_h{horizon}.png'\n",
    "    plot_threshold_analysis(y_true, y_proba, optimal_thresholds, horizon, fig_path)\n",
    "    print(f\"   Saved: {fig_path}\")\n",
    "\n",
    "    # Apply optimal threshold (Balanced Constrained: P=R >= 0.6)\n",
    "    optimal_thresh = optimal_thresholds['balanced_constrained']['threshold']\n",
    "    avg_predictions['y_pred_optimal'] = (avg_predictions['pred_prob'] >= optimal_thresh).astype(int)\n",
    "    avg_predictions['ar_failure_optimal'] = (\n",
    "        (avg_predictions['y_pred_optimal'] == 0) & (avg_predictions['ipc_future_crisis'] == 1)\n",
    "    ).astype(int)\n",
    "    avg_predictions['false_alarm_optimal'] = (\n",
    "        (avg_predictions['y_pred_optimal'] == 1) & (avg_predictions['ipc_future_crisis'] == 0)\n",
    "    ).astype(int)\n",
    "    avg_predictions['optimal_threshold'] = optimal_thresh\n",
    "    avg_predictions['constraint_met'] = optimal_thresholds['balanced_constrained']['constraint_met']\n",
    "\n",
    "    # Overall metrics at default threshold (0.5) - for reference only\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Overall Performance at DEFAULT Threshold (0.5) - Reference\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    default_metrics = evaluate_predictions(\n",
    "        avg_predictions['ipc_future_crisis'].values,\n",
    "        avg_predictions['y_pred'].values,\n",
    "        avg_predictions['pred_prob'].values,\n",
    "        horizon,\n",
    "        fold_idx=None\n",
    "    )\n",
    "\n",
    "    print(f\"   Total districts: {avg_predictions['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"   Accuracy: {default_metrics['accuracy']:.3f}\")\n",
    "    print(f\"   Precision: {default_metrics['precision']:.3f}\")\n",
    "    print(f\"   Recall: {default_metrics['recall']:.3f}\")\n",
    "    print(f\"   F1 Score: {default_metrics['f1']:.3f}\")\n",
    "    print(f\"   AUC-ROC: {default_metrics['auc_roc']:.3f}\")\n",
    "    print(f\"   AUC-PR: {default_metrics['auc_pr']:.3f}\")\n",
    "\n",
    "    # AR Failures at default\n",
    "    n_ar_failures_default = avg_predictions['ar_failure'].sum()\n",
    "    n_crisis = int(avg_predictions['ipc_future_crisis'].sum())\n",
    "    ar_pct_default = (n_ar_failures_default/n_crisis*100) if n_crisis > 0 else 0\n",
    "    print(f\"\\n   AR Failures (missed crises): {n_ar_failures_default}/{n_crisis} ({ar_pct_default:.1f}%)\")\n",
    "    print(f\"   False Alarms: {avg_predictions['false_alarm'].sum()}\")\n",
    "\n",
    "    # Overall metrics at OPTIMAL threshold (Balanced Constrained)\n",
    "    constraint_status = \"MET\" if optimal_thresholds['balanced_constrained']['constraint_met'] else \"NOT MET\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Overall Performance at BALANCED P=R Threshold ({optimal_thresh:.3f}) [Constraint {constraint_status}]\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    optimal_overall = evaluate_at_threshold(y_true, y_proba, optimal_thresh)\n",
    "    print(f\"   Accuracy: {optimal_overall['accuracy']:.3f}\")\n",
    "    print(f\"   Precision: {optimal_overall['precision']:.3f}\")\n",
    "    print(f\"   Recall: {optimal_overall['recall']:.3f}\")\n",
    "    print(f\"   F1 Score: {optimal_overall['f1']:.3f}\")\n",
    "    print(f\"   Specificity: {optimal_overall['specificity']:.3f}\")\n",
    "\n",
    "    n_ar_failures_opt = avg_predictions['ar_failure_optimal'].sum()\n",
    "    ar_pct_opt = (n_ar_failures_opt/n_crisis*100) if n_crisis > 0 else 0\n",
    "    print(f\"\\n   AR Failures (missed crises): {n_ar_failures_opt}/{n_crisis} ({ar_pct_opt:.1f}%)\")\n",
    "    print(f\"   False Alarms: {avg_predictions['false_alarm_optimal'].sum()}\")\n",
    "\n",
    "    # Improvement summary\n",
    "    print(f\"\\n--- Improvement Summary (Default -> Optimal) ---\")\n",
    "    print(f\"   Precision: {default_metrics['precision']:.3f} -> {optimal_overall['precision']:.3f} \"\n",
    "          f\"({(optimal_overall['precision']-default_metrics['precision'])*100:+.1f}%)\")\n",
    "    print(f\"   Recall: {default_metrics['recall']:.3f} -> {optimal_overall['recall']:.3f} \"\n",
    "          f\"({(optimal_overall['recall']-default_metrics['recall'])*100:+.1f}%)\")\n",
    "    print(f\"   F1: {default_metrics['f1']:.3f} -> {optimal_overall['f1']:.3f} \"\n",
    "          f\"({(optimal_overall['f1']-default_metrics['f1'])*100:+.1f}%)\")\n",
    "    print(f\"   False Alarms: {default_metrics['false_positives']} -> {optimal_overall['false_positives']} \"\n",
    "          f\"({optimal_overall['false_positives']-default_metrics['false_positives']:+d})\")\n",
    "    print(f\"   Missed Crises: {default_metrics['false_negatives']} -> {optimal_overall['false_negatives']} \"\n",
    "          f\"({optimal_overall['false_negatives']-default_metrics['false_negatives']:+d})\")\n",
    "\n",
    "    # Create overall_metrics using OPTIMAL threshold for saving to CSV\n",
    "    # This ensures all downstream scripts use optimal threshold metrics\n",
    "    overall_metrics = evaluate_predictions(\n",
    "        avg_predictions['ipc_future_crisis'].values,\n",
    "        avg_predictions['y_pred_optimal'].values,\n",
    "        avg_predictions['pred_prob'].values,\n",
    "        horizon,\n",
    "        fold_idx=None\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    print(f\"\\nSaving results...\")\n",
    "\n",
    "    # Detailed predictions\n",
    "    pred_file = OUTPUT_DIR / f'predictions_h{horizon}_folds.csv'\n",
    "    all_predictions.to_csv(pred_file, index=False)\n",
    "    print(f\"   Saved: {pred_file}\")\n",
    "\n",
    "    # Averaged predictions (includes both default and optimal threshold predictions)\n",
    "    # STANDARDIZED FILENAME: predictions_h{horizon}_averaged.csv\n",
    "    avg_file = OUTPUT_DIR / f'predictions_h{horizon}_averaged.csv'\n",
    "    avg_predictions.to_csv(avg_file, index=False)\n",
    "    print(f\"   Saved: {avg_file}\")\n",
    "\n",
    "    # Parquet\n",
    "    parquet_file = OUTPUT_DIR / f'predictions_h{horizon}_averaged.parquet'\n",
    "    avg_predictions.to_parquet(parquet_file, index=False)\n",
    "    print(f\"   Saved: {parquet_file}\")\n",
    "\n",
    "    # AR failures at default threshold (0.5)\n",
    "    ar_failures = avg_predictions[avg_predictions['ar_failure'] == 1].copy()\n",
    "    ar_file = OUTPUT_DIR / f'ar_failures_h{horizon}_district.csv'\n",
    "    ar_failures.to_csv(ar_file, index=False)\n",
    "    print(f\"   Saved: {ar_file} ({len(ar_failures)} failures at t=0.5)\")\n",
    "\n",
    "    # AR failures at optimal threshold\n",
    "    ar_failures_opt = avg_predictions[avg_predictions['ar_failure_optimal'] == 1].copy()\n",
    "    ar_opt_file = OUTPUT_DIR / f'ar_failures_h{horizon}_district_optimal.csv'\n",
    "    ar_failures_opt.to_csv(ar_opt_file, index=False)\n",
    "    print(f\"   Saved: {ar_opt_file} ({len(ar_failures_opt)} failures at t={optimal_thresh:.3f})\")\n",
    "\n",
    "    # Save optimal threshold summary\n",
    "    threshold_summary = {\n",
    "        'horizon': horizon,\n",
    "        'default_threshold': 0.5,\n",
    "        'optimal_threshold': optimal_thresh,\n",
    "        'optimal_strategy': 'balanced_constrained',\n",
    "        'constraint_met': optimal_thresholds['balanced_constrained']['constraint_met'],\n",
    "        'min_constraint': optimal_thresholds['balanced_constrained']['min_constraint'],\n",
    "        'f1_optimal': optimal_thresholds['f1_optimal'],\n",
    "        'gmean_optimal': optimal_thresholds['gmean_optimal'],\n",
    "        'youden_optimal': optimal_thresholds['youden_optimal'],\n",
    "        'balanced': optimal_thresholds['balanced'],\n",
    "        'balanced_constrained': optimal_thresholds['balanced_constrained'],\n",
    "        'default_metrics': {\n",
    "            'precision': default_metrics['precision'],\n",
    "            'recall': default_metrics['recall'],\n",
    "            'f1': default_metrics['f1'],\n",
    "            'false_positives': default_metrics['false_positives'],\n",
    "            'false_negatives': default_metrics['false_negatives']\n",
    "        },\n",
    "        'optimal_metrics': {\n",
    "            'precision': optimal_overall['precision'],\n",
    "            'recall': optimal_overall['recall'],\n",
    "            'f1': optimal_overall['f1'],\n",
    "            'false_positives': optimal_overall['false_positives'],\n",
    "            'false_negatives': optimal_overall['false_negatives']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return avg_predictions, metrics_list, coefficients_list, overall_metrics, threshold_summary\n",
    "\n",
    "print(\"process_horizon part 2 defined (threshold tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-header",
   "metadata": {},
   "source": [
    "## Main Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Stage 1: Logistic Regression Baseline - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Start time: {datetime.now()}\\n\")\n",
    "\n",
    "# Load data\n",
    "print(\"1. Loading feature-engineered district dataset...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "print(f\"   Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "print(f\"   Unique districts: {df['ipc_geographic_unit_full'].nunique():,}\")\n",
    "\n",
    "# Convert dates\n",
    "df['ipc_period_start'] = pd.to_datetime(df['ipc_period_start'])\n",
    "df['ipc_period_end'] = pd.to_datetime(df['ipc_period_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-folds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial folds\n",
    "df = create_spatial_folds(df, n_folds=N_FOLDS, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-horizons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each horizon\n",
    "all_metrics = []\n",
    "all_coefficients = []\n",
    "all_threshold_summaries = []\n",
    "\n",
    "for horizon in HORIZONS:\n",
    "    # Part 1: CV loop\n",
    "    all_preds, avg_preds, metrics, coeffs = process_horizon(df, horizon)\n",
    "    \n",
    "    # Part 2: Threshold tuning\n",
    "    predictions, metrics, coefficients, overall, thresh_summary = process_horizon_threshold_tuning(\n",
    "        avg_preds, all_preds, metrics, coeffs, horizon\n",
    "    )\n",
    "    \n",
    "    all_metrics.extend(metrics)\n",
    "    all_coefficients.extend(coefficients)\n",
    "    all_metrics.append(overall)\n",
    "    all_threshold_summaries.append(thresh_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-summary-header",
   "metadata": {},
   "source": [
    "## Save Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Saving Summary Statistics\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "metrics_file = OUTPUT_DIR / 'performance_metrics_district.csv'\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"   Saved: {metrics_file}\")\n",
    "\n",
    "coefficients_df = pd.DataFrame(all_coefficients)\n",
    "coef_file = OUTPUT_DIR / 'model_coefficients_district.csv'\n",
    "coefficients_df.to_csv(coef_file, index=False)\n",
    "print(f\"   Saved: {coef_file}\")\n",
    "\n",
    "# Save threshold summaries\n",
    "import json\n",
    "thresh_summary_file = OUTPUT_DIR / 'threshold_tuning_summary.json'\n",
    "with open(thresh_summary_file, 'w') as f:\n",
    "    # Convert to serializable format\n",
    "    serializable_summaries = []\n",
    "    for s in all_threshold_summaries:\n",
    "        summary = {k: v for k, v in s.items() if k not in ['f1_optimal', 'gmean_optimal', 'youden_optimal', 'balanced']}\n",
    "        summary['f1_optimal'] = s['f1_optimal']\n",
    "        summary['gmean_optimal'] = s['gmean_optimal']\n",
    "        summary['youden_optimal'] = s['youden_optimal']\n",
    "        summary['balanced'] = s['balanced']\n",
    "        summary['default_metrics'] = s['default_metrics']\n",
    "        summary['optimal_metrics'] = s['optimal_metrics']\n",
    "        serializable_summaries.append(summary)\n",
    "    json.dump(serializable_summaries, f, indent=2)\n",
    "print(f\"   Saved: {thresh_summary_file}\")\n",
    "\n",
    "# Create summary DataFrame for threshold results\n",
    "thresh_rows = []\n",
    "for s in all_threshold_summaries:\n",
    "    thresh_rows.append({\n",
    "        'horizon': s['horizon'],\n",
    "        'default_threshold': 0.5,\n",
    "        'optimal_threshold': s['optimal_threshold'],\n",
    "        'default_precision': s['default_metrics']['precision'],\n",
    "        'default_recall': s['default_metrics']['recall'],\n",
    "        'default_f1': s['default_metrics']['f1'],\n",
    "        'default_fp': s['default_metrics']['false_positives'],\n",
    "        'default_fn': s['default_metrics']['false_negatives'],\n",
    "        'optimal_precision': s['optimal_metrics']['precision'],\n",
    "        'optimal_recall': s['optimal_metrics']['recall'],\n",
    "        'optimal_f1': s['optimal_metrics']['f1'],\n",
    "        'optimal_fp': s['optimal_metrics']['false_positives'],\n",
    "        'optimal_fn': s['optimal_metrics']['false_negatives'],\n",
    "        'precision_gain': s['optimal_metrics']['precision'] - s['default_metrics']['precision'],\n",
    "        'recall_change': s['optimal_metrics']['recall'] - s['default_metrics']['recall'],\n",
    "        'f1_gain': s['optimal_metrics']['f1'] - s['default_metrics']['f1'],\n",
    "        'fp_reduction': s['default_metrics']['false_positives'] - s['optimal_metrics']['false_positives'],\n",
    "    })\n",
    "thresh_summary_df = pd.DataFrame(thresh_rows)\n",
    "thresh_csv = OUTPUT_DIR / 'threshold_tuning_comparison.csv'\n",
    "thresh_summary_df.to_csv(thresh_csv, index=False)\n",
    "print(f\"   Saved: {thresh_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-summary-header",
   "metadata": {},
   "source": [
    "## Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary report\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Summary Report - DISTRICT LEVEL WITH THRESHOLD TUNING\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for i, horizon in enumerate(HORIZONS):\n",
    "    horizon_metrics = metrics_df[\n",
    "        (metrics_df['horizon'] == horizon) &\n",
    "        (metrics_df['fold'].isna())\n",
    "    ].iloc[0]\n",
    "\n",
    "    thresh = all_threshold_summaries[i]\n",
    "\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Horizon h={horizon} months\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"   Samples: {horizon_metrics['n_samples']:,}\")\n",
    "    print(f\"   AUC-ROC: {horizon_metrics['auc_roc']:.3f}\")\n",
    "    print(f\"   AUC-PR: {horizon_metrics['auc_pr']:.3f}\")\n",
    "\n",
    "    print(f\"\\n   --- Default (t=0.5) ---\")\n",
    "    print(f\"   Precision: {horizon_metrics['precision']:.3f}\")\n",
    "    print(f\"   Recall: {horizon_metrics['recall']:.3f}\")\n",
    "    print(f\"   F1 Score: {horizon_metrics['f1']:.3f}\")\n",
    "    print(f\"   False Positives: {thresh['default_metrics']['false_positives']}\")\n",
    "    print(f\"   False Negatives: {thresh['default_metrics']['false_negatives']}\")\n",
    "\n",
    "    constraint_status = \"MET\" if thresh['constraint_met'] else \"NOT MET\"\n",
    "    print(f\"\\n   --- Balanced P=R (t={thresh['optimal_threshold']:.3f}) [Constraint {constraint_status}] ---\")\n",
    "    print(f\"   Precision: {thresh['optimal_metrics']['precision']:.3f} ({(thresh['optimal_metrics']['precision']-horizon_metrics['precision'])*100:+.1f}%)\")\n",
    "    print(f\"   Recall: {thresh['optimal_metrics']['recall']:.3f} ({(thresh['optimal_metrics']['recall']-horizon_metrics['recall'])*100:+.1f}%)\")\n",
    "    print(f\"   F1 Score: {thresh['optimal_metrics']['f1']:.3f} ({(thresh['optimal_metrics']['f1']-horizon_metrics['f1'])*100:+.1f}%)\")\n",
    "    print(f\"   False Positives: {thresh['optimal_metrics']['false_positives']} ({thresh['optimal_metrics']['false_positives']-thresh['default_metrics']['false_positives']:+d})\")\n",
    "    print(f\"   False Negatives: {thresh['optimal_metrics']['false_negatives']} ({thresh['optimal_metrics']['false_negatives']-thresh['default_metrics']['false_negatives']:+d})\")\n",
    "\n",
    "    avg_coefs = coefficients_df[coefficients_df['horizon'] == horizon].mean(numeric_only=True)\n",
    "    print(f\"\\n   Average Coefficients:\")\n",
    "    print(f\"      Intercept: {avg_coefs['intercept']:.3f}\")\n",
    "    print(f\"      Lt (temporal): {avg_coefs['coef_Lt']:.3f}\")\n",
    "    print(f\"      Ls (spatial): {avg_coefs['coef_Ls']:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"THRESHOLD TUNING SUMMARY (Balanced P=R >= 0.6)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n{'Horizon':<8} {'Thresh':<8} {'Status':<10} {'P=R':<8} {'F1':<8} {'FP Chg':<10} {'FN Chg':<10}\")\n",
    "print(f\"{'-'*70}\")\n",
    "for s in all_threshold_summaries:\n",
    "    status = \"OK\" if s['constraint_met'] else \"BELOW\"\n",
    "    prec = s['optimal_metrics']['precision']\n",
    "    f1 = s['optimal_metrics']['f1']\n",
    "    fp_chg = s['optimal_metrics']['false_positives'] - s['default_metrics']['false_positives']\n",
    "    fn_chg = s['optimal_metrics']['false_negatives'] - s['default_metrics']['false_negatives']\n",
    "    print(f\"h={s['horizon']:<5} {s['optimal_threshold']:.3f}{'':>3} {status:<10} {prec:.3f}{'':>3} {f1:.3f}{'':>3} {fp_chg:+d}{'':>5} {fn_chg:+d}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Stage 1 Baseline Complete - WITH THRESHOLD TUNING\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nEnd time: {datetime.now()}\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Figures saved to: {STAGE1_FIGURES_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
