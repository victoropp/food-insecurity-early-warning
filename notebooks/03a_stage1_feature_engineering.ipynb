{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# Stage 1: Autoregressive Feature Engineering - District Level\n",
    "\n",
    "**Script**: `scripts/03_stage1_baseline/06_stage1_feature_engineering.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Creates autoregressive baseline features for Stage 1 AR model:\n",
    "- **Lt (temporal lag)**: Previous period IPC value for same district\n",
    "- **Ls (spatial lag)**: Weighted average of neighbor IPC values at same time\n",
    "- **Target variables**: y_h4, y_h8, y_h12 (crisis 4, 8, 12 months ahead)\n",
    "\n",
    "**Spatial weights**: Inverse distance weighting within 300km radius\n",
    "\n",
    "**Runtime**: ~30 minutes\n",
    "\n",
    "**Input**: `data/district_level/ml_dataset_deduplicated.parquet` (from 02e)\n",
    "\n",
    "**Output**: \n",
    "- `data/district_level/stage1_features.parquet`\n",
    "- `data/district_level/spatial_weights.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add parent directory to path for config import\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "\n",
    "# Import from config\n",
    "from config import (\n",
    "    BASE_DIR,\n",
    "    STAGE1_DATA_DIR,\n",
    "    STAGE1_RESULTS_DIR,\n",
    "    STAGE2_FEATURES_DIR,\n",
    "    STAGE2_MODELS_DIR,\n",
    "    FIGURES_DIR,\n",
    "    RANDOM_STATE\n",
    ")\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# District pipeline I/O (district_level subfolder)\n",
    "DISTRICT_DATA_DIR = BASE_DIR / 'data' / 'district_level'\n",
    "INPUT_FILE = DISTRICT_DATA_DIR / 'ml_dataset_deduplicated.parquet'\n",
    "OUTPUT_FILE = DISTRICT_DATA_DIR / 'stage1_features.parquet'\n",
    "OUTPUT_CSV = DISTRICT_DATA_DIR / 'stage1_features.csv'\n",
    "SPATIAL_WEIGHTS_FILE = DISTRICT_DATA_DIR / 'spatial_weights.parquet'\n",
    "\n",
    "# Parameters\n",
    "RADIUS_KM = 300  # Spatial radius in kilometers\n",
    "\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")\n",
    "print(f\"Spatial weights: {SPATIAL_WEIGHTS_FILE}\")\n",
    "print(f\"Spatial radius: {RADIUS_KM} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Functions for distance calculation and spatial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "haversine-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate great circle distance in kilometers\"\"\"\n",
    "    R = 6371  # Earth radius in km\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "print(\"haversine_distance defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-weights-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spatial_weights_matrix(df, radius_km=300):\n",
    "    \"\"\"\n",
    "    Build inverse distance weighted spatial matrix at DISTRICT level.\n",
    "    W_ij = 1/d_ij if d_ij <= radius_km, else 0\n",
    "    Row-normalized so each row sums to 1\n",
    "\n",
    "    KEY CHANGE: Use ipc_geographic_unit_full as unique district identifier\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuilding spatial weights matrix (radius={radius_km}km)...\")\n",
    "    print(\"   Using ipc_geographic_unit_full as district identifier\")\n",
    "\n",
    "    # Get unique districts with their average coordinates\n",
    "    # Each district should have consistent coordinates across time\n",
    "    district_coords = df.groupby('ipc_geographic_unit_full').agg({\n",
    "        'avg_latitude': 'mean',\n",
    "        'avg_longitude': 'mean',\n",
    "        'ipc_country': 'first',\n",
    "        'ipc_district': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    district_coords = district_coords.dropna(subset=['avg_latitude', 'avg_longitude'])\n",
    "    district_coords = district_coords.reset_index(drop=True)\n",
    "\n",
    "    n_districts = len(district_coords)\n",
    "    print(f\"   Computing distances for {n_districts} unique districts...\")\n",
    "\n",
    "    # Extract coordinates\n",
    "    coords = district_coords[['avg_latitude', 'avg_longitude']].values\n",
    "\n",
    "    # Compute distance matrix in chunks\n",
    "    print(f\"   Computing distance matrix...\")\n",
    "    coords_rad = np.radians(coords)\n",
    "    chunk_size = 500\n",
    "    n_chunks = (n_districts + chunk_size - 1) // chunk_size\n",
    "\n",
    "    # Initialize sparse weights\n",
    "    W_sparse = {}\n",
    "\n",
    "    for chunk_idx in range(n_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, n_districts)\n",
    "\n",
    "        print(f\"   Processing chunk {chunk_idx+1}/{n_chunks}...\", end='\\r')\n",
    "\n",
    "        chunk_coords = coords_rad[start_idx:end_idx]\n",
    "\n",
    "        lat1 = chunk_coords[:, 0:1]\n",
    "        lon1 = chunk_coords[:, 1:2]\n",
    "        lat2 = coords_rad[:, 0]\n",
    "        lon2 = coords_rad[:, 1]\n",
    "\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        distances = 6371 * c\n",
    "\n",
    "        for i, global_i in enumerate(range(start_idx, end_idx)):\n",
    "            valid_mask = (distances[i, :] > 0) & (distances[i, :] <= radius_km)\n",
    "\n",
    "            if valid_mask.any():\n",
    "                neighbor_indices = np.where(valid_mask)[0]\n",
    "                neighbor_distances = distances[i, valid_mask]\n",
    "                neighbor_weights = 1.0 / neighbor_distances\n",
    "\n",
    "                weight_sum = neighbor_weights.sum()\n",
    "                if weight_sum > 0:\n",
    "                    neighbor_weights = neighbor_weights / weight_sum\n",
    "\n",
    "                W_sparse[global_i] = dict(zip(neighbor_indices, neighbor_weights))\n",
    "\n",
    "    print(f\"\\n   Computed {len(W_sparse)} districts with neighbors\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    W = np.zeros((n_districts, n_districts))\n",
    "    for i, neighbors in W_sparse.items():\n",
    "        for j, weight in neighbors.items():\n",
    "            W[i, j] = weight\n",
    "\n",
    "    W_df = pd.DataFrame(\n",
    "        W,\n",
    "        index=district_coords['ipc_geographic_unit_full'],\n",
    "        columns=district_coords['ipc_geographic_unit_full']\n",
    "    )\n",
    "\n",
    "    # Add metadata\n",
    "    W_df['country'] = district_coords['ipc_country'].values\n",
    "    W_df['district'] = district_coords['ipc_district'].values\n",
    "\n",
    "    print(f\"   Spatial weights matrix: {W_df.shape}\")\n",
    "    print(f\"   Avg neighbors per district: {(W > 0).sum(axis=1).mean():.1f}\")\n",
    "    print(f\"   Districts with no neighbors: {(W.sum(axis=1) == 0).sum()}\")\n",
    "\n",
    "    return W_df, district_coords\n",
    "\n",
    "print(\"build_spatial_weights_matrix defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-lag-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spatial_lag(df, W_df):\n",
    "    \"\"\"\n",
    "    Compute spatial autoregressive feature Ls at DISTRICT level.\n",
    "    Ls_it = sum_j(W_ij * IPC_jt) for neighboring districts j at same time t\n",
    "\n",
    "    KEY CHANGE: Uses ipc_geographic_unit_full for district identification\n",
    "    OPTIMIZED: O(n) using pre-built lookup dictionary instead of O(nÂ²) filtering\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating Ls (spatial lag) at DISTRICT level...\")\n",
    "    print(\"   Building (district, period) -> IPC lookup for O(n) performance...\")\n",
    "\n",
    "    # Get metadata columns to drop from weights\n",
    "    meta_cols = ['country', 'district']\n",
    "    meta_cols = [c for c in meta_cols if c in W_df.columns]\n",
    "\n",
    "    # PRE-BUILD LOOKUP: (district_full, period_start) -> ipc_value\n",
    "    # This makes neighbor lookups O(1) instead of O(n)\n",
    "    ipc_lookup = {}\n",
    "    for _, row in df.iterrows():\n",
    "        key = (row['ipc_geographic_unit_full'], row['ipc_period_start'])\n",
    "        ipc_lookup[key] = row['ipc_value']\n",
    "\n",
    "    print(f\"   Built lookup with {len(ipc_lookup):,} entries\")\n",
    "\n",
    "    # Pre-extract weights for each district (avoid repeated .loc calls)\n",
    "    district_weights = {}\n",
    "    district_neighbors = {}\n",
    "    for district_full in W_df.index:\n",
    "        weights = W_df.loc[district_full, :].drop(meta_cols, errors='ignore')\n",
    "        neighbors = weights[weights > 0]\n",
    "        if len(neighbors) > 0:\n",
    "            district_weights[district_full] = neighbors.to_dict()\n",
    "            district_neighbors[district_full] = list(neighbors.index)\n",
    "\n",
    "    print(f\"   Pre-computed weights for {len(district_weights):,} districts with neighbors\")\n",
    "\n",
    "    # Compute Ls for each row\n",
    "    ls_values = []\n",
    "    n_valid = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 5000 == 0:\n",
    "            print(f\"   Processing row {idx:,}/{len(df):,}...\", end='\\r')\n",
    "\n",
    "        district_full = row['ipc_geographic_unit_full']\n",
    "        period_start = row['ipc_period_start']\n",
    "\n",
    "        # Check if district has neighbors\n",
    "        if district_full not in district_neighbors:\n",
    "            ls_values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        neighbors = district_neighbors[district_full]\n",
    "        weights = district_weights[district_full]\n",
    "\n",
    "        # Compute weighted average using O(1) lookups\n",
    "        weighted_sum = 0\n",
    "        weight_sum = 0\n",
    "\n",
    "        for neighbor_full in neighbors:\n",
    "            neighbor_key = (neighbor_full, period_start)\n",
    "            if neighbor_key in ipc_lookup:\n",
    "                neighbor_ipc = ipc_lookup[neighbor_key]\n",
    "                w = weights[neighbor_full]\n",
    "                weighted_sum += w * neighbor_ipc\n",
    "                weight_sum += w\n",
    "\n",
    "        if weight_sum > 0:\n",
    "            ls_values.append(weighted_sum / weight_sum)\n",
    "            n_valid += 1\n",
    "        else:\n",
    "            ls_values.append(np.nan)\n",
    "\n",
    "    print(f\"\\n   Ls computed: {n_valid:,} valid\")\n",
    "\n",
    "    return ls_values\n",
    "\n",
    "print(\"create_spatial_lag defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-header",
   "metadata": {},
   "source": [
    "## Main Processing\n",
    "\n",
    "Load data, create features, and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Stage 1: Feature Engineering - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Start time: {datetime.now()}\\n\")\n",
    "\n",
    "# Load data\n",
    "print(\"1. Loading deduplicated district-level dataset...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "print(f\"   Loaded {len(df):,} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert-dates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates\n",
    "print(\"\\n2. Converting dates to datetime...\")\n",
    "df['ipc_period_start'] = pd.to_datetime(df['ipc_period_start'])\n",
    "df['ipc_period_end'] = pd.to_datetime(df['ipc_period_end'])\n",
    "\n",
    "# Sort by district and time (CRITICAL for temporal lag)\n",
    "print(\"\\n3. Sorting by district and time...\")\n",
    "df = df.sort_values(['ipc_geographic_unit_full', 'ipc_period_start'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"   Date range: {df['ipc_period_start'].min()} to {df['ipc_period_end'].max()}\")\n",
    "print(f\"   Countries: {df['ipc_country'].nunique()}\")\n",
    "print(f\"   Unique districts: {df['ipc_geographic_unit_full'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-spatial-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build spatial weights matrix\n",
    "W_df, district_coords = build_spatial_weights_matrix(df, radius_km=RADIUS_KM)\n",
    "\n",
    "# Save spatial weights\n",
    "print(f\"\\n   Saving spatial weights to {SPATIAL_WEIGHTS_FILE}...\")\n",
    "W_df.to_parquet(SPATIAL_WEIGHTS_FILE)\n",
    "print(\"   [OK] Spatial weights saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-lt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lt (temporal lag)\n",
    "print(\"\\n4. Creating Lt (temporal autoregressive) features...\")\n",
    "print(\"   Lt = previous period IPC value (t-1) for SAME DISTRICT\")\n",
    "print(\"   Grouped by: ipc_geographic_unit_full (unique district identifier)\")\n",
    "\n",
    "# Group by district (geographic_unit_full) and get previous IPC\n",
    "df['Lt'] = df.groupby('ipc_geographic_unit_full')['ipc_value'].shift(1)\n",
    "\n",
    "lt_missing = df['Lt'].isna().sum()\n",
    "print(f\"   Lt created: {lt_missing:,} missing (first observation per district)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-ls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ls (spatial lag)\n",
    "ls_values = create_spatial_lag(df, W_df)\n",
    "df['Ls'] = ls_values\n",
    "\n",
    "ls_missing = df['Ls'].isna().sum()\n",
    "print(f\"   Ls created: {ls_missing:,} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-targets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create future crisis targets\n",
    "print(\"\\n5. Creating future crisis targets...\")\n",
    "print(\"   Building district -> periods lookup for O(n) performance...\")\n",
    "\n",
    "# PRE-BUILD LOOKUP: district -> list of (period_start, ipc_value) sorted by date\n",
    "# This makes future observation lookups O(log n) instead of O(n)\n",
    "district_periods = {}\n",
    "for _, row in df.iterrows():\n",
    "    district_full = row['ipc_geographic_unit_full']\n",
    "    if district_full not in district_periods:\n",
    "        district_periods[district_full] = []\n",
    "    district_periods[district_full].append((row['ipc_period_start'], row['ipc_value']))\n",
    "\n",
    "# Sort each district's periods by date\n",
    "for district_full in district_periods:\n",
    "    district_periods[district_full].sort(key=lambda x: x[0])\n",
    "\n",
    "print(f\"   Built lookup for {len(district_periods):,} districts\")\n",
    "\n",
    "for h in [4, 8, 12]:\n",
    "    print(f\"\\n   Creating y_h{h} (crisis {h} months ahead)...\")\n",
    "\n",
    "    target_col = f'y_h{h}'\n",
    "    target_values = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 10000 == 0:\n",
    "            print(f\"      Processing row {idx:,}/{len(df):,}...\", end='\\r')\n",
    "\n",
    "        district_full = row['ipc_geographic_unit_full']\n",
    "        current_start = row['ipc_period_start']\n",
    "\n",
    "        # Calculate target date window\n",
    "        target_date_min = current_start + relativedelta(months=h)\n",
    "        target_date_max = current_start + relativedelta(months=h+2)\n",
    "\n",
    "        # Find future observation using pre-built lookup (O(k) where k = periods per district)\n",
    "        periods = district_periods.get(district_full, [])\n",
    "        future_ipc = None\n",
    "\n",
    "        for period_start, ipc_value in periods:\n",
    "            if period_start >= target_date_min:\n",
    "                if period_start <= target_date_max:\n",
    "                    future_ipc = ipc_value\n",
    "                    break\n",
    "                else:\n",
    "                    # Past the window, no match\n",
    "                    break\n",
    "\n",
    "        if future_ipc is not None:\n",
    "            target_values.append(1 if future_ipc >= 3 else 0)\n",
    "        else:\n",
    "            target_values.append(np.nan)\n",
    "\n",
    "    df[target_col] = target_values\n",
    "\n",
    "    valid = df[target_col].notna().sum()\n",
    "    crisis = (df[target_col] == 1).sum()\n",
    "    pct = (crisis/valid*100) if valid > 0 else 0\n",
    "    print(f\"\\n      Valid: {valid:,}, Crisis: {crisis:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate no data leakage\n",
    "print(\"\\n6. Validating no data leakage...\")\n",
    "\n",
    "leakage_check = df.groupby('ipc_geographic_unit_full').apply(\n",
    "    lambda g: (g['ipc_period_start'].shift(1) < g['ipc_period_start']).all(),\n",
    "    include_groups=False\n",
    ")\n",
    "\n",
    "if leakage_check.all():\n",
    "    print(\"   [OK] Lt validation passed: All temporal lags are from previous periods\")\n",
    "else:\n",
    "    failed = (~leakage_check).sum()\n",
    "    print(f\"   [WARNING] {failed} districts failed temporal validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Feature Engineering Summary - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "\n",
    "new_cols = ['Lt', 'Ls', 'y_h4', 'y_h8', 'y_h12']\n",
    "print(f\"\\nNew columns added:\")\n",
    "for col in new_cols:\n",
    "    if col in df.columns:\n",
    "        valid = df[col].notna().sum()\n",
    "        missing = df[col].isna().sum()\n",
    "        print(f\"  - {col}: {valid:,} valid ({missing:,} missing)\")\n",
    "\n",
    "print(f\"\\nRows usable for training (complete features + label):\")\n",
    "for h in [4, 8, 12]:\n",
    "    usable = df[['Lt', 'Ls', f'y_h{h}']].notna().all(axis=1).sum()\n",
    "    print(f\"  h={h} months: {usable:,} rows ({usable/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "print(f\"\\n7. Saving feature-engineered dataset...\")\n",
    "print(f\"   Parquet: {OUTPUT_FILE}\")\n",
    "df.to_parquet(OUTPUT_FILE, index=False)\n",
    "print(\"   [OK] Parquet saved\")\n",
    "\n",
    "print(f\"\\n   CSV: {OUTPUT_CSV}\")\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"   [OK] CSV saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Feature Engineering Complete - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nEnd time: {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
