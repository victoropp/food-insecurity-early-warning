{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3656b608",
   "metadata": {},
   "source": [
    "# Phase 2: Z-score Features\n",
    "\n",
    "**Script**: `scripts\\04_stage2_feature_engineering\\phase2_feature_creation\\02_zscore_features.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Creates z-score standardized temporal features.\n",
    "\n",
    "**KEY FEATURES**:\n",
    "- article_count_zscore: (Current - μ) / σ over 3-month window\n",
    "- Standardized theme/tone features\n",
    "\n",
    "Normalizes across different district baselines.\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22716a15",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a97c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Z-Score Feature Engineering for XGBoost Pipeline (Stratified Spatial CV Variant)\n",
    "=================================================================================\n",
    "Phase 2, Step 2: Create z-score based features for pooled models.\n",
    "\n",
    "VARIANT: STRATIFIED SPATIAL CV\n",
    "==============================\n",
    "This variant uses:\n",
    "1. STRATIFIED SPATIAL CV - Balanced folds by crisis rate AND geography\n",
    "2. MEANINGFUL LOCATION FEATURES - Replace arbitrary label encoding\n",
    "\n",
    "KEY DIFFERENCES FROM KMEANS-ONLY PIPELINE:\n",
    "- Uses stratified spatial CV (balanced crisis rates across folds)\n",
    "- Adds meaningful location features\n",
    "\n",
    "FEATURES CREATED:\n",
    "1. Rolling 12-month z-scores for all macro categories\n",
    "2. Z-score derivatives (delta, acceleration, extreme indicators)\n",
    "3. Tone z-scores\n",
    "4. Volume z-scores\n",
    "5. Interaction z-scores\n",
    "6. Compound indicators\n",
    "7. MEANINGFUL LOCATION FEATURES (NEW)\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "Date: December 2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for config import\n",
    "sys.path.append(str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "# Import from config\n",
    "from config import (\n",
    "    BASE_DIR,\n",
    "    STAGE1_DATA_DIR,\n",
    "    STAGE1_RESULTS_DIR,\n",
    "    STAGE2_DATA_DIR,\n",
    "    STAGE2_FEATURES_DIR,\n",
    "    STAGE2_MODELS_DIR,\n",
    "    FIGURES_DIR,\n",
    "    RANDOM_STATE\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use self-contained paths from config (no hardcoded paths)\n",
    "PHASE1_RESULTS = STAGE2_FEATURES_DIR / 'phase1_district_threshold'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de7637",
   "metadata": {},
   "source": [
    "## Load Monthly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE2_RESULTS = STAGE2_FEATURES_DIR / 'phase2_features'\n",
    "\n",
    "# Macro categories\n",
    "MACRO_CATEGORIES = [\n",
    "    'conflict_category', 'displacement_category', 'economic_category',\n",
    "    'food_security_category', 'governance_category', 'health_category',\n",
    "    'humanitarian_category', 'other_category', 'weather_category'\n",
    "]\n",
    "\n",
    "# Output files\n",
    "OUTPUT_FILES = {\n",
    "    'valid_districts': 'valid_districts.csv',\n",
    "    'zscore_features': 'zscore_features_h8.csv',\n",
    "}\n",
    "\n",
    "def ensure_directories():\n",
    "    PHASE2_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Spatial CV configuration (self-contained)\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_spatial_folds(df, district_col, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create spatial folds by district using KMeans clustering on coordinates.\n",
    "\n",
    "    NOTE: This uses standard KMeans CV for compatibility with Mixed Effects models.\n",
    "    XGBoost scripts will override with stratified spatial CV at training time.\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "\n",
    "    # Get unique districts with their coordinates\n",
    "    coord_cols = ['avg_latitude', 'avg_longitude']\n",
    "    districts = df[[district_col] + coord_cols].drop_duplicates()\n",
    "    districts_with_coords = districts.dropna(subset=coord_cols)\n",
    "\n",
    "    print(f\"   Creating {n_folds} spatial folds using KMeans clustering...\")\n",
    "    print(f\"   Total districts: {len(districts)}\")\n",
    "    print(f\"   Districts with valid coordinates: {len(districts_with_coords)}\")\n",
    "\n",
    "    # KMeans clustering on coordinates\n",
    "    coords = districts_with_coords[coord_cols].values\n",
    "    kmeans = KMeans(n_clusters=n_folds, random_state=random_state, n_init=10)\n",
    "    districts_with_coords = districts_with_coords.copy()\n",
    "    districts_with_coords['fold'] = kmeans.fit_predict(coords)\n",
    "\n",
    "    # Print fold distribution\n",
    "    print(f\"   Fold distribution (KMeans spatial clustering):\")\n",
    "    for fold in range(n_folds):\n",
    "        n_districts = (districts_with_coords['fold'] == fold).sum()\n",
    "        print(f\"      Fold {fold}: {n_districts} districts\")\n",
    "\n",
    "    # Map folds back to full dataset\n",
    "    fold_map = dict(zip(districts_with_coords[district_col], districts_with_coords['fold']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cec126",
   "metadata": {},
   "source": [
    "## Filter by Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "    return df[district_col].map(fold_map)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Zscore configuration (self-contained)\n",
    "ZSCORE_CONFIG = {\n",
    "    'rolling_window': 12,  # 12 months for rolling statistics\n",
    "    'min_periods': 3,      # Minimum 3 months of data required\n",
    "}\n",
    "\n",
    "ROLLING_WINDOW = ZSCORE_CONFIG['rolling_window']  # 12 months\n",
    "MIN_PERIODS = ZSCORE_CONFIG['min_periods']  # 3 months minimum\n",
    "\n",
    "# Paths\n",
    "MONTHLY_DATA = STAGE2_DATA_DIR / 'ml_dataset_monthly.parquet'\n",
    "IPC_REFERENCE = STAGE1_DATA_DIR / 'ipc_reference.parquet'\n",
    "# AR predictions from Stage 1\n",
    "AR_PREDICTIONS = STAGE1_RESULTS_DIR / 'predictions_h8_averaged.csv'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: Z-SCORE FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nRolling window: {ROLLING_WINDOW} months\")\n",
    "print(f\"Minimum periods: {MIN_PERIODS}\")\n",
    "\n",
    "\n",
    "def load_valid_districts():\n",
    "    \"\"\"\n",
    "    Load valid districts from Phase 1.\n",
    "\n",
    "    UPDATED: Now loads ipc_geographic_unit_full (canonical identifier)\n",
    "    instead of short district names to ensure consistent matching.\n",
    "    \"\"\"\n",
    "    valid_path = PHASE1_RESULTS / OUTPUT_FILES['valid_districts']\n",
    "\n",
    "    if not valid_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Valid districts file not found: {valid_path}\\n\"\n",
    "            \"Please run Phase 1 scripts first.\"\n",
    "        )\n",
    "\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "\n",
    "    # Use canonical identifier\n",
    "    if 'ipc_geographic_unit_full' not in valid_df.columns:\n",
    "        raise ValueError(\n",
    "            \"ipc_geographic_unit_full not found in Phase 1 output! \"\n",
    "            \"Please re-run Phase 1 with updated script.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7c86e",
   "metadata": {},
   "source": [
    "## Compute Z-score Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        )\n",
    "\n",
    "    valid_districts = valid_df['ipc_geographic_unit_full'].tolist()\n",
    "    print(f\"   Loaded {len(valid_districts):,} valid districts from Phase 1\")\n",
    "    print(f\"   Using canonical identifier: ipc_geographic_unit_full\")\n",
    "\n",
    "    return valid_districts\n",
    "\n",
    "\n",
    "def load_and_filter_data(valid_districts):\n",
    "    \"\"\"\n",
    "    Load monthly data and filter to valid districts.\n",
    "\n",
    "    UPDATED: Now FORCES use of ipc_geographic_unit_full to match Phase 1.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Loading and filtering data...\")\n",
    "\n",
    "    # Load monthly data\n",
    "    df = pd.read_parquet(MONTHLY_DATA)\n",
    "    print(f\"   Loaded monthly data: {len(df):,} rows\")\n",
    "\n",
    "    # ALWAYS use canonical identifier\n",
    "    district_col = 'ipc_geographic_unit_full'\n",
    "    if district_col not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"ipc_geographic_unit_full not found in monthly data! \"\n",
    "            f\"Available columns: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    print(f\"   Using canonical identifier: {district_col}\")\n",
    "\n",
    "    # Filter to valid districts\n",
    "    df_filtered = df[df[district_col].isin(valid_districts)].copy()\n",
    "    print(f\"   After filtering to valid districts: {len(df_filtered):,} rows\")\n",
    "\n",
    "    return df_filtered, district_col\n",
    "\n",
    "\n",
    "def compute_rolling_zscore(series, window=ROLLING_WINDOW, min_periods=MIN_PERIODS):\n",
    "    \"\"\"\n",
    "    Compute rolling z-score.\n",
    "\n",
    "    z = (x - rolling_mean) / rolling_std\n",
    "    \"\"\"\n",
    "    rolling_mean = series.rolling(window=window, min_periods=min_periods).mean()\n",
    "    rolling_std = series.rolling(window=window, min_periods=min_periods).std()\n",
    "\n",
    "    zscore = (series - rolling_mean) / rolling_std.replace(0, np.nan)\n",
    "    return zscore\n",
    "\n",
    "\n",
    "def compute_category_zscores(df, district_col):\n",
    "    \"\"\"\n",
    "    Compute 12-month rolling z-scores for all macro categories.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing category z-scores...\")\n",
    "\n",
    "    # Sort by district and time\n",
    "    df = df.sort_values([district_col, 'year_month'])\n",
    "\n",
    "    category_cols = [col for col in MACRO_CATEGORIES if col in df.columns]\n",
    "\n",
    "    for col in category_cols:\n",
    "        zscore_col = col.replace('_category', '_zscore')\n",
    "        df[zscore_col] = df.groupby(district_col)[col].transform(\n",
    "            lambda x: compute_rolling_zscore(x)\n",
    "        )\n",
    "\n",
    "    print(f\"   Created {len(category_cols)} category z-score features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_zscore_derivatives(df, district_col):\n",
    "    \"\"\"\n",
    "    Compute z-score derivatives.\n",
    "\n",
    "    - Delta: z(t) - z(t-1)\n",
    "    - Acceleration: delta(t) - delta(t-1)\n",
    "    - Extreme indicator: z > 2.0\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing z-score derivatives...\")\n",
    "\n",
    "    df = df.sort_values([district_col, 'year_month'])\n",
    "\n",
    "    zscore_cols = [col for col in df.columns if col.endswith('_zscore')]\n",
    "\n",
    "    for col in zscore_cols:\n",
    "        base_name = col.replace('_zscore', '')\n",
    "\n",
    "        # Delta (first derivative)\n",
    "        delta_col = f'{base_name}_zscore_delta'\n",
    "        df[delta_col] = df.groupby(district_col)[col].diff()\n",
    "\n",
    "        # Acceleration (second derivative)\n",
    "        accel_col = f'{base_name}_zscore_accel'\n",
    "        df[accel_col] = df.groupby(district_col)[delta_col].diff()\n",
    "\n",
    "        # Extreme indicator (z > 2)\n",
    "        extreme_col = f'{base_name}_zscore_extreme'\n",
    "        df[extreme_col] = (df[col].abs() > 2.0).astype(int)\n",
    "\n",
    "    print(f\"   Created {len(zscore_cols) * 3} derivative features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_tone_zscores(df, district_col):\n",
    "    \"\"\"\n",
    "    Compute z-scores for tone features.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing tone z-scores...\")\n",
    "\n",
    "    tone_cols = ['avg_tone_score', 'avg_tone_negative_pct', 'tone_polarity', 'sentiment_stress']\n",
    "    available_tone = [col for col in tone_cols if col in df.columns]\n",
    "\n",
    "    df = df.sort_values([district_col, 'year_month'])\n",
    "\n",
    "    for col in available_tone:\n",
    "        zscore_col = f'{col}_zscore'\n",
    "        df[zscore_col] = df.groupby(district_col)[col].transform(\n",
    "            lambda x: compute_rolling_zscore(x)\n",
    "        )\n",
    "\n",
    "    print(f\"   Created {len(available_tone)} tone z-score features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_volume_zscores(df, district_col):\n",
    "    \"\"\"\n",
    "    Compute z-scores for volume features.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing volume z-scores...\")\n",
    "\n",
    "    # Log transform article count for better distribution\n",
    "    if 'article_count' in df.columns:\n",
    "        df['article_count_log'] = np.log1p(df['article_count'])\n",
    "\n",
    "    volume_cols = ['article_count_log', 'unique_sources', 'words_per_article']\n",
    "    available_volume = [col for col in volume_cols if col in df.columns]\n",
    "\n",
    "    df = df.sort_values([district_col, 'year_month'])\n",
    "\n",
    "    for col in available_volume:\n",
    "        zscore_col = f'{col}_zscore'\n",
    "        df[zscore_col] = df.groupby(district_col)[col].transform(\n",
    "            lambda x: compute_rolling_zscore(x)\n",
    "        )\n",
    "\n",
    "    print(f\"   Created {len(available_volume)} volume z-score features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_interaction_zscores(df):\n",
    "    \"\"\"\n",
    "    Compute interaction z-scores between category pairs.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing interaction z-scores...\")\n",
    "\n",
    "    # Define interaction pairs\n",
    "    interactions = [\n",
    "        ('conflict_zscore', 'displacement_zscore'),\n",
    "        ('weather_zscore', 'food_security_zscore'),\n",
    "        ('economic_zscore', 'food_security_zscore'),\n",
    "        ('humanitarian_zscore', 'conflict_zscore'),\n",
    "        ('conflict_zscore', 'food_security_zscore'),\n",
    "    ]\n",
    "\n",
    "    created = 0\n",
    "    for col1, col2 in interactions:\n",
    "        if col1 in df.columns and col2 in df.columns:\n",
    "            interaction_name = f'{col1.replace(\"_zscore\", \"\")}_{col2.replace(\"_zscore\", \"\")}_interaction'\n",
    "            df[interaction_name] = df[col1] * df[col2]\n",
    "            created += 1\n",
    "\n",
    "    print(f\"   Created {created} interaction features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_compound_indicators(df):\n",
    "    \"\"\"\n",
    "    Compute compound stress indicators.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing compound indicators...\")\n",
    "\n",
    "    zscore_cols = [col for col in df.columns if col.endswith('_zscore')\n",
    "                   and not col.endswith('_delta') and not col.endswith('_accel')]\n",
    "\n",
    "    # Keep only category z-scores for compound indicators\n",
    "    category_zscore_cols = [col for col in zscore_cols\n",
    "                           if any(cat.replace('_category', '_zscore') == col\n",
    "                                  for cat in MACRO_CATEGORIES)]\n",
    "\n",
    "    if len(category_zscore_cols) > 0:\n",
    "        # Count of elevated z-scores (> 1.0)\n",
    "        df['compound_stress_count'] = df[category_zscore_cols].apply(\n",
    "            lambda row: (row > 1.0).sum(), axis=1\n",
    "        )\n",
    "\n",
    "        # Count of extreme z-scores (> 2.0)\n",
    "        df['compound_extreme_count'] = df[category_zscore_cols].apply(\n",
    "            lambda row: (row > 2.0).sum(), axis=1\n",
    "        )\n",
    "\n",
    "        # Multi-sector crisis indicator (>= 3 elevated)\n",
    "        df['multi_sector_crisis'] = (df['compound_stress_count'] >= 3).astype(int)\n",
    "\n",
    "        print(\"   Created: compound_stress_count, compound_extreme_count, multi_sector_crisis\")\n",
    "    else:\n",
    "        print(\"   Warning: No category z-score columns found for compound indicators\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_ar_predictions_and_folds(df, district_col):\n",
    "    \"\"\"\n",
    "    Load AR predictions using FORWARD-FILL expansion and create spatial CV folds.\n",
    "\n",
    "    Uses FULL predictions file (all AR predictions, not just failures) to maximize coverage.\n",
    "\n",
    "    AR predictions were made for IPC assessment periods (not individual months).\n",
    "    We expand each AR prediction to ALL months within its assessment period,\n",
    "    matching the same logic used for IPC classifications.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Loading AR predictions...\")\n",
    "\n",
    "    if not AR_PREDICTIONS.exists():\n",
    "        print(f\"   Warning: AR predictions file not found: {AR_PREDICTIONS}\")\n",
    "        print(\"   AR prediction columns will not be available\")\n",
    "    else:\n",
    "        # Load full AR predictions (CSV format)\n",
    "        ar_df = pd.read_csv(AR_PREDICTIONS)\n",
    "        print(f\"   Loaded AR predictions: {len(ar_df):,} IPC assessment periods\")\n",
    "\n",
    "        # Strip whitespace from district identifier for alignment\n",
    "        # (Source data has leading tabs that need to be removed)\n",
    "        ar_df['ipc_geographic_unit_full'] = ar_df['ipc_geographic_unit_full'].str.strip()\n",
    "        df[district_col] = df[district_col].str.strip()\n",
    "\n",
    "        # Convert dates\n",
    "        ar_df['ipc_period_start'] = pd.to_datetime(ar_df['ipc_period_start'])\n",
    "        ar_df['ipc_period_end'] = pd.to_datetime(ar_df['ipc_period_end'])\n",
    "\n",
    "        # FORWARD-FILL EXPANSION: Each IPC period is ~1 month, expand to exact month\n",
    "        # This matches the IPC forward-fill logic for temporal consistency\n",
    "        print(\"   Expanding AR predictions to monthly observations...\")\n",
    "\n",
    "        expanded_rows = []\n",
    "        for _, row in ar_df.iterrows():\n",
    "            # IPC periods are ~1 month (28-31 days)\n",
    "            # Use period start month as the representative month\n",
    "            month = row['ipc_period_start'].replace(day=1)\n",
    "            expanded_rows.append({\n",
    "                district_col: row['ipc_geographic_unit_full'],\n",
    "                'year_month': month.strftime('%Y-%m'),\n",
    "                'ar_pred_optimal': row['y_pred_optimal'],\n",
    "                'ar_prob': row['pred_prob'],\n",
    "                'ar_pred_binary': row['y_pred'],\n",
    "                'ar_period_start': row['ipc_period_start'],\n",
    "                'ar_period_end': row['ipc_period_end']\n",
    "            })\n",
    "\n",
    "        ar_expanded = pd.DataFrame(expanded_rows)\n",
    "        print(f\"   Expanded to {len(ar_expanded):,} district-month observations\")\n",
    "\n",
    "        # Aggregate if multiple predictions per district-month (take max)\n",
    "        ar_lookup = ar_expanded.groupby([district_col, 'year_month']).agg({\n",
    "            'ar_pred_optimal': 'max',\n",
    "            'ar_prob': 'max',\n",
    "            'ar_pred_binary': 'max',\n",
    "            'ar_period_start': 'min',\n",
    "            'ar_period_end': 'max'\n",
    "        }).reset_index()\n",
    "\n",
    "        print(f\"   Created AR lookup: {len(ar_lookup):,} unique district-month observations\")\n",
    "\n",
    "        # Rename columns to _filled suffix to indicate forward-fill methodology\n",
    "        ar_lookup = ar_lookup.rename(columns={\n",
    "            'ar_pred_optimal': 'ar_pred_optimal_filled',\n",
    "            'ar_prob': 'ar_prob_filled',\n",
    "            'ar_pred_binary': 'ar_pred_binary_filled'\n",
    "        })\n",
    "\n",
    "        # Merge with feature data\n",
    "        merge_cols = [district_col, 'year_month']\n",
    "        df = df.merge(ar_lookup, on=merge_cols, how='left', suffixes=('', '_ar'))\n",
    "\n",
    "        # Report coverage\n",
    "        ar_coverage = df['ar_pred_optimal_filled'].notna().sum()\n",
    "        ar_coverage_pct = ar_coverage / len(df) * 100\n",
    "        print(f\"   AR predictions available: {ar_coverage:,} / {len(df):,} observations ({ar_coverage_pct:.1f}%)\")\n",
    "\n",
    "    # Create spatial folds for ALL observations (not just AR failures)\n",
    "    # This is critical for proper train/test splits with both crisis and non-crisis cases\n",
    "    print(\"\\n   Creating spatial CV folds for ALL observations...\")\n",
    "    if 'fold' in df.columns:\n",
    "        df = df.drop(columns=['fold'])  # Remove any partial fold assignments\n",
    "    df['fold'] = create_spatial_folds(df, 'ipc_geographic_unit_full', n_folds=N_FOLDS, random_state=RANDOM_STATE)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_target_variable(df, district_col):\n",
    "    \"\"\"\n",
    "    Create target variable: ipc_future_crisis at t+h (h=8 months ahead).\n",
    "\n",
    "    REPRODUCIBLE SEQUENTIAL PROCESS:\n",
    "    1. Load IPC reference data\n",
    "    2. Create district-month level IPC values\n",
    "    3. Create future crisis variable by shifting IPC values forward\n",
    "    4. Merge with feature data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Creating target variable...\")\n",
    "\n",
    "    HORIZON = 8  # Prediction horizon in months\n",
    "\n",
    "    # If ipc_future_crisis already exists, use it\n",
    "    if 'ipc_future_crisis' in df.columns and df['ipc_future_crisis'].notna().sum() > 0:\n",
    "        print(\"   Using existing ipc_future_crisis column\")\n",
    "    else:\n",
    "        # Step 1: Load IPC reference data\n",
    "        print(f\"   Loading IPC reference data from: {IPC_REFERENCE}\")\n",
    "\n",
    "        if not IPC_REFERENCE.exists():\n",
    "            print(f\"   ERROR: IPC reference file not found: {IPC_REFERENCE}\")\n",
    "            print(\"   Target variable cannot be created\")\n",
    "            return df\n",
    "\n",
    "        ipc_df = pd.read_parquet(IPC_REFERENCE)\n",
    "        print(f\"   Loaded {len(ipc_df):,} IPC observations\")\n",
    "\n",
    "        # Step 2: Create district identifier matching the feature data\n",
    "        ipc_district_col = 'geographic_unit_full_name' if 'geographic_unit_full_name' in ipc_df.columns else 'district'\n",
    "\n",
    "        # Convert dates\n",
    "        ipc_df['projection_start'] = pd.to_datetime(ipc_df['projection_start'])\n",
    "        ipc_df['projection_end'] = pd.to_datetime(ipc_df['projection_end'])\n",
    "\n",
    "        # Create binary crisis indicator (IPC >= 3)\n",
    "        ipc_df['ipc_binary_crisis'] = (ipc_df['ipc_value'] >= 3).astype(int)\n",
    "\n",
    "        # Step 3: Expand IPC observations to ALL months covered by projection period\n",
    "        # This is critical for monthly data - each IPC projection covers multiple months\n",
    "        print(\"   Expanding IPC observations to all covered months...\")\n",
    "\n",
    "        expanded_rows = []\n",
    "        for _, row in ipc_df.iterrows():\n",
    "            # Generate all months from projection_start to projection_end\n",
    "            months = pd.date_range(\n",
    "                start=row['projection_start'].replace(day=1),\n",
    "                end=row['projection_end'].replace(day=1),\n",
    "                freq='MS'  # Month Start frequency\n",
    "            )\n",
    "            for month in months:\n",
    "                expanded_rows.append({\n",
    "                    ipc_district_col: row[ipc_district_col],\n",
    "                    'year_month': month.strftime('%Y-%m'),\n",
    "                    'ipc_value': row['ipc_value'],\n",
    "                    'ipc_binary_crisis': row['ipc_binary_crisis'],\n",
    "                    'ipc_period_start': row['projection_start'],\n",
    "                    'ipc_period_end': row['projection_end']\n",
    "                })\n",
    "\n",
    "        ipc_expanded = pd.DataFrame(expanded_rows)\n",
    "        print(f\"   Expanded to {len(ipc_expanded):,} district-month observations\")\n",
    "\n",
    "        # Step 4: Create IPC lookup at district-month level\n",
    "        # Take the maximum IPC value if multiple per district-month (overlapping projections)\n",
    "        ipc_lookup = ipc_expanded.groupby([ipc_district_col, 'year_month']).agg({\n",
    "            'ipc_value': 'max',\n",
    "            'ipc_binary_crisis': 'max',\n",
    "            'ipc_period_start': 'min',\n",
    "            'ipc_period_end': 'max'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbded6",
   "metadata": {},
   "source": [
    "## Create Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        }).reset_index()\n",
    "\n",
    "        print(f\"   Created IPC lookup: {len(ipc_lookup):,} unique district-month observations\")\n",
    "\n",
    "        # Step 5: Create complete timeline with forward-filling\n",
    "        print(\"   Creating complete timeline with forward-filling...\")\n",
    "\n",
    "        # Get unique districts and months from feature data\n",
    "        feature_districts = df[district_col].unique()\n",
    "        feature_months = sorted(df['year_month'].unique())\n",
    "\n",
    "        # Create full grid of district-months from features\n",
    "        full_grid = pd.DataFrame([\n",
    "            {district_col: d, 'year_month': m}\n",
    "            for d in feature_districts\n",
    "            for m in feature_months\n",
    "        ])\n",
    "\n",
    "        # Rename IPC district column if needed\n",
    "        if ipc_district_col != district_col:\n",
    "            ipc_lookup = ipc_lookup.rename(columns={ipc_district_col: district_col})\n",
    "\n",
    "        # Merge IPC lookup with full grid\n",
    "        full_ipc = full_grid.merge(\n",
    "            ipc_lookup[[district_col, 'year_month', 'ipc_value', 'ipc_binary_crisis', 'ipc_period_start', 'ipc_period_end']],\n",
    "            on=[district_col, 'year_month'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Sort by district and time for forward filling\n",
    "        full_ipc = full_ipc.sort_values([district_col, 'year_month'])\n",
    "\n",
    "        # FIX ISSUE #2: TEMPORAL TARGET LEAKAGE\n",
    "        # CRITICAL: Shift BEFORE forward-fill to prevent future values bleeding into past\n",
    "        # Original (WRONG): ffill -> shift (future IPC values contaminate past)\n",
    "        # Fixed (CORRECT): shift -> ffill (only observed values used for filling)\n",
    "\n",
    "        # Step 6: Create future crisis variable by shifting FIRST\n",
    "        # Shift crisis indicator backward (future value becomes current target)\n",
    "        # This uses ONLY actually observed IPC values (no forward-filling yet)\n",
    "        full_ipc['ipc_future_crisis'] = full_ipc.groupby(district_col)['ipc_binary_crisis'].shift(-HORIZON)\n",
    "\n",
    "        # NOW forward-fill the shifted target (fills gaps with last OBSERVED future value)\n",
    "        # This is safe because we're filling the target variable, not the predictor\n",
    "        full_ipc['ipc_future_crisis'] = full_ipc.groupby(district_col)['ipc_future_crisis'].ffill()\n",
    "\n",
    "        # Fill remaining NaN targets with 0 for:\n",
    "        # 1. Last HORIZON months (no future to predict)\n",
    "        # 2. Districts without any IPC data\n",
    "        full_ipc['ipc_future_crisis'] = full_ipc['ipc_future_crisis'].fillna(0)\n",
    "\n",
    "        # FIX ISSUE #8: FORWARD-FILLED IPC FOR TARGETS\n",
    "        # Still create filled versions for current IPC status (used as features, not targets)\n",
    "        # But document clearly these are CURRENT status, not future\n",
    "        full_ipc['ipc_value_filled'] = full_ipc.groupby(district_col)['ipc_value'].ffill()\n",
    "        full_ipc['ipc_binary_crisis_filled'] = full_ipc.groupby(district_col)['ipc_binary_crisis'].ffill()\n",
    "        full_ipc['ipc_binary_crisis_filled'] = full_ipc['ipc_binary_crisis_filled'].fillna(0)\n",
    "\n",
    "        print(f\"   Created future crisis variable (h={HORIZON} months) - SHIFT THEN FILL\")\n",
    "        print(f\"   Target coverage: {full_ipc['ipc_future_crisis'].notna().sum():,} / {len(full_ipc):,}\")\n",
    "        print(f\"   Forward-filled IPC values: {full_ipc['ipc_value_filled'].notna().sum():,} / {len(full_ipc):,}\")\n",
    "\n",
    "        # Prepare IPC columns for merge\n",
    "        ipc_lookup = full_ipc[[district_col, 'year_month', 'ipc_value', 'ipc_value_filled',\n",
    "                               'ipc_binary_crisis', 'ipc_binary_crisis_filled',\n",
    "                               'ipc_future_crisis', 'ipc_period_start', 'ipc_period_end']]\n",
    "\n",
    "        # Step 7: Merge with feature data\n",
    "        merge_cols = ['year_month', 'ipc_value', 'ipc_value_filled', 'ipc_binary_crisis',\n",
    "                     'ipc_binary_crisis_filled', 'ipc_future_crisis', 'ipc_period_start', 'ipc_period_end']\n",
    "\n",
    "        df = df.merge(\n",
    "            ipc_lookup[[district_col] + merge_cols].drop_duplicates(),\n",
    "            on=[district_col, 'year_month'],\n",
    "            how='left',\n",
    "            suffixes=('', '_ipc')\n",
    "        )\n",
    "\n",
    "        print(f\"   Merged IPC data with features\")\n",
    "        print(f\"   Target coverage: {df['ipc_future_crisis'].notna().sum():,} / {len(df):,} rows\")\n",
    "\n",
    "    # Compute class balance\n",
    "    if 'ipc_future_crisis' in df.columns:\n",
    "        n_crisis = df['ipc_future_crisis'].sum()\n",
    "        n_total = df['ipc_future_crisis'].notna().sum()\n",
    "        prevalence = n_crisis / n_total if n_total > 0 else 0\n",
    "        print(f\"   Class balance: {n_crisis:,} crises / {n_total:,} total ({prevalence:.1%})\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    ensure_directories()\n",
    "\n",
    "    # Step 1: Load valid districts\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 1: Loading valid districts...\")\n",
    "    valid_districts = load_valid_districts()\n",
    "\n",
    "    # Step 2: Load and filter data\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 2: Loading and filtering data...\")\n",
    "    df, district_col = load_and_filter_data(valid_districts)\n",
    "\n",
    "    # Step 3: Compute category z-scores\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 3: Computing category z-scores...\")\n",
    "    df = compute_category_zscores(df, district_col)\n",
    "\n",
    "    # Step 4: Compute z-score derivatives\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 4: Computing z-score derivatives...\")\n",
    "    df = compute_zscore_derivatives(df, district_col)\n",
    "\n",
    "    # Step 5: Compute tone z-scores\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 5: Computing tone z-scores...\")\n",
    "    df = compute_tone_zscores(df, district_col)\n",
    "\n",
    "    # Step 6: Compute volume z-scores\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 6: Computing volume z-scores...\")\n",
    "    df = compute_volume_zscores(df, district_col)\n",
    "\n",
    "    # Step 7: Compute interaction z-scores\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 7: Computing interaction z-scores...\")\n",
    "    df = compute_interaction_zscores(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32d029",
   "metadata": {},
   "source": [
    "## Validation and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 8: Compute compound indicators\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 8: Computing compound indicators...\")\n",
    "    df = compute_compound_indicators(df)\n",
    "\n",
    "    # Step 9: Load AR predictions and folds\n",
    "    # Step 9: Create target variable (must be done BEFORE spatial folds)\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 9: Creating target variable...\")\n",
    "    df = create_target_variable(df, district_col)\n",
    "\n",
    "    # Step 10: Loading AR predictions and creating spatial folds\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 10: Loading AR predictions and creating spatial folds...\")\n",
    "    df = load_ar_predictions_and_folds(df, district_col)\n",
    "\n",
    "    # Step 11: Deduplicate observations (Stage 1 methodology)\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 11: Deduplicating observations...\")\n",
    "    print(f\"   Before deduplication: {len(df):,} rows\")\n",
    "\n",
    "    # Following Stage 1 methodology: unique observation = (ipc_geographic_unit_full, year_month)\n",
    "    df['observation_key'] = (\n",
    "        df[district_col].astype(str) + '_' +\n",
    "        df['year_month'].astype(str)\n",
    "    )\n",
    "\n",
    "    # Check for duplicates\n",
    "    dupes = df.duplicated(subset=['observation_key']).sum()\n",
    "    if dupes > 0:\n",
    "        print(f\"   Found {dupes:,} duplicate observations - aggregating...\")\n",
    "\n",
    "        # Identify columns to aggregate\n",
    "        # Count columns: sum\n",
    "        count_cols = ['article_count', 'location_mention_count', 'unique_sources']\n",
    "        count_cols = [col for col in count_cols if col in df.columns]\n",
    "\n",
    "        # Geographic coordinates: mean\n",
    "        geo_cols = ['avg_latitude', 'avg_longitude', 'latitude_std', 'longitude_std']\n",
    "        geo_cols = [col for col in geo_cols if col in df.columns]\n",
    "\n",
    "        # All z-score features: mean\n",
    "        zscore_cols = [col for col in df.columns if '_zscore' in col or '_delta' in col or\n",
    "                      '_accel' in col or '_extreme' in col or 'compound' in col]\n",
    "\n",
    "        # Metadata: first (should be identical) - INCLUDING IPC AND AR COLUMNS\n",
    "        metadata_cols = [district_col, 'ipc_district', 'ipc_country',\n",
    "                        'ipc_country_code', 'year_month', 'fold', 'ipc_future_crisis',\n",
    "                        'ipc_value', 'ipc_value_filled', 'ipc_binary_crisis', 'ipc_binary_crisis_filled',\n",
    "                        'ipc_period_start', 'ipc_period_end',\n",
    "                        'ar_pred_optimal_filled', 'ar_prob_filled', 'ar_pred_binary_filled',\n",
    "                        'ar_period_start', 'ar_period_end']\n",
    "        metadata_cols = [col for col in metadata_cols if col in df.columns]\n",
    "\n",
    "        # Build aggregation dict\n",
    "        agg_dict = {}\n",
    "        for col in count_cols:\n",
    "            agg_dict[col] = 'sum'\n",
    "        for col in geo_cols:\n",
    "            agg_dict[col] = 'mean'\n",
    "        for col in zscore_cols:\n",
    "            if col in df.columns:\n",
    "                agg_dict[col] = 'mean'\n",
    "        for col in metadata_cols:\n",
    "            if col in df.columns:\n",
    "                agg_dict[col] = 'first'\n",
    "\n",
    "        # Aggregate\n",
    "        df = df.groupby('observation_key').agg(agg_dict).reset_index()\n",
    "        df = df.drop(columns=['observation_key'])\n",
    "\n",
    "        print(f\"   After deduplication: {len(df):,} rows\")\n",
    "    else:\n",
    "        df = df.drop(columns=['observation_key'])\n",
    "        print(f\"   No duplicates found\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # SAVE OUTPUT\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Saving output...\")\n",
    "\n",
    "    output_path = PHASE2_RESULTS / OUTPUT_FILES['zscore_features']\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print(f\"   Saved: {output_path}\")\n",
    "\n",
    "    # CSV for inspection\n",
    "    csv_path = PHASE2_RESULTS / 'zscore_features_h8.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"   Saved: {csv_path}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # SUMMARY\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2 STEP 2 COMPLETE: Z-Score Feature Engineering\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    zscore_cols = [col for col in df.columns if '_zscore' in col and '_delta' not in col and '_accel' not in col]\n",
    "    delta_cols = [col for col in df.columns if '_delta' in col]\n",
    "    accel_cols = [col for col in df.columns if '_accel' in col]\n",
    "    extreme_cols = [col for col in df.columns if '_extreme' in col]\n",
    "\n",
    "    print(f\"\\n   Total observations: {len(df):,}\")\n",
    "    print(f\"   Unique districts: {df[district_col].nunique():,}\")\n",
    "    print(f\"\\n   Features created:\")\n",
    "    print(f\"      Z-score features: {len(zscore_cols)}\")\n",
    "    print(f\"      Delta features: {len(delta_cols)}\")\n",
    "    print(f\"      Acceleration features: {len(accel_cols)}\")\n",
    "    print(f\"      Extreme indicators: {len(extreme_cols)}\")\n",
    "    print(f\"      Compound indicators: 3\")\n",
    "    print(f\"      Total columns: {len(df.columns)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
