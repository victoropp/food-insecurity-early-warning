{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# Article Aggregation - District Level\n",
    "\n",
    "**Script**: `scripts/02_data_processing/02_aggregate_articles.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Aggregates GDELT GKG articles to IPC district-period observations using:\n",
    "- **4-month temporal windows** (3 months before + assessment month)\n",
    "- **Geographic matching hierarchy**: GADM3 (district) → GADM2 (zone) → GADM1 (state) → Country-level\n",
    "- **Fuzzy matching**: 80% threshold for district name matching\n",
    "- **SQLite streaming**: Memory-efficient processing of large CSV files (47GB)\n",
    "\n",
    "**Runtime**: ~45 minutes (1.2 million articles processed)\n",
    "\n",
    "**Input**: \n",
    "- `data/external/gdelt/african_gkg_articles_info.csv` (47GB)\n",
    "- `data/district_level/ipc_reference.parquet` (from 02a)\n",
    "\n",
    "**Output**: `data/district_level/articles_aggregated.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import gc\n",
    "import re\n",
    "import unicodedata\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from rapidfuzz import fuzz\n",
    "from config import BASE_DIR\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(str(BASE_DIR.parent.parent.parent))\n",
    "LOCATIONS_FILE = BASE_DIR / 'aligned_data' / 'african_gkg_locations_aligned.parquet'\n",
    "ARTICLES_FILE = BASE_DIR / 'aligned_data' / 'african_gkg_articles_info.csv'\n",
    "\n",
    "# IPC reference from previous step\n",
    "DISTRICT_DATA_DIR = BASE_DIR / 'data' / 'district_level'\n",
    "IPC_REF_FILE = DISTRICT_DATA_DIR / 'ipc_reference.parquet'\n",
    "\n",
    "# Output\n",
    "OUTPUT_FILE = DISTRICT_DATA_DIR / 'articles_aggregated.parquet'\n",
    "OUTPUT_CSV = DISTRICT_DATA_DIR / 'articles_aggregated.csv'\n",
    "\n",
    "# SQLite temp file for memory-efficient processing\n",
    "SQLITE_FILE = DISTRICT_DATA_DIR / 'temp_gkg_ipc_mapping.db'\n",
    "\n",
    "print(f\"Locations file: {LOCATIONS_FILE}\")\n",
    "print(f\"Articles file: {ARTICLES_FILE}\")\n",
    "print(f\"IPC reference: {IPC_REF_FILE}\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")\n",
    "print(f\"SQLite temp: {SQLITE_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constants-header",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal aggregation window\n",
    "AGGREGATION_MONTHS = 4  # 3 months before + assessment month\n",
    "\n",
    "# Fuzzy matching threshold\n",
    "FUZZY_THRESHOLD = 80  # 80% similarity\n",
    "\n",
    "# SQLite batch size for inserts\n",
    "SQLITE_BATCH_SIZE = 50000\n",
    "\n",
    "# LHZ countries (use word-based lookup)\n",
    "LHZ_COUNTRIES = ['Zimbabwe', 'Burundi', 'Kenya']\n",
    "\n",
    "# FIPS to Country mapping (from 02a)\n",
    "FIPS_TO_COUNTRY = {\n",
    "    'AO': 'Angola', 'UV': 'Burkina Faso', 'BY': 'Burundi', 'CM': 'Cameroon',\n",
    "    'CT': 'Central African Republic', 'CD': 'Chad', 'CG': 'Democratic Republic of the Congo',\n",
    "    'ET': 'Ethiopia', 'KE': 'Kenya', 'LT': 'Lesotho', 'MA': 'Madagascar',\n",
    "    'MI': 'Malawi', 'ML': 'Mali', 'MR': 'Mauritania', 'MZ': 'Mozambique',\n",
    "    'NG': 'Niger', 'NI': 'Nigeria', 'RW': 'Rwanda', 'SO': 'Somalia',\n",
    "    'OD': 'South Sudan', 'SU': 'Sudan', 'TO': 'Togo', 'UG': 'Uganda', 'ZI': 'Zimbabwe'\n",
    "}\n",
    "\n",
    "print(f\"Aggregation window: {AGGREGATION_MONTHS} months\")\n",
    "print(f\"Fuzzy threshold: {FUZZY_THRESHOLD}%\")\n",
    "print(f\"SQLite batch size: {SQLITE_BATCH_SIZE:,}\")\n",
    "print(f\"LHZ countries: {LHZ_COUNTRIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Functions for text normalization, fuzzy matching, and lookup building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for matching: lowercase, strip, remove accents and special chars\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = str(text).lower().strip()\n",
    "    # Remove accents using unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    # Remove special characters but keep spaces\n",
    "    text = ''.join(c if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "    # Collapse multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Test\n",
    "print(\"Test normalization:\")\n",
    "print(f\"  'Kasaï Central' → '{normalize_text('Kasaï Central')}'\")\n",
    "print(f\"  'Nord-Kivu' → '{normalize_text('Nord-Kivu')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-match-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fuzzy_match(loc_name_norm, country_candidates, country_code):\n",
    "    \"\"\"\n",
    "    Find best fuzzy match for a location name among IPC district candidates.\n",
    "    Returns list of IPC records if match found, else empty list.\n",
    "    \"\"\"\n",
    "    if country_code not in country_candidates:\n",
    "        return []\n",
    "    \n",
    "    best_score = 0\n",
    "    best_match = []\n",
    "    \n",
    "    # Try fuzzy matching against all district names in this country\n",
    "    for district_norm, ipc_list in country_candidates[country_code]:\n",
    "        score = fuzz.ratio(loc_name_norm, district_norm)\n",
    "        if score >= FUZZY_THRESHOLD and score > best_score:\n",
    "            best_score = score\n",
    "            best_match = ipc_list\n",
    "    \n",
    "    return best_match\n",
    "\n",
    "print(\"find_fuzzy_match defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "word-lookup-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_lookup(ipc_ref):\n",
    "    \"\"\"\n",
    "    Build word-based lookup for LHZ countries (Zimbabwe, Burundi, Kenya).\n",
    "    \n",
    "    For these countries, IPC records often use long LHZ names that don't match\n",
    "    GADM district names. Instead, we extract words from full_name_normalized\n",
    "    and create lookup by individual words.\n",
    "    \"\"\"\n",
    "    word_lookup = {}\n",
    "    \n",
    "    for country in LHZ_COUNTRIES:\n",
    "        country_ipc = ipc_ref[ipc_ref['country'] == country]\n",
    "        word_lookup[country] = {}\n",
    "        \n",
    "        for _, row in country_ipc.iterrows():\n",
    "            full_name_norm = row['full_name_normalized']\n",
    "            if pd.notna(full_name_norm):\n",
    "                words = full_name_norm.split()\n",
    "                for word in words:\n",
    "                    if len(word) >= 3:  # Skip very short words\n",
    "                        if word not in word_lookup[country]:\n",
    "                            word_lookup[country][word] = []\n",
    "                        word_lookup[country][word].append(row.to_dict())\n",
    "    \n",
    "    return word_lookup\n",
    "\n",
    "print(\"build_word_lookup defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combine-lookups-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_lookups(district_lookup, word_lookup):\n",
    "    \"\"\"\n",
    "    Combine district-based and word-based lookups into unified structure.\n",
    "    \n",
    "    Returns: Dictionary with keys (country_fips, district_normalized) → list of IPC records\n",
    "    \"\"\"\n",
    "    combined = {}\n",
    "    \n",
    "    # Add district lookup\n",
    "    combined.update(district_lookup)\n",
    "    \n",
    "    # Add word lookup for LHZ countries\n",
    "    for country, words in word_lookup.items():\n",
    "        # Get FIPS code for country\n",
    "        country_to_fips = {v: k for k, v in FIPS_TO_COUNTRY.items()}\n",
    "        fips = country_to_fips.get(country)\n",
    "        if not fips:\n",
    "            continue\n",
    "        \n",
    "        for word, ipc_list in words.items():\n",
    "            key = (fips, word)\n",
    "            if key not in combined:\n",
    "                combined[key] = ipc_list\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"combine_lookups defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precompute-fuzzy-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_fuzzy_matches(country_candidates, all_gadm_names_by_country):\n",
    "    \"\"\"\n",
    "    Pre-compute ALL fuzzy matches at startup to avoid repeated computation.\n",
    "    \n",
    "    For each unique GADM name in the dataset, compute the best fuzzy match\n",
    "    against IPC districts. Store results in a dictionary for instant lookup.\n",
    "    \n",
    "    This trades memory for speed - dramatically reduces processing time.\n",
    "    \"\"\"\n",
    "    print(\"\\nPre-computing fuzzy matches for all unique GADM names...\")\n",
    "    \n",
    "    fuzzy_cache = {}\n",
    "    total_names = sum(len(names) for names in all_gadm_names_by_country.values())\n",
    "    \n",
    "    processed = 0\n",
    "    for country_code, gadm_names in all_gadm_names_by_country.items():\n",
    "        if country_code not in country_candidates:\n",
    "            continue\n",
    "        \n",
    "        for gadm_name_norm in gadm_names:\n",
    "            # Find best fuzzy match\n",
    "            best_match = find_fuzzy_match(gadm_name_norm, country_candidates, country_code)\n",
    "            fuzzy_cache[(country_code, gadm_name_norm)] = best_match\n",
    "            \n",
    "            processed += 1\n",
    "            if processed % 1000 == 0:\n",
    "                print(f\"   Processed {processed:,}/{total_names:,} names\", end='\\r')\n",
    "    \n",
    "    print(f\"   Pre-computed {len(fuzzy_cache):,} fuzzy match mappings\" + \" \" * 30)\n",
    "    return fuzzy_cache\n",
    "\n",
    "print(\"precompute_fuzzy_matches defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ipc-info-to-dict-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipc_info_to_dict(ipc_info):\n",
    "    \"\"\"\n",
    "    Convert IPC record to dictionary for database insertion.\n",
    "    Extracts key fields needed for downstream analysis.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'ipc_id': ipc_info.get('ipc_id'),\n",
    "        'ipc_country': ipc_info.get('country'),\n",
    "        'ipc_country_code': ipc_info.get('country_code'),\n",
    "        'ipc_fips_code': ipc_info.get('fips_code'),\n",
    "        'ipc_district': ipc_info.get('district'),\n",
    "        'ipc_region': ipc_info.get('region'),\n",
    "        'ipc_geographic_unit': ipc_info.get('geographic_unit_name'),\n",
    "        'ipc_geographic_unit_full': ipc_info.get('geographic_unit_full_name'),\n",
    "        'ipc_period_start': ipc_info.get('projection_start'),\n",
    "        'ipc_period_end': ipc_info.get('projection_end'),\n",
    "        'ipc_period_length_days': ipc_info.get('period_length_days'),\n",
    "        'ipc_value': ipc_info.get('ipc_value'),\n",
    "        'ipc_description': ipc_info.get('ipc_description'),\n",
    "        'ipc_binary_crisis': ipc_info.get('ipc_binary_crisis'),\n",
    "        'ipc_is_allowing_assistance': ipc_info.get('is_allowing_for_assistance'),\n",
    "        'ipc_fewsnet_region': ipc_info.get('fewsnet_region'),\n",
    "        'ipc_geographic_group': ipc_info.get('geographic_group'),\n",
    "        'ipc_scenario': ipc_info.get('scenario'),\n",
    "        'ipc_classification_scale': ipc_info.get('classification_scale'),\n",
    "        'ipc_reporting_date': ipc_info.get('reporting_date')\n",
    "    }\n",
    "\n",
    "print(\"ipc_info_to_dict defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-match-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_for_row(gadm3_norm, gadm2_norm, gadm1_norm, country_code,\n",
    "                       ipc_lookup, country_candidates, fuzzy_cache):\n",
    "    \"\"\"\n",
    "    Find IPC match for a single row using hierarchical matching strategy.\n",
    "    \n",
    "    Priority order:\n",
    "    1. GADM3 exact match (district level)\n",
    "    2. GADM3 fuzzy match (>= 80% similarity)\n",
    "    3. GADM2 exact match (zone level)\n",
    "    4. GADM2 fuzzy match\n",
    "    5. GADM1 exact match (state/province level)\n",
    "    6. GADM1 fuzzy match\n",
    "    7. Country-level match (last resort)\n",
    "    \n",
    "    Returns: (ipc_records, match_level)\n",
    "    \"\"\"\n",
    "    # Try GADM3 exact\n",
    "    key = (country_code, gadm3_norm)\n",
    "    if key in ipc_lookup:\n",
    "        return ipc_lookup[key], 'GADM3_exact'\n",
    "    \n",
    "    # Try GADM3 fuzzy\n",
    "    if key in fuzzy_cache and fuzzy_cache[key]:\n",
    "        return fuzzy_cache[key], 'GADM3_fuzzy'\n",
    "    \n",
    "    # Try GADM2 exact\n",
    "    key = (country_code, gadm2_norm)\n",
    "    if key in ipc_lookup:\n",
    "        return ipc_lookup[key], 'GADM2_exact'\n",
    "    \n",
    "    # Try GADM2 fuzzy\n",
    "    if key in fuzzy_cache and fuzzy_cache[key]:\n",
    "        return fuzzy_cache[key], 'GADM2_fuzzy'\n",
    "    \n",
    "    # Try GADM1 exact\n",
    "    key = (country_code, gadm1_norm)\n",
    "    if key in ipc_lookup:\n",
    "        return ipc_lookup[key], 'GADM1_exact'\n",
    "    \n",
    "    # Try GADM1 fuzzy\n",
    "    if key in fuzzy_cache and fuzzy_cache[key]:\n",
    "        return fuzzy_cache[key], 'GADM1_fuzzy'\n",
    "    \n",
    "    # Try country-level\n",
    "    key = (country_code, '')\n",
    "    if key in ipc_lookup:\n",
    "        return ipc_lookup[key], 'country'\n",
    "    \n",
    "    return [], 'no_match'\n",
    "\n",
    "print(\"get_match_for_row defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-header",
   "metadata": {},
   "source": [
    "## Main Processing\n",
    "\n",
    "Load IPC reference, build lookups, process locations, and aggregate articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-ipc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Article Aggregation - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Start time: {datetime.now()}\\n\")\n",
    "\n",
    "# Load IPC reference\n",
    "print(\"1. Loading IPC reference...\")\n",
    "ipc_ref = pd.read_parquet(IPC_REF_FILE)\n",
    "print(f\"   Loaded {len(ipc_ref):,} IPC records\")\n",
    "print(f\"   Unique districts: {ipc_ref['district'].nunique():,}\")\n",
    "print(f\"   Date range: {ipc_ref['projection_start'].min()} to {ipc_ref['projection_end'].max()}\")\n",
    "\n",
    "# Show sample\n",
    "ipc_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-ipc-lookup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build IPC lookup dictionary\n",
    "print(\"\\n2. Building IPC lookup dictionary...\")\n",
    "\n",
    "# Group by (country_fips, district_normalized)\n",
    "ipc_lookup = {}\n",
    "\n",
    "for _, row in ipc_ref.iterrows():\n",
    "    key = (row['fips_code'], row['district_normalized'])\n",
    "    if key not in ipc_lookup:\n",
    "        ipc_lookup[key] = []\n",
    "    ipc_lookup[key].append(row.to_dict())\n",
    "\n",
    "print(f\"   Created lookup with {len(ipc_lookup):,} unique (country, district) keys\")\n",
    "\n",
    "# Build country-level lookup for fallback\n",
    "country_lookup = {}\n",
    "for _, row in ipc_ref.iterrows():\n",
    "    key = (row['fips_code'], '')\n",
    "    if key not in country_lookup:\n",
    "        country_lookup[key] = []\n",
    "    country_lookup[key].append(row.to_dict())\n",
    "\n",
    "# Merge lookups\n",
    "ipc_lookup.update(country_lookup)\n",
    "\n",
    "print(f\"   Total lookup keys: {len(ipc_lookup):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-word-lookup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word-based lookup for LHZ countries\n",
    "print(\"\\n3. Building word-based lookup for LHZ countries...\")\n",
    "word_lookup = build_word_lookup(ipc_ref)\n",
    "\n",
    "for country, words in word_lookup.items():\n",
    "    print(f\"   {country}: {len(words):,} unique words\")\n",
    "\n",
    "# Combine lookups\n",
    "ipc_lookup = combine_lookups(ipc_lookup, word_lookup)\n",
    "print(f\"\\n   Combined lookup size: {len(ipc_lookup):,} keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-country-candidates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build country candidates for fuzzy matching\n",
    "print(\"\\n4. Building country candidates for fuzzy matching...\")\n",
    "\n",
    "country_candidates = {}\n",
    "\n",
    "for fips_code, country_name in FIPS_TO_COUNTRY.items():\n",
    "    country_ipc = ipc_ref[ipc_ref['fips_code'] == fips_code]\n",
    "    candidates = []\n",
    "    \n",
    "    for district_norm in country_ipc['district_normalized'].unique():\n",
    "        if pd.notna(district_norm) and district_norm != '':\n",
    "            ipc_list = country_ipc[country_ipc['district_normalized'] == district_norm].to_dict('records')\n",
    "            candidates.append((district_norm, ipc_list))\n",
    "    \n",
    "    country_candidates[fips_code] = candidates\n",
    "\n",
    "for fips, candidates in list(country_candidates.items())[:5]:\n",
    "    print(f\"   {FIPS_TO_COUNTRY[fips]}: {len(candidates)} district candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sqlite-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SQLite database for memory-efficient processing\n",
    "print(\"\\n5. Setting up SQLite database...\")\n",
    "\n",
    "# Remove existing database if present\n",
    "if SQLITE_FILE.exists():\n",
    "    SQLITE_FILE.unlink()\n",
    "    print(\"   Removed existing SQLite file\")\n",
    "\n",
    "# Create new database\n",
    "conn = sqlite3.connect(str(SQLITE_FILE))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Enable WAL mode for better performance\n",
    "cursor.execute(\"PRAGMA journal_mode=WAL\")\n",
    "cursor.execute(\"PRAGMA synchronous=OFF\")\n",
    "cursor.execute(\"PRAGMA cache_size=-256000\")  # 256MB cache\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "CREATE TABLE gkg_ipc (\n",
    "    GKGRECORDID TEXT,\n",
    "    match_level TEXT,\n",
    "    ipc_id TEXT,\n",
    "    ipc_country TEXT,\n",
    "    ipc_country_code TEXT,\n",
    "    ipc_fips_code TEXT,\n",
    "    ipc_district TEXT,\n",
    "    ipc_region TEXT,\n",
    "    ipc_geographic_unit TEXT,\n",
    "    ipc_geographic_unit_full TEXT,\n",
    "    ipc_period_start TEXT,\n",
    "    ipc_period_end TEXT,\n",
    "    ipc_period_length_days INTEGER,\n",
    "    ipc_value REAL,\n",
    "    ipc_description TEXT,\n",
    "    ipc_binary_crisis INTEGER,\n",
    "    ipc_is_allowing_assistance TEXT,\n",
    "    ipc_fewsnet_region TEXT,\n",
    "    ipc_geographic_group TEXT,\n",
    "    ipc_scenario TEXT,\n",
    "    ipc_classification_scale TEXT,\n",
    "    ipc_reporting_date TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "print(\"   SQLite database created and configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scan-gadm-names",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan locations to get unique GADM names by country\n",
    "print(\"\\n6. Scanning locations for unique GADM names...\")\n",
    "locations = pd.read_parquet(LOCATIONS_FILE)\n",
    "\n",
    "print(f\"   Loaded {len(locations):,} location mentions\")\n",
    "\n",
    "# Normalize GADM names\n",
    "locations['GADM3_normalized'] = locations['GADM3'].apply(normalize_text)\n",
    "locations['GADM2_normalized'] = locations['GADM2'].apply(normalize_text)\n",
    "locations['GADM1_normalized'] = locations['GADM1'].apply(normalize_text)\n",
    "\n",
    "# Get unique GADM names by country for fuzzy matching\n",
    "all_gadm_names_by_country = {}\n",
    "\n",
    "for country_code in locations['ActionGeo_CountryCode'].unique():\n",
    "    country_locs = locations[locations['ActionGeo_CountryCode'] == country_code]\n",
    "    unique_names = set()\n",
    "    \n",
    "    for col in ['GADM3_normalized', 'GADM2_normalized', 'GADM1_normalized']:\n",
    "        unique_names.update(country_locs[col].dropna().unique())\n",
    "    \n",
    "    all_gadm_names_by_country[country_code] = unique_names\n",
    "\n",
    "total_unique_names = sum(len(names) for names in all_gadm_names_by_country.values())\n",
    "print(f\"   Found {total_unique_names:,} unique GADM names across all countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precompute-fuzzy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute all fuzzy matches\n",
    "fuzzy_cache = precompute_fuzzy_matches(country_candidates, all_gadm_names_by_country)\n",
    "print(f\"   Fuzzy cache size: {len(fuzzy_cache):,} mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-locations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process locations and match to IPC districts\n",
    "print(\"\\n7. Processing locations and matching to IPC districts...\")\n",
    "\n",
    "# Process sequentially (avoid multiprocessing memory issues)\n",
    "insert_buffer = []\n",
    "total_matched = 0\n",
    "total_processed = 0\n",
    "\n",
    "# Use vectorized operations for efficiency\n",
    "gadm3_norms = locations['GADM3_normalized'].values\n",
    "gadm2_norms = locations['GADM2_normalized'].values\n",
    "gadm1_norms = locations['GADM1_normalized'].values\n",
    "country_codes = locations['ActionGeo_CountryCode'].values\n",
    "gkgids = locations['GKGRECORDID'].values\n",
    "\n",
    "batch_size = 100000\n",
    "n_batches = len(locations) // batch_size + 1\n",
    "\n",
    "print(f\"   Processing {len(locations):,} locations in {n_batches} batches...\")\n",
    "\n",
    "for batch_idx in range(n_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(locations))\n",
    "    \n",
    "    if start_idx >= len(locations):\n",
    "        break\n",
    "    \n",
    "    # Process batch\n",
    "    for i in range(start_idx, end_idx):\n",
    "        gadm3_norm = gadm3_norms[i]\n",
    "        gadm2_norm = gadm2_norms[i]\n",
    "        gadm1_norm = gadm1_norms[i]\n",
    "        country_code = country_codes[i]\n",
    "        gkgid = gkgids[i]\n",
    "        \n",
    "        # Get match\n",
    "        ipc_records, match_level = get_match_for_row(\n",
    "            gadm3_norm, gadm2_norm, gadm1_norm, country_code,\n",
    "            ipc_lookup, country_candidates, fuzzy_cache\n",
    "        )\n",
    "        \n",
    "        if ipc_records:\n",
    "            # Add to buffer for each IPC record\n",
    "            for ipc_info in ipc_records:\n",
    "                ipc_dict = ipc_info_to_dict(ipc_info)\n",
    "                row_tuple = (\n",
    "                    gkgid, match_level,\n",
    "                    ipc_dict['ipc_id'], ipc_dict['ipc_country'], ipc_dict['ipc_country_code'],\n",
    "                    ipc_dict['ipc_fips_code'], ipc_dict['ipc_district'], ipc_dict['ipc_region'],\n",
    "                    ipc_dict['ipc_geographic_unit'], ipc_dict['ipc_geographic_unit_full'],\n",
    "                    str(ipc_dict['ipc_period_start']), str(ipc_dict['ipc_period_end']),\n",
    "                    ipc_dict['ipc_period_length_days'], ipc_dict['ipc_value'],\n",
    "                    ipc_dict['ipc_description'], ipc_dict['ipc_binary_crisis'],\n",
    "                    ipc_dict['ipc_is_allowing_assistance'], ipc_dict['ipc_fewsnet_region'],\n",
    "                    ipc_dict['ipc_geographic_group'], ipc_dict['ipc_scenario'],\n",
    "                    ipc_dict['ipc_classification_scale'], str(ipc_dict['ipc_reporting_date'])\n",
    "                )\n",
    "                insert_buffer.append(row_tuple)\n",
    "            \n",
    "            total_matched += 1\n",
    "        \n",
    "        total_processed += 1\n",
    "        \n",
    "        # Flush buffer if needed\n",
    "        if len(insert_buffer) >= SQLITE_BATCH_SIZE:\n",
    "            cursor.executemany('INSERT INTO gkg_ipc VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)', insert_buffer)\n",
    "            conn.commit()\n",
    "            insert_buffer.clear()\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"   Batch {batch_idx+1}/{n_batches}: {total_processed:,} processed, {total_matched:,} matched ({total_matched/total_processed*100:.1f}%)\")\n",
    "\n",
    "# Flush remaining buffer\n",
    "if insert_buffer:\n",
    "    cursor.executemany('INSERT INTO gkg_ipc VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)', insert_buffer)\n",
    "    conn.commit()\n",
    "    insert_buffer.clear()\n",
    "\n",
    "print(f\"\\n   Total locations matched: {total_matched:,} / {total_processed:,} ({total_matched/total_processed*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deduplicate-sqlite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate in SQLite using GROUP BY\n",
    "print(\"\\n8. Deduplicating GKG-IPC mappings...\")\n",
    "\n",
    "# Count before deduplication\n",
    "cursor.execute(\"SELECT COUNT(*) FROM gkg_ipc\")\n",
    "before_count = cursor.fetchone()[0]\n",
    "print(f\"   Before deduplication: {before_count:,} rows\")\n",
    "\n",
    "# Create deduplicated table\n",
    "cursor.execute('''\n",
    "CREATE TABLE gkg_ipc_dedup AS\n",
    "SELECT DISTINCT * FROM gkg_ipc\n",
    "''')\n",
    "\n",
    "# Count after deduplication\n",
    "cursor.execute(\"SELECT COUNT(*) FROM gkg_ipc_dedup\")\n",
    "after_count = cursor.fetchone()[0]\n",
    "print(f\"   After deduplication: {after_count:,} rows\")\n",
    "print(f\"   Removed {before_count - after_count:,} duplicates\")\n",
    "\n",
    "# Drop original table and rename\n",
    "cursor.execute(\"DROP TABLE gkg_ipc\")\n",
    "cursor.execute(\"ALTER TABLE gkg_ipc_dedup RENAME TO gkg_ipc\")\n",
    "conn.commit()\n",
    "\n",
    "print(\"   Deduplication complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for faster lookups\n",
    "print(\"\\n9. Creating SQLite index...\")\n",
    "cursor.execute(\"CREATE INDEX idx_gkgid ON gkg_ipc(GKGRECORDID)\")\n",
    "conn.commit()\n",
    "print(\"   Index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-articles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles in chunks and merge with GKG-IPC mappings\n",
    "print(\"\\n10. Loading articles and merging with IPC mappings...\")\n",
    "\n",
    "# Process articles in chunks to manage memory\n",
    "chunksize = 100000\n",
    "article_chunks = []\n",
    "\n",
    "for chunk_idx, chunk in enumerate(pd.read_csv(ARTICLES_FILE, chunksize=chunksize)):\n",
    "    # Query SQLite for matching IPC records\n",
    "    gkgids = chunk['GKGRECORDID'].unique()\n",
    "    placeholders = ','.join(['?' for _ in gkgids])\n",
    "    \n",
    "    query = f\"SELECT * FROM gkg_ipc WHERE GKGRECORDID IN ({placeholders})\"\n",
    "    gkg_ipc_chunk = pd.read_sql_query(query, conn, params=gkgids)\n",
    "    \n",
    "    if len(gkg_ipc_chunk) > 0:\n",
    "        # Merge chunk with IPC mappings\n",
    "        merged = chunk.merge(gkg_ipc_chunk, on='GKGRECORDID', how='inner')\n",
    "        article_chunks.append(merged)\n",
    "    \n",
    "    if (chunk_idx + 1) % 10 == 0:\n",
    "        print(f\"   Processed {(chunk_idx + 1) * chunksize:,} articles\", end='\\r')\n",
    "\n",
    "print(f\"\\n   Concatenating {len(article_chunks)} chunks...\")\n",
    "articles_with_ipc = pd.concat(article_chunks, ignore_index=True)\n",
    "\n",
    "print(f\"   Merged articles: {len(articles_with_ipc):,}\")\n",
    "print(f\"   Unique GKGRECORDIDs: {articles_with_ipc['GKGRECORDID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-sqlite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close SQLite connection\n",
    "conn.close()\n",
    "print(\"\\n   SQLite connection closed\")\n",
    "\n",
    "# Optionally remove SQLite file\n",
    "# SQLITE_FILE.unlink()\n",
    "# print(\"   SQLite file removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-aggregation-header",
   "metadata": {},
   "source": [
    "## Temporal Aggregation\n",
    "\n",
    "Aggregate articles to IPC district-period observations using 4-month windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter articles to temporal window\n",
    "print(\"\\n11. Filtering articles to temporal windows...\")\n",
    "\n",
    "# Convert dates\n",
    "articles_with_ipc['DATE'] = pd.to_datetime(articles_with_ipc['DATE'], format='%Y%m%d')\n",
    "articles_with_ipc['ipc_period_start'] = pd.to_datetime(articles_with_ipc['ipc_period_start'])\n",
    "articles_with_ipc['ipc_period_end'] = pd.to_datetime(articles_with_ipc['ipc_period_end'])\n",
    "\n",
    "# Calculate temporal window (3 months before + assessment month)\n",
    "window_start_offset = pd.DateOffset(months=AGGREGATION_MONTHS - 1)\n",
    "\n",
    "articles_with_ipc['window_start'] = articles_with_ipc['ipc_period_start'] - window_start_offset\n",
    "articles_with_ipc['window_end'] = articles_with_ipc['ipc_period_end']\n",
    "\n",
    "# Filter to articles within window\n",
    "articles_filtered = articles_with_ipc[\n",
    "    (articles_with_ipc['DATE'] >= articles_with_ipc['window_start']) &\n",
    "    (articles_with_ipc['DATE'] <= articles_with_ipc['window_end'])\n",
    "].copy()\n",
    "\n",
    "print(f\"   Before temporal filter: {len(articles_with_ipc):,} article-IPC pairs\")\n",
    "print(f\"   After temporal filter: {len(articles_filtered):,} article-IPC pairs\")\n",
    "print(f\"   Filtered out: {len(articles_with_ipc) - len(articles_filtered):,} pairs ({(1 - len(articles_filtered)/len(articles_with_ipc))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-by-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by (IPC record, period)\n",
    "print(\"\\n12. Aggregating articles by IPC district-period...\")\n",
    "\n",
    "# Group by IPC geographic_unit_full and period\n",
    "group_cols = [\n",
    "    'ipc_id', 'ipc_country', 'ipc_country_code', 'ipc_fips_code',\n",
    "    'ipc_district', 'ipc_region', 'ipc_geographic_unit', 'ipc_geographic_unit_full',\n",
    "    'ipc_period_start', 'ipc_period_end', 'ipc_period_length_days',\n",
    "    'ipc_value', 'ipc_description', 'ipc_binary_crisis',\n",
    "    'ipc_is_allowing_assistance', 'ipc_fewsnet_region', 'ipc_geographic_group',\n",
    "    'ipc_scenario', 'ipc_classification_scale', 'ipc_reporting_date',\n",
    "    'match_level'\n",
    "]\n",
    "\n",
    "# Aggregation functions\n",
    "agg_dict = {\n",
    "    'GKGRECORDID': 'nunique',  # Unique articles\n",
    "    'GoldsteinScale': ['mean', 'std', 'min', 'max'],\n",
    "    'NumMentions': ['sum', 'mean', 'std'],\n",
    "    'AvgTone': ['mean', 'std', 'min', 'max'],\n",
    "    'DATE': 'nunique',  # Unique days\n",
    "    'SourceCommonName': 'nunique'  # Unique sources\n",
    "}\n",
    "\n",
    "articles_agg = articles_filtered.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "# Flatten column names\n",
    "articles_agg.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in articles_agg.columns.values]\n",
    "\n",
    "# Rename columns\n",
    "rename_dict = {\n",
    "    'GKGRECORDID_nunique': 'article_count',\n",
    "    'GoldsteinScale_mean': 'avg_goldstein',\n",
    "    'GoldsteinScale_std': 'std_goldstein',\n",
    "    'GoldsteinScale_min': 'min_goldstein',\n",
    "    'GoldsteinScale_max': 'max_goldstein',\n",
    "    'NumMentions_sum': 'total_mentions',\n",
    "    'NumMentions_mean': 'avg_mentions',\n",
    "    'NumMentions_std': 'std_mentions',\n",
    "    'AvgTone_mean': 'avg_tone',\n",
    "    'AvgTone_std': 'std_tone',\n",
    "    'AvgTone_min': 'min_tone',\n",
    "    'AvgTone_max': 'max_tone',\n",
    "    'DATE_nunique': 'unique_days',\n",
    "    'SourceCommonName_nunique': 'unique_sources'\n",
    "}\n",
    "\n",
    "articles_agg = articles_agg.rename(columns=rename_dict)\n",
    "\n",
    "print(f\"   Aggregated to {len(articles_agg):,} district-period observations\")\n",
    "print(f\"   Unique districts: {articles_agg['ipc_district'].nunique():,}\")\n",
    "print(f\"   Unique IPC IDs: {articles_agg['ipc_id'].nunique():,}\")\n",
    "print(f\"   Countries: {articles_agg['ipc_country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Article Aggregation Summary - DISTRICT LEVEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal district-period observations: {len(articles_agg):,}\")\n",
    "print(f\"Unique districts: {articles_agg['ipc_district'].nunique():,}\")\n",
    "print(f\"Unique IPC assessments: {articles_agg['ipc_id'].nunique():,}\")\n",
    "print(f\"Date range: {articles_agg['ipc_period_start'].min()} to {articles_agg['ipc_period_end'].max()}\")\n",
    "print(f\"Countries: {articles_agg['ipc_country'].nunique()}\")\n",
    "\n",
    "print(f\"\\nArticle statistics:\")\n",
    "print(f\"   Total articles: {articles_agg['article_count'].sum():,}\")\n",
    "print(f\"   Mean articles per district-period: {articles_agg['article_count'].mean():.1f}\")\n",
    "print(f\"   Median articles per district-period: {articles_agg['article_count'].median():.1f}\")\n",
    "print(f\"   Max articles per district-period: {articles_agg['article_count'].max():,}\")\n",
    "\n",
    "print(f\"\\nMatch level distribution:\")\n",
    "print(articles_agg['match_level'].value_counts())\n",
    "\n",
    "print(f\"\\nDistricts per country:\")\n",
    "district_counts = articles_agg.groupby('ipc_country')['ipc_district'].nunique().sort_values(ascending=False)\n",
    "for country, count in district_counts.head(15).items():\n",
    "    records = len(articles_agg[articles_agg['ipc_country'] == country])\n",
    "    articles = articles_agg[articles_agg['ipc_country'] == country]['article_count'].sum()\n",
    "    print(f\"   {country}: {count} districts, {records:,} periods, {articles:,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-header",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Verification Checks\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in key columns:\")\n",
    "for col in ['ipc_id', 'ipc_district', 'ipc_period_start', 'ipc_value', 'article_count']:\n",
    "    missing = articles_agg[col].isna().sum()\n",
    "    print(f\"   {col}: {missing} ({missing/len(articles_agg)*100:.1f}%)\")\n",
    "\n",
    "# Check uniqueness\n",
    "unique_check = articles_agg.groupby(['ipc_geographic_unit_full', 'ipc_period_start']).size()\n",
    "duplicates = (unique_check > 1).sum()\n",
    "print(f\"\\n(geographic_unit_full, period) duplicates: {duplicates}\")\n",
    "print(\"(Expected: 0 - each observation should be unique)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nWARNING: Found duplicate observations. Investigation needed.\")\n",
    "    dup_idx = unique_check[unique_check > 1].head(3).index\n",
    "    for idx in dup_idx:\n",
    "        dup_rows = articles_agg[\n",
    "            (articles_agg['ipc_geographic_unit_full'] == idx[0]) &\n",
    "            (articles_agg['ipc_period_start'] == idx[1])\n",
    "        ]\n",
    "        print(f\"\\n   Duplicate: {idx[0][:50]}..., {idx[1]}\")\n",
    "        print(f\"   Rows: {len(dup_rows)}, IPC values: {dup_rows['ipc_value'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "print(f\"\\n13. Saving to {OUTPUT_FILE}...\")\n",
    "articles_agg.to_parquet(OUTPUT_FILE, index=False)\n",
    "print(\"   [OK] Parquet saved\")\n",
    "\n",
    "# Also save as CSV for inspection\n",
    "print(f\"\\n14. Saving CSV to {OUTPUT_CSV}...\")\n",
    "articles_agg.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"   [OK] CSV saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Article Aggregation Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nKey columns for downstream scripts:\")\n",
    "print(\"   - ipc_id: Unique IPC assessment ID\")\n",
    "print(\"   - ipc_geographic_unit_full: THE PRIMARY GEOGRAPHIC IDENTIFIER\")\n",
    "print(\"   - ipc_district: Extracted district name\")\n",
    "print(\"   - article_count: Number of articles in 4-month window\")\n",
    "print(\"   - avg_goldstein, avg_tone: Aggregated event characteristics\")\n",
    "print(f\"\\nEnd time: {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
