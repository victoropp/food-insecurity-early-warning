{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079c0840",
   "metadata": {},
   "source": [
    "# Phase 1: District Threshold Analysis\n",
    "\n",
    "**Script**: `scripts\\04_stage2_feature_engineering\\Phase1_District_Threshold\\01_district_threshold_analysis.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Analyzes Stage 1 AR baseline performance to determine district inclusion threshold.\n",
    "\n",
    "**KEY TASK**: Identify districts with sufficient article counts for reliable feature engineering.\n",
    "\n",
    "Uses h=8 predictions to determine minimum article count threshold.\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a604db",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBoost Pipeline - Phase 1: District Threshold Analysis\n",
    "=======================================================\n",
    "Apply 200 articles/year threshold at DISTRICT level for XGBoost pipeline.\n",
    "\n",
    "Self-contained script with XGBoost-specific paths.\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "Date: December 19, 2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for config import\n",
    "sys.path.append(str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "# Import from config\n",
    "from config import (\n",
    "    BASE_DIR,\n",
    "    STAGE1_DATA_DIR,\n",
    "    STAGE1_RESULTS_DIR,\n",
    "    STAGE2_FEATURES_DIR,\n",
    "    STAGE2_MODELS_DIR,\n",
    "    FIGURES_DIR,\n",
    "    RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c527caf",
   "metadata": {},
   "source": [
    "## Load Stage 1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1adc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# XGBOOST PIPELINE PATHS (SELF-CONTAINED)\n",
    "# =============================================================================\n",
    "\n",
    "# Use self-contained paths from config\n",
    "from config import STAGE2_DATA_DIR, STAGE2_FEATURES_DIR\n",
    "\n",
    "# Phase 1 output directory\n",
    "PHASE1_OUTPUT = STAGE2_FEATURES_DIR / \"phase1_district_threshold\"\n",
    "PHASE1_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input from self-contained Stage 2 data\n",
    "MONTHLY_DATA_FILE = STAGE2_DATA_DIR / \"ml_dataset_monthly.parquet\"\n",
    "\n",
    "# Outputs\n",
    "PHASE1_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "VALID_DISTRICTS_FILE = PHASE1_OUTPUT / \"valid_districts.csv\"\n",
    "DISTRICT_STATS_FILE = PHASE1_OUTPUT / \"district_statistics.csv\"\n",
    "SUMMARY_FILE = PHASE1_OUTPUT / \"phase1_summary.json\"\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "MIN_ARTICLES_PER_YEAR = 200  # District-level threshold\n",
    "MIN_MONTHS_WITH_DATA = 12    # Minimum data availability\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST PIPELINE - PHASE 1: DISTRICT THRESHOLD ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nThreshold: {MIN_ARTICLES_PER_YEAR} articles/year per district\")\n",
    "print(f\"Minimum months: {MIN_MONTHS_WITH_DATA}\")\n",
    "print(f\"\\nStart time: {datetime.now()}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e76e0a",
   "metadata": {},
   "source": [
    "## Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"Loading monthly data...\")\n",
    "df = pd.read_parquet(MONTHLY_DATA_FILE)\n",
    "print(f\"   Loaded: {len(df):,} observations\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "\n",
    "# Use canonical identifier\n",
    "DISTRICT_COL = 'ipc_geographic_unit_full'\n",
    "if DISTRICT_COL not in df.columns:\n",
    "    raise ValueError(f\"{DISTRICT_COL} not found!\")\n",
    "print(f\"   District column: {DISTRICT_COL}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT YEAR\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Extracting year...\")\n",
    "\n",
    "if 'year' not in df.columns:\n",
    "    if 'year_month' in df.columns:\n",
    "        df['year'] = df['year_month'].str[:4].astype(int)\n",
    "    else:\n",
    "        df['year'] = pd.to_datetime(df['ipc_period_start']).dt.year\n",
    "\n",
    "print(f\"   Years: {sorted(df['year'].unique())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE ANNUAL ARTICLE COUNTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Computing annual article counts...\")\n",
    "\n",
    "annual = df.groupby([DISTRICT_COL, 'year']).agg({\n",
    "    'article_count': 'sum'\n",
    "}).reset_index()\n",
    "annual.columns = ['district', 'year', 'annual_articles']\n",
    "\n",
    "print(f\"   Unique districts: {annual['district'].nunique():,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE DISTRICT STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Computing district statistics...\")\n",
    "\n",
    "stats = annual.groupby('district').agg({\n",
    "    'annual_articles': ['mean', 'std', 'min', 'max', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "stats.columns = ['district', 'mean_annual', 'std_annual', 'min_annual', 'max_annual', 'n_years']\n",
    "stats['meets_threshold'] = stats['mean_annual'] >= MIN_ARTICLES_PER_YEAR\n",
    "\n",
    "print(f\"   Above threshold: {stats['meets_threshold'].sum():,}\")\n",
    "print(f\"   Below threshold: {(~stats['meets_threshold']).sum():,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK MONTHS WITH DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Checking data availability...\")\n",
    "\n",
    "month_counts = df.groupby(DISTRICT_COL).size().reset_index(name='n_months')\n",
    "month_counts.columns = ['district', 'n_months']\n",
    "\n",
    "stats = stats.merge(month_counts, on='district', how='left')\n",
    "stats['meets_months_req'] = stats['n_months'] >= MIN_MONTHS_WITH_DATA\n",
    "\n",
    "print(f\"   Districts with {MIN_MONTHS_WITH_DATA}+ months: {stats['meets_months_req'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1f865",
   "metadata": {},
   "source": [
    "## Visualization and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# IDENTIFY VALID DISTRICTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Identifying valid districts...\")\n",
    "\n",
    "stats['is_valid'] = stats['meets_threshold'] & stats['meets_months_req']\n",
    "valid_districts = stats[stats['is_valid']].copy()\n",
    "\n",
    "print(f\"   Valid districts: {len(valid_districts):,}\")\n",
    "\n",
    "# Add geographic metadata\n",
    "district_meta = df[[DISTRICT_COL, 'ipc_country']].drop_duplicates()\n",
    "district_meta.columns = ['district', 'ipc_country']\n",
    "\n",
    "valid_districts = valid_districts.merge(district_meta, on='district', how='left')\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE OUTPUTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Saving outputs...\")\n",
    "\n",
    "# Valid districts\n",
    "valid_output = valid_districts[['ipc_country', 'district', 'mean_annual', 'n_months']].copy()\n",
    "valid_output.columns = ['ipc_country', 'ipc_geographic_unit_full', 'mean_annual_articles', 'n_months']\n",
    "\n",
    "# CRITICAL: Strip whitespace from district identifiers to match Phase 2 processing\n",
    "# (Source data has leading tabs that must be removed for consistency)\n",
    "valid_output['ipc_geographic_unit_full'] = valid_output['ipc_geographic_unit_full'].str.strip()\n",
    "valid_output['ipc_country'] = valid_output['ipc_country'].str.strip()\n",
    "\n",
    "# After stripping, some districts may become duplicates - keep best performing one\n",
    "pre_dedup = len(valid_output)\n",
    "valid_output = valid_output.sort_values('mean_annual_articles', ascending=False).drop_duplicates(\n",
    "    subset=['ipc_geographic_unit_full'], keep='first'\n",
    ")\n",
    "if len(valid_output) < pre_dedup:\n",
    "    print(f\"   Removed {pre_dedup - len(valid_output)} duplicates after whitespace stripping\")\n",
    "\n",
    "valid_output.to_csv(VALID_DISTRICTS_FILE, index=False)\n",
    "print(f\"   Saved: {VALID_DISTRICTS_FILE}\")\n",
    "\n",
    "# All district statistics\n",
    "stats.to_csv(DISTRICT_STATS_FILE, index=False)\n",
    "print(f\"   Saved: {DISTRICT_STATS_FILE}\")\n",
    "\n",
    "# Summary JSON\n",
    "summary = {\n",
    "    'phase': 'Phase 1 - District Threshold',\n",
    "    'pipeline': 'XGBoost_Pipeline',\n",
    "    'threshold': {\n",
    "        'min_articles_per_year': MIN_ARTICLES_PER_YEAR,\n",
    "        'min_months_with_data': MIN_MONTHS_WITH_DATA\n",
    "    },\n",
    "    'results': {\n",
    "        'total_districts': int(stats['district'].nunique()),\n",
    "        'valid_districts': int(len(valid_districts)),\n",
    "        'filtered_out': int(stats['district'].nunique() - len(valid_districts)),\n",
    "        'countries': int(valid_districts['ipc_country'].nunique())\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(SUMMARY_FILE, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"   Saved: {SUMMARY_FILE}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 1 COMPLETE: DISTRICT THRESHOLD ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n   Valid districts: {len(valid_districts):,}\")\n",
    "print(f\"   Countries: {valid_districts['ipc_country'].nunique()}\")\n",
    "print(f\"   Output: {PHASE1_OUTPUT}\")\n",
    "print(f\"\\nEnd time: {datetime.now()}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
