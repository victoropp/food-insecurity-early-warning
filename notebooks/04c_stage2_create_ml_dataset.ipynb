{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d871d9",
   "metadata": {},
   "source": [
    "# Stage 2: ML Dataset Creation\n",
    "\n",
    "**Script**: `scripts\\02_data_processing\\04a_stage2_create_ml_dataset.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Combines monthly articles and locations aggregations into unified ML-ready dataset.\n",
    "\n",
    "Merges on (ipc_geographic_unit_full, year_month).\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406956b5",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91362075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stage 2: Create Monthly ML Dataset\n",
    "Combines monthly articles and locations into unified dataset for z-score computation.\n",
    "\n",
    "KEY DIFFERENCE FROM STAGE 1 (Script 04):\n",
    "- Stage 1: Merges on IPC period keys (ipc_id, ipc_period_start, etc.)\n",
    "- Stage 2: Merges on (ipc_geographic_unit_full, year_month)\n",
    "\n",
    "This creates a monthly time series per district for rolling z-score computation.\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from config import BASE_DIR\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(str(BASE_DIR.parent.parent.parent))\n",
    "\n",
    "# District pipeline I/O\n",
    "DISTRICT_DATA_DIR = BASE_DIR / 'data' / 'district_level'\n",
    "STAGE2_DATA_DIR = DISTRICT_DATA_DIR / 'stage2'\n",
    "\n",
    "# Input files\n",
    "ARTICLES_FILE = STAGE2_DATA_DIR / 'articles_aggregated_monthly.parquet'\n",
    "LOCATIONS_FILE = STAGE2_DATA_DIR / 'locations_aggregated_monthly.parquet'\n",
    "\n",
    "# Output files\n",
    "OUTPUT_COMPLETE = STAGE2_DATA_DIR / 'ml_dataset_monthly.parquet'\n",
    "OUTPUT_CSV = STAGE2_DATA_DIR / 'ml_dataset_monthly.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c597746",
   "metadata": {},
   "source": [
    "## Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Stage 2: Create Monthly ML Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Start time: {datetime.now()}\")\n",
    "    print(\"\\nKEY DIFFERENCE from Stage 1:\")\n",
    "    print(\"  - Stage 1: Merges on IPC period keys\")\n",
    "    print(\"  - Stage 2: Merges on (ipc_geographic_unit_full, year_month)\")\n",
    "\n",
    "    # Load monthly aggregated articles\n",
    "    print(\"\\n1. Loading monthly aggregated articles...\")\n",
    "    if not ARTICLES_FILE.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Articles file not found: {ARTICLES_FILE}\\n\"\n",
    "            \"Run 02a_stage2_aggregate_articles_monthly.py first.\"\n",
    "        )\n",
    "\n",
    "    articles = pd.read_parquet(ARTICLES_FILE)\n",
    "    print(f\"   Loaded {len(articles):,} article aggregations\")\n",
    "    print(f\"   Unique districts: {articles['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"   Unique months: {articles['year_month'].nunique()}\")\n",
    "    print(f\"   Columns: {len(articles.columns)}\")\n",
    "\n",
    "    # Load monthly aggregated locations\n",
    "    print(\"\\n2. Loading monthly aggregated locations...\")\n",
    "    if not LOCATIONS_FILE.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Locations file not found: {LOCATIONS_FILE}\\n\"\n",
    "            \"Run 03a_stage2_aggregate_locations_monthly.py first.\"\n",
    "        )\n",
    "\n",
    "    locations = pd.read_parquet(LOCATIONS_FILE)\n",
    "    print(f\"   Loaded {len(locations):,} location aggregations\")\n",
    "    print(f\"   Unique districts: {locations['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"   Unique months: {locations['year_month'].nunique()}\")\n",
    "    print(f\"   Columns: {len(locations.columns)}\")\n",
    "\n",
    "    # Define merge keys for Stage 2 (district + month)\n",
    "    merge_keys = [\n",
    "        'ipc_country',\n",
    "        'ipc_country_code',\n",
    "        'ipc_fips_code',\n",
    "        'ipc_district',\n",
    "        'ipc_region',\n",
    "        'ipc_geographic_unit',\n",
    "        'ipc_geographic_unit_full',  # KEY: District identifier\n",
    "        'ipc_fewsnet_region',\n",
    "        'ipc_geographic_group',\n",
    "        'year_month'  # KEY: Monthly time bucket\n",
    "    ]\n",
    "\n",
    "    # Verify merge keys exist in both datasets\n",
    "    print(\"\\n3. Verifying merge keys...\")\n",
    "    articles_keys = set(merge_keys) & set(articles.columns)\n",
    "    locations_keys = set(merge_keys) & set(locations.columns)\n",
    "\n",
    "    print(f\"   Articles has {len(articles_keys)}/{len(merge_keys)} keys\")\n",
    "    print(f\"   Locations has {len(locations_keys)}/{len(merge_keys)} keys\")\n",
    "\n",
    "    # Use common keys\n",
    "    actual_merge_keys = list(articles_keys & locations_keys)\n",
    "    print(f\"   Using {len(actual_merge_keys)} merge keys\")\n",
    "\n",
    "    # Critical key validation\n",
    "    critical_keys = ['ipc_geographic_unit_full', 'year_month']\n",
    "    missing_critical = set(critical_keys) - set(actual_merge_keys)\n",
    "    if missing_critical:\n",
    "        raise ValueError(f\"CRITICAL ERROR: Missing essential merge keys: {missing_critical}\")\n",
    "\n",
    "    # Create merged dataset (outer join to preserve all data)\n",
    "    print(\"\\n4. Creating merged monthly dataset...\")\n",
    "    df_merged = articles.merge(\n",
    "        locations,\n",
    "        on=actual_merge_keys,\n",
    "        how='outer',\n",
    "        suffixes=('_articles', '_locations')\n",
    "    )\n",
    "\n",
    "    # Fill missing values with 0 for count columns\n",
    "    count_cols = [c for c in df_merged.columns if 'count' in c.lower() or 'article' in c.lower()]\n",
    "    for col in count_cols:\n",
    "        if df_merged[col].dtype in ['float64', 'int64']:\n",
    "            df_merged[col] = df_merged[col].fillna(0)\n",
    "\n",
    "    print(f\"   Result: {len(df_merged):,} rows\")\n",
    "    print(f\"   Unique districts: {df_merged['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"   Unique months: {df_merged['year_month'].nunique()}\")\n",
    "\n",
    "    # Handle match_level columns from both sources\n",
    "    if 'match_level_articles' in df_merged.columns and 'match_level_locations' in df_merged.columns:\n",
    "        df_merged['match_level'] = df_merged['match_level_locations'].fillna(\n",
    "            df_merged['match_level_articles']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ef9f6",
   "metadata": {},
   "source": [
    "## Validation and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "        df_merged = df_merged.drop(['match_level_articles', 'match_level_locations'], axis=1, errors='ignore')\n",
    "    elif 'match_level_articles' in df_merged.columns:\n",
    "        df_merged = df_merged.rename(columns={'match_level_articles': 'match_level'})\n",
    "    elif 'match_level_locations' in df_merged.columns:\n",
    "        df_merged = df_merged.rename(columns={'match_level_locations': 'match_level'})\n",
    "\n",
    "    # Extract date components\n",
    "    print(\"\\n5. Adding date components...\")\n",
    "    df_merged['year_month_dt'] = pd.to_datetime(df_merged['year_month'].astype(str))\n",
    "    df_merged['year'] = df_merged['year_month_dt'].dt.year\n",
    "    df_merged['month'] = df_merged['year_month_dt'].dt.month\n",
    "\n",
    "    # Sort by district and time for rolling computations\n",
    "    df_merged = df_merged.sort_values(['ipc_geographic_unit_full', 'year_month']).reset_index(drop=True)\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Monthly Dataset Summary - Stage 2\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nTotal records: {len(df_merged):,}\")\n",
    "    print(f\"Unique districts: {df_merged['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"Unique months: {df_merged['year_month'].nunique()}\")\n",
    "    print(f\"Month range: {df_merged['year_month'].min()} to {df_merged['year_month'].max()}\")\n",
    "    print(f\"Countries: {df_merged['ipc_country'].nunique()}\")\n",
    "\n",
    "    print(\"\\nRecords per country:\")\n",
    "    country_counts = df_merged.groupby('ipc_country').size().sort_values(ascending=False)\n",
    "    for country, count in country_counts.head(10).items():\n",
    "        districts = df_merged[df_merged['ipc_country'] == country]['ipc_geographic_unit_full'].nunique()\n",
    "        print(f\"   {country}: {count:,} records, {districts} districts\")\n",
    "\n",
    "    print(\"\\nMatch level distribution:\")\n",
    "    if 'match_level' in df_merged.columns:\n",
    "        print(df_merged['match_level'].value_counts())\n",
    "\n",
    "    # Verify data quality\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Data Quality Check\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Check for duplicates\n",
    "    unique_check = df_merged.groupby(['ipc_geographic_unit_full', 'year_month']).size()\n",
    "    duplicates = (unique_check > 1).sum()\n",
    "    print(f\"\\n(district, month) duplicates: {duplicates}\")\n",
    "    print(\"(Expected: 0 - each observation should be unique)\")\n",
    "\n",
    "    if duplicates > 0:\n",
    "        print(\"\\nWARNING: Found duplicate observations. Deduplicating...\")\n",
    "        # Keep first occurrence\n",
    "        df_merged = df_merged.drop_duplicates(subset=['ipc_geographic_unit_full', 'year_month'], keep='first')\n",
    "        print(f\"   After deduplication: {len(df_merged):,} rows\")\n",
    "\n",
    "    # Check months per district\n",
    "    months_per_district = df_merged.groupby('ipc_geographic_unit_full')['year_month'].nunique()\n",
    "    print(f\"\\nMonths per district:\")\n",
    "    print(f\"   Min: {months_per_district.min()}\")\n",
    "    print(f\"   Max: {months_per_district.max()}\")\n",
    "    print(f\"   Mean: {months_per_district.mean():.1f}\")\n",
    "    print(f\"   Median: {months_per_district.median():.1f}\")\n",
    "\n",
    "    # Check alignment with Stage 1 districts\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Alignment with Stage 1\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    stage1_articles = pd.read_parquet(DISTRICT_DATA_DIR / 'articles_aggregated.parquet')\n",
    "    stage1_districts = set(stage1_articles['ipc_geographic_unit_full'].unique())\n",
    "    stage2_districts = set(df_merged['ipc_geographic_unit_full'].unique())\n",
    "\n",
    "    overlap = stage1_districts & stage2_districts\n",
    "    only_stage1 = stage1_districts - stage2_districts\n",
    "    only_stage2 = stage2_districts - stage1_districts\n",
    "\n",
    "    print(f\"\\nStage 1 districts: {len(stage1_districts):,}\")\n",
    "    print(f\"Stage 2 districts: {len(stage2_districts):,}\")\n",
    "    print(f\"Overlap: {len(overlap):,} ({100*len(overlap)/len(stage1_districts):.1f}% of Stage 1)\")\n",
    "    print(f\"Only in Stage 1: {len(only_stage1):,}\")\n",
    "    print(f\"Only in Stage 2: {len(only_stage2):,}\")\n",
    "\n",
    "    if len(only_stage1) > 0 and len(only_stage1) <= 10:\n",
    "        print(f\"\\nDistricts only in Stage 1:\")\n",
    "        for d in list(only_stage1)[:10]:\n",
    "            print(f\"   {d}\")\n",
    "\n",
    "    # Save\n",
    "    print(f\"\\n6. Saving monthly ML dataset...\")\n",
    "    print(f\"   Parquet: {OUTPUT_COMPLETE}\")\n",
    "    df_merged.to_parquet(OUTPUT_COMPLETE, index=False)\n",
    "    print(\"   [OK] Parquet saved\")\n",
    "\n",
    "    print(f\"\\n   CSV: {OUTPUT_CSV}\")\n",
    "    df_merged.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(\"   [OK] CSV saved\")\n",
    "\n",
    "    # Print column summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Column Summary\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    ipc_cols = [c for c in df_merged.columns if c.startswith('ipc_')]\n",
    "    article_cols = [c for c in df_merged.columns if 'article' in c.lower() or 'category' in c.lower()]\n",
    "    location_cols = [c for c in df_merged.columns if 'location' in c.lower() or 'latitude' in c.lower() or 'longitude' in c.lower()]\n",
    "    time_cols = ['year', 'month', 'year_month', 'year_month_dt']\n",
    "\n",
    "    print(f\"\\nIPC metadata columns: {len(ipc_cols)}\")\n",
    "    print(f\"Article-derived columns: {len(article_cols)}\")\n",
    "    print(f\"Location-derived columns: {len(location_cols)}\")\n",
    "    print(f\"Total columns: {len(df_merged.columns)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Stage 2 Monthly ML Dataset Complete\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nOutput: {OUTPUT_COMPLETE}\")\n",
    "    print(f\"\\nNext step: Run 12_stage2_feature_engineering.py\")\n",
    "    print(f\"\\nEnd time: {datetime.now()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
