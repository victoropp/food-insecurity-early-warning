{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f0df8c",
   "metadata": {},
   "source": [
    "# Stage 2: Monthly Article Aggregation\n",
    "\n",
    "**Script**: `scripts\\02_data_processing\\02a_stage2_aggregate_articles_monthly.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Aggregates GDELT articles by (district, year-month) using Stage 1 matching logic.\n",
    "\n",
    "**KEY DIFFERENCE FROM STAGE 1**:\n",
    "- Stage 1: Aggregates within IPC assessment periods (Feb, Jun, Oct - ~3/year)\n",
    "- Stage 2: Aggregates by calendar month (all 12 months) for rolling z-score computation\n",
    "\n",
    "**CRITICAL**: Uses the SAME district matching logic as Stage 1 to ensure alignment with AR predictions.\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85364b5f",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stage 2: Monthly Article Aggregation for Dynamic News Features\n",
    "Aggregates GDELT articles by (district, year-month) using Stage 1 matching logic.\n",
    "\n",
    "KEY DIFFERENCE FROM STAGE 1 (Script 02):\n",
    "- Stage 1: Aggregates within IPC assessment periods (Feb, Jun, Oct - ~3/year)\n",
    "- Stage 2: Aggregates by calendar month (all 12 months) for rolling z-score computation\n",
    "\n",
    "CRITICAL: Uses the SAME district matching logic as Script 02 to ensure alignment\n",
    "with Stage 1 predictions and AR failures.\n",
    "\n",
    "Matching Priority: GADM3 → GADM2 → GADM1 → Country-level\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz  # 10-100x faster than fuzzywuzzy\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import unicodedata\n",
    "import gc\n",
    "import sqlite3\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import os\n",
    "from config import BASE_DIR\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(str(BASE_DIR.parent.parent.parent))\n",
    "\n",
    "# Input files (same as Stage 1)\n",
    "ARTICLES_FILE = BASE_DIR / 'data' / 'african_gkg_articles.csv'\n",
    "LOCATIONS_FILE = BASE_DIR / 'data' / 'african_gkg_locations_aligned.parquet'\n",
    "\n",
    "# District pipeline I/O\n",
    "DISTRICT_DATA_DIR = BASE_DIR / 'data' / 'district_level'\n",
    "IPC_REF_FILE = DISTRICT_DATA_DIR / 'ipc_reference.parquet'\n",
    "\n",
    "# Stage 2 output\n",
    "STAGE2_DATA_DIR = DISTRICT_DATA_DIR / 'stage2'\n",
    "OUTPUT_PARQUET = STAGE2_DATA_DIR / 'articles_aggregated_monthly.parquet'\n",
    "OUTPUT_CSV = STAGE2_DATA_DIR / 'articles_aggregated_monthly.csv'\n",
    "\n",
    "# Processing parameters\n",
    "CHUNK_SIZE = 100000  # Reduced for 8GB RAM systems\n",
    "FUZZY_THRESHOLD = 80  # Same as Stage 1\n",
    "NUM_WORKERS = max(1, multiprocessing.cpu_count() - 1)  # Use all but one CPU core\n",
    "SQLITE_BATCH_SIZE = 50000  # Batch size for SQLite inserts (memory-efficient)\n",
    "DEDUP_BATCH_SIZE = 5000000  # 5M rows per dedup batch for memory safety\n",
    "CONSOLIDATE_EVERY = 50  # Consolidate aggregations every 50 chunks\n",
    "SQL_QUERY_BATCH_SIZE = 500  # Max SQL placeholders per query\n",
    "\n",
    "# Global cache for fuzzy matches (populated at startup)\n",
    "FUZZY_MATCH_CACHE = {}\n",
    "\n",
    "# Countries with ONLY national-level IPC data (same as Stage 1)\n",
    "COUNTRY_LEVEL_ONLY = {'AO', 'CG', 'CT', 'LT', 'MR', 'RW', 'TO'}\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for matching with accent removal. (Same as Stage 1)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = str(text).lower().strip()\n",
    "    # Remove accents (e.g., Kasaï -> kasai, Équateur -> equateur)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    text = ''.join(c if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573b5d7",
   "metadata": {},
   "source": [
    "## Matching Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_fuzzy_match(loc_name_norm, country_candidates, country_code):\n",
    "    \"\"\"\n",
    "    Find best fuzzy match for location name among IPC district candidates.\n",
    "    Uses pre-grouped country_candidates for O(1) country lookup.\n",
    "    Returns: (ipc_info_list, match_score) or (None, 0)\n",
    "    (Same as Stage 1)\n",
    "    \"\"\"\n",
    "    if not loc_name_norm or country_code not in country_candidates:\n",
    "        return None, 0\n",
    "\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "\n",
    "    # Only iterate through candidates for this country (much faster!)\n",
    "    for district_norm, ipc_list in country_candidates[country_code]:\n",
    "        # Calculate fuzzy match score\n",
    "        score = fuzz.ratio(loc_name_norm, district_norm)\n",
    "\n",
    "        if score >= FUZZY_THRESHOLD and score > best_score:\n",
    "            best_score = score\n",
    "            best_match = ipc_list\n",
    "\n",
    "    return best_match, best_score\n",
    "\n",
    "\n",
    "def build_country_candidates(combined_lookup):\n",
    "    \"\"\"Pre-group candidates by country for fast fuzzy matching.\"\"\"\n",
    "    country_candidates = defaultdict(list)\n",
    "    for (fips, district_norm), ipc_list in combined_lookup.items():\n",
    "        country_candidates[fips].append((district_norm, ipc_list))\n",
    "    return dict(country_candidates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0092c",
   "metadata": {},
   "source": [
    "## Pre-compute Fuzzy Match Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precompute_fuzzy_matches(country_candidates, all_gadm_names_by_country):\n",
    "    \"\"\"\n",
    "    Pre-compute ALL fuzzy matches at startup to avoid repeated computation.\n",
    "    Returns: dict of {(country_code, gadm_norm): matched_district_list}\n",
    "    \"\"\"\n",
    "    print(\"   Pre-computing fuzzy matches (one-time cost)...\", flush=True)\n",
    "    cache = {}\n",
    "    total_computed = 0\n",
    "\n",
    "    for country_code, gadm_names in all_gadm_names_by_country.items():\n",
    "        if country_code not in country_candidates:\n",
    "            continue\n",
    "\n",
    "        candidates = country_candidates[country_code]\n",
    "        for gadm_norm in gadm_names:\n",
    "            if not gadm_norm:\n",
    "                continue\n",
    "\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "\n",
    "            for district_norm, district_list in candidates:\n",
    "                score = fuzz.ratio(gadm_norm, district_norm)\n",
    "                if score >= FUZZY_THRESHOLD and score > best_score:\n",
    "                    best_score = score\n",
    "                    best_match = district_list\n",
    "\n",
    "            if best_match:\n",
    "                cache[(country_code, gadm_norm)] = best_match\n",
    "                total_computed += 1\n",
    "\n",
    "    print(f\"   Pre-computed {total_computed:,} fuzzy matches\", flush=True)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def get_fuzzy_match_cached(gadm_norm, country_code, fuzzy_cache, country_candidates):\n",
    "    \"\"\"Get fuzzy match from cache, or compute if not cached.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9e7d6",
   "metadata": {},
   "source": [
    "## District Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f32133",
   "metadata": {},
   "outputs": [],
   "source": [
    "    key = (country_code, gadm_norm)\n",
    "    if key in fuzzy_cache:\n",
    "        return fuzzy_cache[key], FUZZY_THRESHOLD\n",
    "    # Fallback to computation (should be rare after pre-computation)\n",
    "    return find_fuzzy_match(gadm_norm, country_candidates, country_code)\n",
    "\n",
    "\n",
    "def build_district_lookup(ipc_ref):\n",
    "    \"\"\"\n",
    "    Build lookup dictionary for district matching.\n",
    "\n",
    "    KEY: This builds the SAME lookup as Stage 1 Script 02, but stores only\n",
    "    district metadata (not period-specific info) for monthly aggregation.\n",
    "\n",
    "    Returns: (ipc_lookup, word_lookup, unique_districts)\n",
    "    \"\"\"\n",
    "    print(\"\\n2. Building district lookup dictionary...\", flush=True)\n",
    "\n",
    "    # Get unique districts from IPC reference\n",
    "    # Use the same columns as Stage 1 for matching\n",
    "    unique_districts = ipc_ref.drop_duplicates(subset=['geographic_unit_full_name'])[\n",
    "        ['country', 'country_code', 'fips_code', 'district', 'region',\n",
    "         'geographic_unit_name', 'geographic_unit_full_name', 'district_normalized',\n",
    "         'full_name_normalized', 'fewsnet_region', 'geographic_group']\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"   Unique districts: {len(unique_districts):,}\", flush=True)\n",
    "\n",
    "    # Build primary lookup: (country_fips, district_normalized) -> district_info\n",
    "    ipc_lookup = defaultdict(list)\n",
    "\n",
    "    for idx, row in unique_districts.iterrows():\n",
    "        if pd.notna(row['fips_code']) and pd.notna(row['district_normalized']):\n",
    "            key = (row['fips_code'], row['district_normalized'])\n",
    "            district_info = {\n",
    "                'ipc_country': row['country'],\n",
    "                'ipc_country_code': row['country_code'],\n",
    "                'ipc_fips_code': row['fips_code'],\n",
    "                'ipc_district': row['district'],\n",
    "                'ipc_region': row['region'],\n",
    "                'ipc_geographic_unit': row['geographic_unit_name'],\n",
    "                'ipc_geographic_unit_full': row['geographic_unit_full_name'],\n",
    "                'ipc_fewsnet_region': row['fewsnet_region'],\n",
    "                'ipc_geographic_group': row['geographic_group'],\n",
    "            }\n",
    "            ipc_lookup[key].append(district_info)\n",
    "\n",
    "    print(f\"   Primary lookup: {len(ipc_lookup):,} (country, district) keys\", flush=True)\n",
    "\n",
    "    # Build word-based lookup from full_name_normalized (for livelihood zones)\n",
    "    print(\"   Building word-based lookup from full_name_normalized...\", flush=True)\n",
    "    word_lookup = defaultdict(list)\n",
    "\n",
    "    for idx, row in unique_districts.iterrows():\n",
    "        if pd.notna(row['fips_code']) and pd.notna(row.get('full_name_normalized')):\n",
    "            country = row['fips_code']\n",
    "            full_name_norm = normalize_text(row['full_name_normalized'])\n",
    "            words = full_name_norm.split()\n",
    "\n",
    "            district_info = {\n",
    "                'ipc_country': row['country'],\n",
    "                'ipc_country_code': row['country_code'],\n",
    "                'ipc_fips_code': row['fips_code'],\n",
    "                'ipc_district': row['district'],\n",
    "                'ipc_region': row['region'],\n",
    "                'ipc_geographic_unit': row['geographic_unit_name'],\n",
    "                'ipc_geographic_unit_full': row['geographic_unit_full_name'],\n",
    "                'ipc_fewsnet_region': row['fewsnet_region'],\n",
    "                'ipc_geographic_group': row['geographic_group'],\n",
    "            }\n",
    "\n",
    "            for word in words:\n",
    "                if len(word) > 2:  # Skip short words\n",
    "                    key = (country, word)\n",
    "                    word_lookup[key].append(district_info)\n",
    "\n",
    "    print(f\"   Word-based lookup: {len(word_lookup):,} keys\", flush=True)\n",
    "\n",
    "    # Combine lookups (ipc_lookup takes precedence)\n",
    "    combined_lookup = defaultdict(list)\n",
    "    for k, v in word_lookup.items():\n",
    "        combined_lookup[k].extend(v)\n",
    "    for k, v in ipc_lookup.items():\n",
    "        combined_lookup[k].extend(v)\n",
    "\n",
    "    print(f\"   Combined lookup: {len(combined_lookup):,} keys\", flush=True)\n",
    "\n",
    "    # Pre-group candidates by country for fast fuzzy matching\n",
    "    country_candidates = build_country_candidates(combined_lookup)\n",
    "    print(f\"   Pre-grouped by country: {len(country_candidates)} countries\", flush=True)\n",
    "\n",
    "    return combined_lookup, unique_districts, country_candidates\n",
    "\n",
    "\n",
    "def cleanup_temp_files(temp_parquets, lookup_file=None):\n",
    "    \"\"\"Clean up temporary files on success or failure\"\"\"\n",
    "    cleaned = 0\n",
    "    for temp_file in temp_parquets:\n",
    "        try:\n",
    "            if temp_file.exists():\n",
    "                temp_file.unlink()\n",
    "                cleaned += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Could not delete {temp_file}: {e}\")\n",
    "\n",
    "    if lookup_file and lookup_file.exists():\n",
    "        try:\n",
    "            lookup_file.unlink()\n",
    "            cleaned += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Could not delete {lookup_file}: {e}\")\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def process_batch_monthly(args):\n",
    "    \"\"\"\n",
    "    Process a single batch of locations in parallel for monthly aggregation.\n",
    "    Workers read their own batch from file to avoid memory duplication.\n",
    "    Returns: (batch_num, results_list, match_stats)\n",
    "    \"\"\"\n",
    "    batch_num, row_start, row_end, locations_path, ipc_ref_path = args\n",
    "\n",
    "    # Each worker builds its own lookup to avoid passing large dicts\n",
    "    ipc_ref = pd.read_parquet(ipc_ref_path)\n",
    "\n",
    "    # Build lookup in worker\n",
    "    unique_districts = ipc_ref.drop_duplicates(subset=['geographic_unit_full_name'])[\n",
    "        ['country', 'country_code', 'fips_code', 'district', 'region',\n",
    "         'geographic_unit_name', 'geographic_unit_full_name', 'district_normalized',\n",
    "         'full_name_normalized', 'fewsnet_region', 'geographic_group']\n",
    "    ].copy()\n",
    "\n",
    "    ipc_lookup = defaultdict(list)\n",
    "    for idx, row in unique_districts.iterrows():\n",
    "        if pd.notna(row['fips_code']) and pd.notna(row['district_normalized']):\n",
    "            key = (row['fips_code'], row['district_normalized'])\n",
    "            district_info = {\n",
    "                'ipc_country': row['country'],\n",
    "                'ipc_country_code': row['country_code'],\n",
    "                'ipc_fips_code': row['fips_code'],\n",
    "                'ipc_district': row['district'],\n",
    "                'ipc_region': row['region'],\n",
    "                'ipc_geographic_unit': row['geographic_unit_name'],\n",
    "                'ipc_geographic_unit_full': row['geographic_unit_full_name'],\n",
    "                'ipc_fewsnet_region': row['fewsnet_region'],\n",
    "                'ipc_geographic_group': row['geographic_group'],\n",
    "            }\n",
    "            ipc_lookup[key].append(district_info)\n",
    "\n",
    "    word_lookup = defaultdict(list)\n",
    "    for idx, row in unique_districts.iterrows():\n",
    "        if pd.notna(row['fips_code']) and pd.notna(row.get('full_name_normalized')):\n",
    "            country = row['fips_code']\n",
    "            full_name_norm = normalize_text(row['full_name_normalized'])\n",
    "            words = full_name_norm.split()\n",
    "            district_info = {\n",
    "                'ipc_country': row['country'],\n",
    "                'ipc_country_code': row['country_code'],\n",
    "                'ipc_fips_code': row['fips_code'],\n",
    "                'ipc_district': row['district'],\n",
    "                'ipc_region': row['region'],\n",
    "                'ipc_geographic_unit': row['geographic_unit_name'],\n",
    "                'ipc_geographic_unit_full': row['geographic_unit_full_name'],\n",
    "                'ipc_fewsnet_region': row['fewsnet_region'],\n",
    "                'ipc_geographic_group': row['geographic_group'],\n",
    "            }\n",
    "            for word in words:\n",
    "                if len(word) > 2:\n",
    "                    key = (country, word)\n",
    "                    word_lookup[key].append(district_info)\n",
    "\n",
    "    combined_lookup = defaultdict(list)\n",
    "    for k, v in word_lookup.items():\n",
    "        combined_lookup[k].extend(v)\n",
    "    for k, v in ipc_lookup.items():\n",
    "        combined_lookup[k].extend(v)\n",
    "\n",
    "    country_candidates = build_country_candidates(combined_lookup)\n",
    "\n",
    "    # Read just this batch from the parquet file\n",
    "    parquet_file = pq.ParquetFile(locations_path)\n",
    "    batch_df = parquet_file.read_row_group(batch_num).to_pandas()\n",
    "\n",
    "    results = []\n",
    "    match_stats = {\n",
    "        'GADM3_exact': 0, 'GADM3_fuzzy': 0,\n",
    "        'GADM2_exact': 0, 'GADM2_fuzzy': 0,\n",
    "        'GADM1_exact': 0, 'GADM1_fuzzy': 0,\n",
    "        'Country_level': 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1eb94",
   "metadata": {},
   "source": [
    "## SQLite Setup and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    locations_chunk = batch_df\n",
    "    locations_chunk['date_extracted'] = pd.to_datetime(locations_chunk['date_extracted'])\n",
    "    locations_chunk['year_month'] = locations_chunk['date_extracted'].dt.to_period('M')\n",
    "\n",
    "    # Normalize geographic fields\n",
    "    locations_chunk['gadm1_norm'] = locations_chunk['gadm1_name'].apply(normalize_text)\n",
    "    locations_chunk['gadm2_norm'] = locations_chunk['gadm2_name'].apply(normalize_text)\n",
    "    locations_chunk['gadm3_norm'] = locations_chunk['gadm3_name'].apply(normalize_text)\n",
    "\n",
    "    # Group by country for efficiency\n",
    "    for country_code, country_group in locations_chunk.groupby('african_country_code'):\n",
    "\n",
    "        # PRIORITY 1: GADM3 Matching\n",
    "        for gadm3_norm, gadm3_group in country_group.groupby('gadm3_norm'):\n",
    "            if not gadm3_norm:\n",
    "                continue\n",
    "\n",
    "            key = (country_code, gadm3_norm)\n",
    "            matched_district_list = None\n",
    "            match_type = None\n",
    "\n",
    "            if key in combined_lookup:\n",
    "                matched_district_list = combined_lookup[key]\n",
    "                match_type = 'GADM3_exact'\n",
    "            else:\n",
    "                matched_district_list, _ = find_fuzzy_match(gadm3_norm, country_candidates, country_code)\n",
    "                if matched_district_list:\n",
    "                    match_type = 'GADM3_fuzzy'\n",
    "\n",
    "            if matched_district_list:\n",
    "                n_rows = len(gadm3_group)\n",
    "                gkgids = gadm3_group['GKGRECORDID'].values\n",
    "                year_months = gadm3_group['year_month'].astype(str).values\n",
    "                dates = gadm3_group['date_extracted'].astype(str).values\n",
    "\n",
    "                for district_info in matched_district_list:\n",
    "                    batch_tuples = list(zip(\n",
    "                        gkgids,\n",
    "                        year_months,\n",
    "                        dates,\n",
    "                        [match_type] * n_rows,\n",
    "                        [district_info['ipc_country']] * n_rows,\n",
    "                        [district_info['ipc_country_code']] * n_rows,\n",
    "                        [district_info['ipc_fips_code']] * n_rows,\n",
    "                        [district_info['ipc_district']] * n_rows,\n",
    "                        [district_info['ipc_region']] * n_rows,\n",
    "                        [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                        [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                        [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                        [district_info['ipc_geographic_group']] * n_rows\n",
    "                    ))\n",
    "                    results.extend(batch_tuples)\n",
    "                    match_stats[match_type] += n_rows\n",
    "\n",
    "        # PRIORITY 2: GADM2 Matching\n",
    "        for gadm2_norm, gadm2_group in country_group.groupby('gadm2_norm'):\n",
    "            if not gadm2_norm:\n",
    "                continue\n",
    "\n",
    "            key = (country_code, gadm2_norm)\n",
    "            matched_district_list = None\n",
    "            match_type = None\n",
    "\n",
    "            if key in combined_lookup:\n",
    "                matched_district_list = combined_lookup[key]\n",
    "                match_type = 'GADM2_exact'\n",
    "            else:\n",
    "                matched_district_list, _ = find_fuzzy_match(gadm2_norm, country_candidates, country_code)\n",
    "                if matched_district_list:\n",
    "                    match_type = 'GADM2_fuzzy'\n",
    "\n",
    "            if matched_district_list:\n",
    "                n_rows = len(gadm2_group)\n",
    "                gkgids = gadm2_group['GKGRECORDID'].values\n",
    "                year_months = gadm2_group['year_month'].astype(str).values\n",
    "                dates = gadm2_group['date_extracted'].astype(str).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5ac9b",
   "metadata": {},
   "source": [
    "## Chunk Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e49b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "                for district_info in matched_district_list:\n",
    "                    batch_tuples = list(zip(\n",
    "                        gkgids,\n",
    "                        year_months,\n",
    "                        dates,\n",
    "                        [match_type] * n_rows,\n",
    "                        [district_info['ipc_country']] * n_rows,\n",
    "                        [district_info['ipc_country_code']] * n_rows,\n",
    "                        [district_info['ipc_fips_code']] * n_rows,\n",
    "                        [district_info['ipc_district']] * n_rows,\n",
    "                        [district_info['ipc_region']] * n_rows,\n",
    "                        [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                        [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                        [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                        [district_info['ipc_geographic_group']] * n_rows\n",
    "                    ))\n",
    "                    results.extend(batch_tuples)\n",
    "                    match_stats[match_type] += n_rows\n",
    "\n",
    "        # PRIORITY 3: GADM1 Matching\n",
    "        for gadm1_norm, gadm1_group in country_group.groupby('gadm1_norm'):\n",
    "            if not gadm1_norm:\n",
    "                continue\n",
    "\n",
    "            key = (country_code, gadm1_norm)\n",
    "            matched_district_list = None\n",
    "            match_type = None\n",
    "\n",
    "            if key in combined_lookup:\n",
    "                matched_district_list = combined_lookup[key]\n",
    "                match_type = 'GADM1_exact'\n",
    "            else:\n",
    "                matched_district_list, _ = find_fuzzy_match(gadm1_norm, country_candidates, country_code)\n",
    "                if matched_district_list:\n",
    "                    match_type = 'GADM1_fuzzy'\n",
    "\n",
    "            if matched_district_list:\n",
    "                n_rows = len(gadm1_group)\n",
    "                gkgids = gadm1_group['GKGRECORDID'].values\n",
    "                year_months = gadm1_group['year_month'].astype(str).values\n",
    "                dates = gadm1_group['date_extracted'].astype(str).values\n",
    "\n",
    "                for district_info in matched_district_list:\n",
    "                    batch_tuples = list(zip(\n",
    "                        gkgids,\n",
    "                        year_months,\n",
    "                        dates,\n",
    "                        [match_type] * n_rows,\n",
    "                        [district_info['ipc_country']] * n_rows,\n",
    "                        [district_info['ipc_country_code']] * n_rows,\n",
    "                        [district_info['ipc_fips_code']] * n_rows,\n",
    "                        [district_info['ipc_district']] * n_rows,\n",
    "                        [district_info['ipc_region']] * n_rows,\n",
    "                        [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                        [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                        [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                        [district_info['ipc_geographic_group']] * n_rows\n",
    "                    ))\n",
    "                    results.extend(batch_tuples)\n",
    "                    match_stats[match_type] += n_rows\n",
    "\n",
    "        # PRIORITY 4: Country-level matching\n",
    "        if country_code in COUNTRY_LEVEL_ONLY:\n",
    "            n_rows = len(country_group)\n",
    "            gkgids = country_group['GKGRECORDID'].values\n",
    "            year_months = country_group['year_month'].astype(str).values\n",
    "            dates = country_group['date_extracted'].astype(str).values\n",
    "\n",
    "            for (fips, district), district_list in combined_lookup.items():\n",
    "                if fips == country_code:\n",
    "                    for district_info in district_list:\n",
    "                        batch_tuples = list(zip(\n",
    "                            gkgids,\n",
    "                            year_months,\n",
    "                            dates,\n",
    "                            ['Country_level'] * n_rows,\n",
    "                            [district_info['ipc_country']] * n_rows,\n",
    "                            [district_info['ipc_country_code']] * n_rows,\n",
    "                            [district_info['ipc_fips_code']] * n_rows,\n",
    "                            [district_info['ipc_district']] * n_rows,\n",
    "                            [district_info['ipc_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                            [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_group']] * n_rows\n",
    "                        ))\n",
    "                        results.extend(batch_tuples)\n",
    "                        match_stats['Country_level'] += n_rows\n",
    "\n",
    "    return batch_num, results, match_stats\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Stage 2: Monthly Article Aggregation\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Start time: {datetime.now()}\")\n",
    "    print(f\"Fuzzy matching threshold: {FUZZY_THRESHOLD}\")\n",
    "    print(\"\\nKEY DIFFERENCE from Stage 1:\")\n",
    "    print(\"  - Stage 1: Aggregates within IPC assessment periods (Feb, Jun, Oct)\")\n",
    "    print(\"  - Stage 2: Aggregates by calendar month (all 12 months)\")\n",
    "    print(\"  - Uses SAME district matching logic as Stage 1 for alignment\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    STAGE2_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load IPC reference (same as Stage 1)\n",
    "    print(\"\\n1. Loading IPC district reference...\")\n",
    "    ipc_ref = pd.read_parquet(IPC_REF_FILE)\n",
    "    print(f\"   Loaded {len(ipc_ref):,} IPC periods\")\n",
    "    print(f\"   Unique districts: {ipc_ref['district'].nunique():,}\")\n",
    "    print(f\"   Unique full_names: {ipc_ref['geographic_unit_full_name'].nunique():,}\")\n",
    "\n",
    "    # Build district lookup (same matching logic as Stage 1)\n",
    "    combined_lookup, unique_districts, country_candidates = build_district_lookup(ipc_ref)\n",
    "\n",
    "    # Process locations to build GKGRECORDID -> district mapping\n",
    "    # KEY CHANGE: Instead of filtering by IPC period, we capture the article date\n",
    "    print(\"\\n3. Processing locations to build GKGRECORDID -> district mapping...\", flush=True)\n",
    "    print(\"   Matching priority: GADM3 -> GADM2 -> GADM1 -> Country-level\", flush=True)\n",
    "    print(f\"   Using {NUM_WORKERS} parallel workers\", flush=True)\n",
    "\n",
    "    parquet_file = pq.ParquetFile(LOCATIONS_FILE)\n",
    "\n",
    "    # Use SQLite for streaming - eliminates memory issues\n",
    "    # Use unique timestamped filename to avoid conflicts from old processes\n",
    "    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    sqlite_file = STAGE2_DATA_DIR / f'temp_gkg_district_lookup_{timestamp_str}.db'\n",
    "    # Clean up old temp files (ignore errors if locked)\n",
    "    for old_file in STAGE2_DATA_DIR.glob('temp_gkg_district_lookup_*.db'):\n",
    "        try:\n",
    "            old_file.unlink()\n",
    "        except:\n",
    "            pass  # Ignore locked files\n",
    "\n",
    "    conn = sqlite3.connect(str(sqlite_file))\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # SQLite optimizations for bulk inserts\n",
    "    cursor.execute(\"PRAGMA journal_mode=WAL\")\n",
    "    cursor.execute(\"PRAGMA synchronous=OFF\")\n",
    "    cursor.execute(\"PRAGMA cache_size=100000\")\n",
    "    cursor.execute(\"PRAGMA temp_store=MEMORY\")\n",
    "    conn.commit()\n",
    "\n",
    "    # Create table for GKG-district mappings\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE gkg_district (\n",
    "            GKGRECORDID TEXT,\n",
    "            year_month TEXT,\n",
    "            date_extracted TEXT,\n",
    "            match_level TEXT,\n",
    "            ipc_country TEXT,\n",
    "            ipc_country_code TEXT,\n",
    "            ipc_fips_code TEXT,\n",
    "            ipc_district TEXT,\n",
    "            ipc_region TEXT,\n",
    "            ipc_geographic_unit TEXT,\n",
    "            ipc_geographic_unit_full TEXT,\n",
    "            ipc_fewsnet_region TEXT,\n",
    "            ipc_geographic_group TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "    # Get number of row groups in parquet file\n",
    "    num_row_groups = parquet_file.metadata.num_row_groups\n",
    "    print(f\"   Total row groups to process: {num_row_groups}\", flush=True)\n",
    "\n",
    "    # Use SEQUENTIAL processing to avoid memory issues with multiprocessing\n",
    "    print(f\"\\n   Processing {num_row_groups} batches sequentially (SQLite streaming)...\", flush=True)\n",
    "    print(\"   (Sequential mode avoids memory duplication in subprocess pickling)\", flush=True)\n",
    "\n",
    "    # Pre-compute unique GADM names from locations file for fuzzy cache\n",
    "    print(\"   Scanning locations for unique GADM names (for fuzzy cache)...\", flush=True)\n",
    "    all_gadm_names_by_country = defaultdict(set)\n",
    "    for rg_idx in range(num_row_groups):\n",
    "        sample_batch = parquet_file.read_row_group(rg_idx, columns=['african_country_code', 'gadm1_name', 'gadm2_name', 'gadm3_name']).to_pandas()\n",
    "        for country_code in sample_batch['african_country_code'].unique():\n",
    "            country_data = sample_batch[sample_batch['african_country_code'] == country_code]\n",
    "            all_gadm_names_by_country[country_code].update(country_data['gadm1_name'].dropna().apply(normalize_text).unique())\n",
    "            all_gadm_names_by_country[country_code].update(country_data['gadm2_name'].dropna().apply(normalize_text).unique())\n",
    "            all_gadm_names_by_country[country_code].update(country_data['gadm3_name'].dropna().apply(normalize_text).unique())\n",
    "        del sample_batch\n",
    "        if (rg_idx + 1) % 20 == 0:\n",
    "            print(f\"      Scanned {rg_idx + 1}/{num_row_groups} row groups...\", flush=True)\n",
    "    print(f\"   Found {sum(len(v) for v in all_gadm_names_by_country.values()):,} unique GADM names across {len(all_gadm_names_by_country)} countries\", flush=True)\n",
    "\n",
    "    # Pre-compute fuzzy matches\n",
    "    fuzzy_cache = precompute_fuzzy_matches(country_candidates, all_gadm_names_by_country)\n",
    "    del all_gadm_names_by_country  # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "    total_match_stats = {\n",
    "        'GADM3_exact': 0, 'GADM3_fuzzy': 0,\n",
    "        'GADM2_exact': 0, 'GADM2_fuzzy': 0,\n",
    "        'GADM1_exact': 0, 'GADM1_fuzzy': 0,\n",
    "        'Country_level': 0\n",
    "    }\n",
    "\n",
    "    total_rows = 0\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Process batches sequentially - write directly to SQLite\n",
    "    for batch_num in range(num_row_groups):\n",
    "        batch_start = datetime.now()\n",
    "\n",
    "        # Read this batch from parquet\n",
    "        batch_df = parquet_file.read_row_group(batch_num).to_pandas()\n",
    "        batch_df['date_extracted'] = pd.to_datetime(batch_df['date_extracted'])\n",
    "        batch_df['year_month'] = batch_df['date_extracted'].dt.to_period('M').astype(str)\n",
    "\n",
    "        # Normalize geographic fields\n",
    "        batch_df['gadm1_norm'] = batch_df['gadm1_name'].apply(normalize_text)\n",
    "        batch_df['gadm2_norm'] = batch_df['gadm2_name'].apply(normalize_text)\n",
    "        batch_df['gadm3_norm'] = batch_df['gadm3_name'].apply(normalize_text)\n",
    "\n",
    "        batch_matches = 0\n",
    "        batch_stats = {'GADM3_exact': 0, 'GADM3_fuzzy': 0, 'GADM2_exact': 0, 'GADM2_fuzzy': 0,\n",
    "                       'GADM1_exact': 0, 'GADM1_fuzzy': 0, 'Country_level': 0}\n",
    "\n",
    "        # Buffer for batched SQLite inserts\n",
    "        insert_buffer = []\n",
    "\n",
    "        # Track matched GKGRECORDIDs to avoid redundant lower-priority matching\n",
    "        matched_gkgids = set()\n",
    "\n",
    "        # Process by country for efficiency\n",
    "        for country_code, country_group in batch_df.groupby('african_country_code'):\n",
    "\n",
    "            # PRIORITY 1: GADM3 Matching\n",
    "            for gadm3_norm, gadm3_group in country_group.groupby('gadm3_norm'):\n",
    "                if not gadm3_norm:\n",
    "                    continue\n",
    "\n",
    "                key = (country_code, gadm3_norm)\n",
    "                matched_district_list = None\n",
    "                match_type = None\n",
    "\n",
    "                if key in combined_lookup:\n",
    "                    matched_district_list = combined_lookup[key]\n",
    "                    match_type = 'GADM3_exact'\n",
    "                else:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958694ae",
   "metadata": {},
   "source": [
    "## Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57038c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # Use cached fuzzy match\n",
    "                    matched_district_list, _ = get_fuzzy_match_cached(gadm3_norm, country_code, fuzzy_cache, country_candidates)\n",
    "                    if matched_district_list:\n",
    "                        match_type = 'GADM3_fuzzy'\n",
    "\n",
    "                if matched_district_list:\n",
    "                    for district_info in matched_district_list:\n",
    "                        # VECTORIZED: Use .values arrays instead of iterrows()\n",
    "                        gkgids = gadm3_group['GKGRECORDID'].values\n",
    "                        year_months = gadm3_group['year_month'].values\n",
    "                        dates = gadm3_group['date_extracted'].astype(str).values\n",
    "                        n_rows = len(gkgids)\n",
    "                        tuples = list(zip(\n",
    "                            gkgids,\n",
    "                            year_months,\n",
    "                            dates,\n",
    "                            [match_type] * n_rows,\n",
    "                            [district_info['ipc_country']] * n_rows,\n",
    "                            [district_info['ipc_country_code']] * n_rows,\n",
    "                            [district_info['ipc_fips_code']] * n_rows,\n",
    "                            [district_info['ipc_district']] * n_rows,\n",
    "                            [district_info['ipc_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                            [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_group']] * n_rows\n",
    "                        ))\n",
    "                        insert_buffer.extend(tuples)\n",
    "                        batch_matches += n_rows\n",
    "                        batch_stats[match_type] += n_rows\n",
    "                        matched_gkgids.update(gkgids)\n",
    "\n",
    "                        # Flush buffer when it gets large\n",
    "                        if len(insert_buffer) >= SQLITE_BATCH_SIZE:\n",
    "                            cursor.executemany('INSERT INTO gkg_district VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)', insert_buffer)\n",
    "                            insert_buffer.clear()\n",
    "\n",
    "            # PRIORITY 2: GADM2 Matching\n",
    "            for gadm2_norm, gadm2_group in country_group.groupby('gadm2_norm'):\n",
    "                if not gadm2_norm:\n",
    "                    continue\n",
    "\n",
    "                # Skip already matched GKGRECORDIDs\n",
    "                gadm2_group = gadm2_group[~gadm2_group['GKGRECORDID'].isin(matched_gkgids)]\n",
    "                if len(gadm2_group) == 0:\n",
    "                    continue\n",
    "\n",
    "                key = (country_code, gadm2_norm)\n",
    "                matched_district_list = None\n",
    "                match_type = None\n",
    "\n",
    "                if key in combined_lookup:\n",
    "                    matched_district_list = combined_lookup[key]\n",
    "                    match_type = 'GADM2_exact'\n",
    "                else:\n",
    "                    # Use cached fuzzy match\n",
    "                    matched_district_list, _ = get_fuzzy_match_cached(gadm2_norm, country_code, fuzzy_cache, country_candidates)\n",
    "                    if matched_district_list:\n",
    "                        match_type = 'GADM2_fuzzy'\n",
    "\n",
    "                if matched_district_list:\n",
    "                    for district_info in matched_district_list:\n",
    "                        # VECTORIZED: Use .values arrays instead of iterrows()\n",
    "                        gkgids = gadm2_group['GKGRECORDID'].values\n",
    "                        year_months = gadm2_group['year_month'].values\n",
    "                        dates = gadm2_group['date_extracted'].astype(str).values\n",
    "                        n_rows = len(gkgids)\n",
    "                        tuples = list(zip(\n",
    "                            gkgids,\n",
    "                            year_months,\n",
    "                            dates,\n",
    "                            [match_type] * n_rows,\n",
    "                            [district_info['ipc_country']] * n_rows,\n",
    "                            [district_info['ipc_country_code']] * n_rows,\n",
    "                            [district_info['ipc_fips_code']] * n_rows,\n",
    "                            [district_info['ipc_district']] * n_rows,\n",
    "                            [district_info['ipc_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                            [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_group']] * n_rows\n",
    "                        ))\n",
    "                        insert_buffer.extend(tuples)\n",
    "                        batch_matches += n_rows\n",
    "                        batch_stats[match_type] += n_rows\n",
    "                        matched_gkgids.update(gkgids)\n",
    "\n",
    "                        # Flush buffer when it gets large\n",
    "                        if len(insert_buffer) >= SQLITE_BATCH_SIZE:\n",
    "                            cursor.executemany('INSERT INTO gkg_district VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)', insert_buffer)\n",
    "                            insert_buffer.clear()\n",
    "\n",
    "            # PRIORITY 3: GADM1 Matching\n",
    "            for gadm1_norm, gadm1_group in country_group.groupby('gadm1_norm'):\n",
    "                if not gadm1_norm:\n",
    "                    continue\n",
    "\n",
    "                # Skip already matched GKGRECORDIDs\n",
    "                gadm1_group = gadm1_group[~gadm1_group['GKGRECORDID'].isin(matched_gkgids)]\n",
    "                if len(gadm1_group) == 0:\n",
    "                    continue\n",
    "\n",
    "                key = (country_code, gadm1_norm)\n",
    "                matched_district_list = None\n",
    "                match_type = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e22fa00",
   "metadata": {},
   "source": [
    "## Aggregation by District-Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "                if key in combined_lookup:\n",
    "                    matched_district_list = combined_lookup[key]\n",
    "                    match_type = 'GADM1_exact'\n",
    "                else:\n",
    "                    # Use cached fuzzy match\n",
    "                    matched_district_list, _ = get_fuzzy_match_cached(gadm1_norm, country_code, fuzzy_cache, country_candidates)\n",
    "                    if matched_district_list:\n",
    "                        match_type = 'GADM1_fuzzy'\n",
    "\n",
    "                if matched_district_list:\n",
    "                    for district_info in matched_district_list:\n",
    "                        # VECTORIZED: Use .values arrays instead of iterrows()\n",
    "                        gkgids = gadm1_group['GKGRECORDID'].values\n",
    "                        year_months = gadm1_group['year_month'].values\n",
    "                        dates = gadm1_group['date_extracted'].astype(str).values\n",
    "                        n_rows = len(gkgids)\n",
    "                        tuples = list(zip(\n",
    "                            gkgids,\n",
    "                            year_months,\n",
    "                            dates,\n",
    "                            [match_type] * n_rows,\n",
    "                            [district_info['ipc_country']] * n_rows,\n",
    "                            [district_info['ipc_country_code']] * n_rows,\n",
    "                            [district_info['ipc_fips_code']] * n_rows,\n",
    "                            [district_info['ipc_district']] * n_rows,\n",
    "                            [district_info['ipc_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                            [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                            [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                            [district_info['ipc_geographic_group']] * n_rows\n",
    "                        ))\n",
    "                        insert_buffer.extend(tuples)\n",
    "                        batch_matches += n_rows\n",
    "                        batch_stats[match_type] += n_rows\n",
    "                        matched_gkgids.update(gkgids)\n",
    "\n",
    "                        # Flush buffer when it gets large\n",
    "                        if len(insert_buffer) >= SQLITE_BATCH_SIZE:\n",
    "                            cursor.executemany('INSERT INTO gkg_district VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)', insert_buffer)\n",
    "                            insert_buffer.clear()\n",
    "\n",
    "            # PRIORITY 4: Country-level matching\n",
    "            if country_code in COUNTRY_LEVEL_ONLY:\n",
    "                # Skip already matched GKGRECORDIDs\n",
    "                unmatched_group = country_group[~country_group['GKGRECORDID'].isin(matched_gkgids)]\n",
    "                if len(unmatched_group) == 0:\n",
    "                    continue\n",
    "\n",
    "                for (fips, district), district_list in combined_lookup.items():\n",
    "                    if fips == country_code:\n",
    "                        for district_info in district_list:\n",
    "                            # VECTORIZED: Use .values arrays instead of iterrows()\n",
    "                            gkgids = unmatched_group['GKGRECORDID'].values\n",
    "                            year_months = unmatched_group['year_month'].values\n",
    "                            dates = unmatched_group['date_extracted'].astype(str).values\n",
    "                            n_rows = len(gkgids)\n",
    "                            tuples = list(zip(\n",
    "                                gkgids,\n",
    "                                year_months,\n",
    "                                dates,\n",
    "                                ['Country_level'] * n_rows,\n",
    "                                [district_info['ipc_country']] * n_rows,\n",
    "                                [district_info['ipc_country_code']] * n_rows,\n",
    "                                [district_info['ipc_fips_code']] * n_rows,\n",
    "                                [district_info['ipc_district']] * n_rows,\n",
    "                                [district_info['ipc_region']] * n_rows,\n",
    "                                [district_info['ipc_geographic_unit']] * n_rows,\n",
    "                                [district_info['ipc_geographic_unit_full']] * n_rows,\n",
    "                                [district_info['ipc_fewsnet_region']] * n_rows,\n",
    "                                [district_info['ipc_geographic_group']] * n_rows\n",
    "                            ))\n",
    "                            insert_buffer.extend(tuples)\n",
    "                            batch_matches += n_rows\n",
    "                            batch_stats['Country_level'] += n_rows\n",
    "\n",
    "                            # Flush buffer when it gets large\n",
    "                            if len(insert_buffer) >= SQLITE_BATCH_SIZE:\n",
    "                                cursor.executemany('INSERT INTO gkg_district VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)', insert_buffer)\n",
    "                                insert_buffer.clear()\n",
    "\n",
    "        # Flush any remaining buffer and commit after each batch\n",
    "        if insert_buffer:\n",
    "            cursor.executemany('INSERT INTO gkg_district VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)', insert_buffer)\n",
    "            insert_buffer.clear()\n",
    "        conn.commit()\n",
    "        total_rows += batch_matches\n",
    "\n",
    "        # Update aggregate stats\n",
    "        for key, count in batch_stats.items():\n",
    "            total_match_stats[key] += count\n",
    "\n",
    "        # Progress reporting\n",
    "        batch_time = (datetime.now() - batch_start).total_seconds()\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = (batch_num + 1) / elapsed if elapsed > 0 else 0\n",
    "        eta = (num_row_groups - batch_num - 1) / rate if rate > 0 else 0\n",
    "        print(f\"      Batch {batch_num + 1}/{num_row_groups}: {batch_matches:,} matches ({batch_time:.1f}s) | Total: {total_rows:,} | ETA: {eta/60:.1f}min\", flush=True)\n",
    "\n",
    "        # Free memory\n",
    "        del batch_df, matched_gkgids\n",
    "        gc.collect()\n",
    "\n",
    "    # Free fuzzy cache before deduplication\n",
    "    del fuzzy_cache\n",
    "    gc.collect()\n",
    "\n",
    "    # SKIP index on large table - causes OOM on 181M+ rows\n",
    "    # Index will be created on deduplicated table instead (much smaller)\n",
    "    print(\"   Skipping index on main table (will index after dedup)...\", flush=True)\n",
    "\n",
    "    match_stats = total_match_stats\n",
    "    gc.collect()\n",
    "\n",
    "    # Print match statistics\n",
    "    print(f\"\\n   Match statistics:\")\n",
    "    total_matches = sum(match_stats.values())\n",
    "    for method, count in sorted(match_stats.items(), key=lambda x: x[1], reverse=True):\n",
    "        pct = (count / total_matches * 100) if total_matches > 0 else 0\n",
    "        print(f\"     {method}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # Check if we have data\n",
    "    cursor.execute('SELECT COUNT(*) FROM gkg_district')\n",
    "    total_rows = cursor.fetchone()[0]\n",
    "    if total_rows == 0:\n",
    "        print(\"\\n   WARNING: No matched locations found!\")\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    # Deduplicate in SQLite using BATCHED approach (memory-safe for 8GB systems)\n",
    "    print(f\"\\n4. Deduplicating {total_rows:,} records in SQLite (batched for memory safety)...\")\n",
    "\n",
    "    # Checkpoint WAL to reduce memory footprint before heavy operation\n",
    "    print(\"   Checkpointing WAL to free memory...\", flush=True)\n",
    "    cursor.execute('PRAGMA wal_checkpoint(TRUNCATE)')\n",
    "    gc.collect()\n",
    "\n",
    "    # Get max rowid for batching\n",
    "    cursor.execute('SELECT MAX(rowid) FROM gkg_district')\n",
    "    max_rowid = cursor.fetchone()[0] or 0\n",
    "\n",
    "    # Create intermediate dedup table for batched processing\n",
    "    print(f\"   Creating batched dedup table (processing {max_rowid:,} rows in {DEDUP_BATCH_SIZE:,}-row batches)...\", flush=True)\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS gkg_district_dedup_temp (\n",
    "            GKGRECORDID TEXT,\n",
    "            year_month TEXT,\n",
    "            date_extracted TEXT,\n",
    "            match_level TEXT,\n",
    "            ipc_country TEXT,\n",
    "            ipc_country_code TEXT,\n",
    "            ipc_fips_code TEXT,\n",
    "            ipc_district TEXT,\n",
    "            ipc_region TEXT,\n",
    "            ipc_geographic_unit TEXT,\n",
    "            ipc_geographic_unit_full TEXT,\n",
    "            ipc_fewsnet_region TEXT,\n",
    "            ipc_geographic_group TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "    # Process deduplication in batches\n",
    "    num_batches = (max_rowid + DEDUP_BATCH_SIZE - 1) // DEDUP_BATCH_SIZE\n",
    "    batch_start_time = datetime.now()\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * DEDUP_BATCH_SIZE\n",
    "        batch_end = min((batch_idx + 1) * DEDUP_BATCH_SIZE, max_rowid)\n",
    "\n",
    "        cursor.execute('''\n",
    "            INSERT INTO gkg_district_dedup_temp\n",
    "            SELECT * FROM gkg_district\n",
    "            WHERE rowid > ? AND rowid <= ?\n",
    "            GROUP BY GKGRECORDID, ipc_geographic_unit_full, year_month\n",
    "        ''', (batch_start, batch_end))\n",
    "        conn.commit()\n",
    "        gc.collect()\n",
    "\n",
    "        elapsed = (datetime.now() - batch_start_time).total_seconds()\n",
    "        rate = (batch_idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        eta = (num_batches - batch_idx - 1) / rate if rate > 0 else 0\n",
    "        print(f\"      Dedup batch {batch_idx + 1}/{num_batches} (rowid {batch_start:,}-{batch_end:,}) | ETA: {eta/60:.1f}min\", flush=True)\n",
    "\n",
    "    # Final cross-batch deduplication (now on much smaller intermediate table)\n",
    "    cursor.execute('SELECT COUNT(*) FROM gkg_district_dedup_temp')\n",
    "    temp_rows = cursor.fetchone()[0]\n",
    "    print(f\"   Intermediate table: {temp_rows:,} rows (now doing final cross-batch dedup)...\", flush=True)\n",
    "\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE gkg_district_dedup AS\n",
    "        SELECT * FROM gkg_district_dedup_temp\n",
    "        GROUP BY GKGRECORDID, ipc_geographic_unit_full, year_month\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "    # Drop intermediate table to free space\n",
    "    cursor.execute('DROP TABLE gkg_district_dedup_temp')\n",
    "    conn.commit()\n",
    "    gc.collect()\n",
    "\n",
    "    # Now create index on the SMALLER deduplicated table\n",
    "    print(\"   Creating index on deduplicated table...\", flush=True)\n",
    "    cursor.execute('CREATE INDEX idx_dedup_gkgrecordid ON gkg_district_dedup(GKGRECORDID)')\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.execute('SELECT COUNT(*) FROM gkg_district_dedup')\n",
    "    dedup_rows = cursor.fetchone()[0]\n",
    "    print(f\"   Deduplicated: {total_rows:,} -> {dedup_rows:,} unique matches\")\n",
    "\n",
    "    # Load articles and aggregate BY MONTH\n",
    "    print(\"\\n5. Loading and aggregating articles BY MONTH...\")\n",
    "    articles_sample = pd.read_csv(ARTICLES_FILE, nrows=1)\n",
    "    all_article_cols = articles_sample.columns.tolist()\n",
    "\n",
    "    # Define numeric columns for aggregation (same as Stage 1)\n",
    "    numeric_cols = [col for col in all_article_cols if col not in [\n",
    "        'GKGRECORDID', 'DATE', 'SourceCollectionIdentifier', 'SourceCommonName',\n",
    "        'DocumentIdentifier', 'Counts', 'V2Counts', 'Themes', 'V2Themes',\n",
    "        'Dates', 'Persons', 'V2Persons', 'Organizations', 'V2Organizations',\n",
    "        'V2AllNames', 'Locations', 'V2Locations', 'ADM1', 'ADM2', 'ADM3',\n",
    "        'V2Quotations', 'V2Amounts', 'V2RelatedImages', 'V2DateTimeFields',\n",
    "        'V2Extras', 'V2ExtendedField', 'V2TranslationInfo', 'OutletType',\n",
    "        'all_countries_mentioned', 'all_african_countries', 'date_extracted',\n",
    "        'V2Tone'\n",
    "    ]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1f4cf",
   "metadata": {},
   "source": [
    "## Main Processing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    aggregated_data = []\n",
    "\n",
    "    # Define group columns once for reuse\n",
    "    group_cols = [\n",
    "        'ipc_country', 'ipc_country_code', 'ipc_fips_code',\n",
    "        'ipc_district', 'ipc_region',\n",
    "        'ipc_geographic_unit', 'ipc_geographic_unit_full',\n",
    "        'ipc_fewsnet_region', 'ipc_geographic_group',\n",
    "        'year_month', 'match_level'\n",
    "    ]\n",
    "\n",
    "    for chunk_num, articles_chunk in enumerate(pd.read_csv(ARTICLES_FILE, chunksize=CHUNK_SIZE)):\n",
    "        print(f\"   Articles chunk {chunk_num + 1}: {len(articles_chunk):,} articles...\", flush=True)\n",
    "\n",
    "        article_ids = list(articles_chunk['GKGRECORDID'].unique())\n",
    "\n",
    "        # Query SQLite for matching GKGRECORDIDs in BATCHES to avoid too many placeholders\n",
    "        gkg_chunks = []\n",
    "        for i in range(0, len(article_ids), SQL_QUERY_BATCH_SIZE):\n",
    "            batch_ids = article_ids[i:i + SQL_QUERY_BATCH_SIZE]\n",
    "            placeholders = ','.join(['?'] * len(batch_ids))\n",
    "            query = f'SELECT * FROM gkg_district_dedup WHERE GKGRECORDID IN ({placeholders})'\n",
    "            gkg_chunk = pd.read_sql_query(query, conn, params=batch_ids)\n",
    "            if len(gkg_chunk) > 0:\n",
    "                gkg_chunks.append(gkg_chunk)\n",
    "\n",
    "        if not gkg_chunks:\n",
    "            del articles_chunk, article_ids\n",
    "            continue\n",
    "\n",
    "        gkg_lookup_chunk = pd.concat(gkg_chunks, ignore_index=True)\n",
    "        del gkg_chunks\n",
    "\n",
    "        merged = articles_chunk.merge(gkg_lookup_chunk, on='GKGRECORDID', how='inner')\n",
    "        print(f\"      Matched: {len(merged):,} article-month pairs\")\n",
    "\n",
    "        del articles_chunk, gkg_lookup_chunk, article_ids\n",
    "\n",
    "        if len(merged) == 0:\n",
    "            continue\n",
    "\n",
    "        # Aggregate\n",
    "        agg_dict = {col: 'sum' for col in numeric_cols if col in merged.columns}\n",
    "        agg_dict['GKGRECORDID'] = 'count'\n",
    "        if 'SourceCommonName' in merged.columns:\n",
    "            agg_dict['SourceCommonName'] = 'nunique'\n",
    "\n",
    "        agg_result = merged.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "        agg_result = agg_result.rename(columns={\n",
    "            'GKGRECORDID': 'article_count',\n",
    "            'SourceCommonName': 'unique_sources'\n",
    "        })\n",
    "\n",
    "        aggregated_data.append(agg_result)\n",
    "\n",
    "        del merged, agg_result\n",
    "        gc.collect()\n",
    "\n",
    "        # PERIODIC CONSOLIDATION: Prevent memory accumulation (memory-safe for 8GB)\n",
    "        if len(aggregated_data) >= CONSOLIDATE_EVERY:\n",
    "            print(f\"      Consolidating {len(aggregated_data)} partial aggregations...\", flush=True)\n",
    "            combined = pd.concat(aggregated_data, ignore_index=True)\n",
    "\n",
    "            # Re-aggregate to reduce memory footprint\n",
    "            numeric_to_sum = [c for c in combined.columns if c not in group_cols]\n",
    "            agg_dict_consol = {col: 'sum' for col in numeric_to_sum if col in combined.columns}\n",
    "            if 'match_level' in combined.columns and 'match_level' not in group_cols:\n",
    "                agg_dict_consol['match_level'] = 'first'\n",
    "\n",
    "            consolidated = combined.groupby(group_cols).agg(agg_dict_consol).reset_index()\n",
    "            aggregated_data = [consolidated]\n",
    "            del combined, consolidated\n",
    "            gc.collect()\n",
    "            print(f\"      Consolidated to {len(aggregated_data[0]):,} aggregated rows\", flush=True)\n",
    "\n",
    "    # Combine and final aggregation\n",
    "    print(\"\\n6. Combining and final aggregation...\")\n",
    "    if not aggregated_data:\n",
    "        print(\"   WARNING: No data to aggregate!\")\n",
    "        return\n",
    "\n",
    "    final_df = pd.concat(aggregated_data, ignore_index=True)\n",
    "    del aggregated_data\n",
    "    gc.collect()\n",
    "\n",
    "    # Final aggregation by (district, month) - note: excludes match_level to merge across match types\n",
    "    final_group_cols = [\n",
    "        'ipc_country', 'ipc_country_code', 'ipc_fips_code',\n",
    "        'ipc_district', 'ipc_region',\n",
    "        'ipc_geographic_unit', 'ipc_geographic_unit_full',\n",
    "        'ipc_fewsnet_region', 'ipc_geographic_group',\n",
    "        'year_month'\n",
    "    ]\n",
    "\n",
    "    # Sum numeric columns, keep first match_level\n",
    "    numeric_to_sum = [c for c in final_df.columns if c not in final_group_cols and c != 'match_level']\n",
    "    agg_dict = {col: 'sum' for col in numeric_to_sum if col in final_df.columns}\n",
    "    agg_dict['match_level'] = 'first'\n",
    "\n",
    "    final_agg = final_df.groupby(final_group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Monthly Aggregation Summary - Stage 2\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal monthly aggregations: {len(final_agg):,}\")\n",
    "    print(f\"Unique districts (ipc_geographic_unit_full): {final_agg['ipc_geographic_unit_full'].nunique():,}\")\n",
    "    print(f\"Unique months: {final_agg['year_month'].nunique()}\")\n",
    "    print(f\"Month range: {final_agg['year_month'].min()} to {final_agg['year_month'].max()}\")\n",
    "    print(f\"Countries: {final_agg['ipc_country'].nunique()}\")\n",
    "\n",
    "    print(f\"\\nMatch level distribution:\")\n",
    "    print(final_agg['match_level'].value_counts())\n",
    "\n",
    "    print(f\"\\nRecords by country:\")\n",
    "    print(final_agg['ipc_country'].value_counts().head(10))\n",
    "\n",
    "    # Verify alignment with Stage 1 districts\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"District Alignment Check\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    stage1_articles = pd.read_parquet(DISTRICT_DATA_DIR / 'articles_aggregated.parquet')\n",
    "    stage1_districts = set(stage1_articles['ipc_geographic_unit_full'].unique())\n",
    "    stage2_districts = set(final_agg['ipc_geographic_unit_full'].unique())\n",
    "\n",
    "    overlap = stage1_districts & stage2_districts\n",
    "    only_stage1 = stage1_districts - stage2_districts\n",
    "    only_stage2 = stage2_districts - stage1_districts\n",
    "\n",
    "    print(f\"\\nStage 1 districts: {len(stage1_districts):,}\")\n",
    "    print(f\"Stage 2 districts: {len(stage2_districts):,}\")\n",
    "    print(f\"Overlap: {len(overlap):,} ({100*len(overlap)/len(stage1_districts):.1f}% of Stage 1)\")\n",
    "    print(f\"Only in Stage 1: {len(only_stage1):,}\")\n",
    "    print(f\"Only in Stage 2: {len(only_stage2):,}\")\n",
    "\n",
    "    if len(only_stage1) > 0:\n",
    "        print(f\"\\nSample districts only in Stage 1:\")\n",
    "        for d in list(only_stage1)[:5]:\n",
    "            print(f\"   {d}\")\n",
    "\n",
    "    # Save\n",
    "    print(f\"\\n7. Saving to {OUTPUT_PARQUET}...\")\n",
    "    final_agg.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "    print(\"   [OK] Parquet saved\")\n",
    "\n",
    "    print(f\"\\n8. Saving to {OUTPUT_CSV}...\")\n",
    "    final_agg.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(\"   [OK] CSV saved\")\n",
    "\n",
    "    # Cleanup SQLite database\n",
    "    print(\"\\n   Cleaning up temporary SQLite database...\")\n",
    "    conn.close()\n",
    "    if sqlite_file.exists():\n",
    "        sqlite_file.unlink()\n",
    "        print(\"   Cleaned up SQLite database\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Stage 2 Monthly Aggregation Complete\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"End time: {datetime.now()}\")\n",
    "    print(f\"\\nOutput: {OUTPUT_PARQUET}\")\n",
    "    print(f\"Next step: Run 03a_stage2_aggregate_locations_monthly.py\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nERROR: {e}\")\n",
    "        print(\"Attempting to clean up temporary SQLite database...\")\n",
    "        # Try to clean up SQLite file\n",
    "        sqlite_file = STAGE2_DATA_DIR / 'temp_gkg_district_lookup.db'\n",
    "        if sqlite_file.exists():\n",
    "            try:\n",
    "                sqlite_file.unlink()\n",
    "                print(\"Cleaned up SQLite database after error\")\n",
    "            except Exception:\n",
    "                print(\"Could not clean up SQLite database\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
