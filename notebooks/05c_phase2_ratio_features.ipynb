{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1e0106",
   "metadata": {},
   "source": [
    "# Phase 2: Ratio Features\n",
    "\n",
    "**Script**: `scripts\\04_stage2_feature_engineering\\phase2_feature_creation\\01_ratio_features.py`\n",
    "\n",
    "**Author**: Victor Collins Oppon, MSc Data Science, Middlesex University 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Creates ratio-based temporal features using moving averages.\n",
    "\n",
    "**KEY FEATURES**:\n",
    "- article_count_ratio: Current / 3-month MA\n",
    "- unique_sources_ratio: Current / 3-month MA\n",
    "- All GDELT theme/tone ratios\n",
    "\n",
    "Captures deviation from district baseline.\n",
    "\n",
    "**Runtime**: See script header for details\n",
    "\n",
    "**Input/Output**: See script header for file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f919eea",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07882e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ratio Feature Engineering for XGBoost Pipeline (Stratified Spatial CV Variant)\n",
    "===============================================================================\n",
    "Phase 2, Step 1: Create ratio-based features for pooled models.\n",
    "\n",
    "VARIANT: STRATIFIED SPATIAL CV\n",
    "==============================\n",
    "This variant uses:\n",
    "1. STRATIFIED SPATIAL CV - Balanced folds by crisis rate AND geography\n",
    "2. MEANINGFUL LOCATION FEATURES - Replace arbitrary label encoding with\n",
    "   features that describe WHY a location has certain risk characteristics\n",
    "\n",
    "KEY DIFFERENCES FROM KMEANS-ONLY PIPELINE:\n",
    "- Uses stratified spatial CV (balanced crisis rates across folds)\n",
    "- Adds country_historical_crisis_rate, district_historical_crisis_rate\n",
    "- Adds country_baseline_conflict, country_baseline_food_security\n",
    "- Adds district_crisis_volatility, country_crisis_trend\n",
    "- Removes arbitrary country_encoded, district_encoded\n",
    "\n",
    "FEATURES CREATED:\n",
    "1. Category ratios (proportion of total articles)\n",
    "2. Category concentrations (HHI, entropy)\n",
    "3. Temporal trends (3-month and 6-month slopes)\n",
    "4. Momentum features (delta)\n",
    "5. MEANINGFUL LOCATION FEATURES (NEW)\n",
    "\n",
    "Author: Victor Collins Oppon\n",
    "Date: December 2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for config import\n",
    "sys.path.append(str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "# Import from config\n",
    "from config import (\n",
    "    BASE_DIR,\n",
    "    STAGE1_DATA_DIR,\n",
    "    STAGE1_RESULTS_DIR,\n",
    "    STAGE2_DATA_DIR,\n",
    "    STAGE2_FEATURES_DIR,\n",
    "    STAGE2_MODELS_DIR,\n",
    "    FIGURES_DIR,\n",
    "    RANDOM_STATE\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b5154",
   "metadata": {},
   "source": [
    "## Load Monthly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use self-contained paths from config (no hardcoded paths)\n",
    "PHASE1_RESULTS = STAGE2_FEATURES_DIR / 'phase1_district_threshold'\n",
    "PHASE2_RESULTS = STAGE2_FEATURES_DIR / 'phase2_features'\n",
    "\n",
    "# Macro categories\n",
    "MACRO_CATEGORIES = [\n",
    "    'conflict_category', 'displacement_category', 'economic_category',\n",
    "    'food_security_category', 'governance_category', 'health_category',\n",
    "    'humanitarian_category', 'other_category', 'weather_category'\n",
    "]\n",
    "\n",
    "# Output files\n",
    "OUTPUT_FILES = {\n",
    "    'valid_districts': 'valid_districts.csv',\n",
    "    'ratio_features': 'ratio_features_h8.csv',\n",
    "}\n",
    "\n",
    "def ensure_directories():\n",
    "    PHASE2_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "# Spatial CV configuration (self-contained)\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def create_spatial_folds(df, district_col, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create spatial folds by district using KMeans clustering on coordinates.\n",
    "\n",
    "    NOTE: This uses standard KMeans CV for compatibility with Mixed Effects models.\n",
    "    XGBoost scripts will override with stratified spatial CV at training time.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset containing district identifiers and coordinates\n",
    "    district_col : str\n",
    "        Column name for district identifier\n",
    "    n_folds : int\n",
    "        Number of spatial folds (default: 5)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility (default: 42)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Fold assignments for each observation\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "\n",
    "    # Get unique districts with their coordinates\n",
    "    coord_cols = ['avg_latitude', 'avg_longitude']\n",
    "    districts = df[[district_col] + coord_cols].drop_duplicates()\n",
    "    districts_with_coords = districts.dropna(subset=coord_cols)\n",
    "\n",
    "    print(f\"   Creating {n_folds} spatial folds using KMeans clustering...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4651ce",
   "metadata": {},
   "source": [
    "## Filter by Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618000c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(f\"   Total districts: {len(districts)}\")\n",
    "    print(f\"   Districts with valid coordinates: {len(districts_with_coords)}\")\n",
    "\n",
    "    # KMeans clustering on coordinates\n",
    "    coords = districts_with_coords[coord_cols].values\n",
    "    kmeans = KMeans(n_clusters=n_folds, random_state=random_state, n_init=10)\n",
    "    districts_with_coords = districts_with_coords.copy()\n",
    "    districts_with_coords['fold'] = kmeans.fit_predict(coords)\n",
    "\n",
    "    # Print fold distribution\n",
    "    print(f\"   Fold distribution (KMeans spatial clustering):\")\n",
    "    for fold in range(n_folds):\n",
    "        n_districts = (districts_with_coords['fold'] == fold).sum()\n",
    "        print(f\"      Fold {fold}: {n_districts} districts\")\n",
    "\n",
    "    # Map folds back to full dataset\n",
    "    fold_map = dict(zip(districts_with_coords[district_col], districts_with_coords['fold']))\n",
    "    return df[district_col].map(fold_map)\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS\n",
    "# =============================================================================\n",
    "\n",
    "MONTHLY_DATA = STAGE2_DATA_DIR / 'ml_dataset_monthly.parquet'\n",
    "IPC_REFERENCE = STAGE1_DATA_DIR / 'ipc_reference.parquet'\n",
    "# AR predictions from Stage 1\n",
    "AR_PREDICTIONS = STAGE1_RESULTS_DIR / 'predictions_h8_averaged.csv'\n",
    "\n",
    "# =============================================================================\n",
    "# METADATA COLUMNS TO PRESERVE\n",
    "# =============================================================================\n",
    "\n",
    "METADATA_COLUMNS = [\n",
    "    # Geographic\n",
    "    'ipc_country', 'ipc_country_code', 'ipc_district', 'ipc_region',\n",
    "    'ipc_geographic_unit_full', 'ipc_fips_code',\n",
    "    # Temporal\n",
    "    'year_month', 'year', 'month',\n",
    "    # Coordinates\n",
    "    'avg_latitude', 'avg_longitude', 'latitude_std', 'longitude_std',\n",
    "    # IPC\n",
    "    'ipc_value', 'ipc_value_filled', 'ipc_binary_crisis', 'ipc_future_crisis',\n",
    "    'ipc_period_start', 'ipc_period_end',\n",
    "    # Volume\n",
    "    'article_count', 'unique_sources',\n",
    "    # CV\n",
    "    'fold',\n",
    "    # AR predictions\n",
    "    'ar_pred_optimal_filled', 'ar_source_month', 'ar_target_month',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565a6ef",
   "metadata": {},
   "source": [
    "## Compute Ratio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST PIPELINE - PHASE 2: RATIO FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def load_valid_districts():\n",
    "    \"\"\"\n",
    "    Load valid districts from Phase 1.\n",
    "\n",
    "    UPDATED: Now loads ipc_geographic_unit_full (canonical identifier)\n",
    "    instead of short district names to ensure consistent matching.\n",
    "    \"\"\"\n",
    "    valid_path = PHASE1_RESULTS / OUTPUT_FILES['valid_districts']\n",
    "\n",
    "    if not valid_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Valid districts file not found: {valid_path}\\n\"\n",
    "            \"Please run Phase 1 scripts first.\"\n",
    "        )\n",
    "\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "\n",
    "    # Use canonical identifier\n",
    "    if 'ipc_geographic_unit_full' not in valid_df.columns:\n",
    "        raise ValueError(\n",
    "            \"ipc_geographic_unit_full not found in Phase 1 output! \"\n",
    "            \"Please re-run Phase 1 with updated script.\"\n",
    "        )\n",
    "\n",
    "    valid_districts = valid_df['ipc_geographic_unit_full'].tolist()\n",
    "    print(f\"   Loaded {len(valid_districts):,} valid districts from Phase 1\")\n",
    "    print(f\"   Using canonical identifier: ipc_geographic_unit_full\")\n",
    "\n",
    "    return valid_districts\n",
    "\n",
    "\n",
    "def load_and_filter_data(valid_districts):\n",
    "    \"\"\"\n",
    "    Load monthly data and filter to valid districts.\n",
    "\n",
    "    UPDATED: Now FORCES use of ipc_geographic_unit_full to match Phase 1.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Loading and filtering data...\")\n",
    "\n",
    "    # Load monthly data\n",
    "    df = pd.read_parquet(MONTHLY_DATA)\n",
    "    print(f\"   Loaded monthly data: {len(df):,} rows\")\n",
    "\n",
    "    # ALWAYS use canonical identifier\n",
    "    district_col = 'ipc_geographic_unit_full'\n",
    "    if district_col not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"ipc_geographic_unit_full not found in monthly data! \"\n",
    "            f\"Available columns: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    print(f\"   Using canonical identifier: {district_col}\")\n",
    "\n",
    "    # Filter to valid districts\n",
    "    df_filtered = df[df[district_col].isin(valid_districts)].copy()\n",
    "    print(f\"   After filtering to valid districts: {len(df_filtered):,} rows\")\n",
    "    print(f\"   Unique districts in filtered data: {df_filtered[district_col].nunique():,}\")\n",
    "\n",
    "    # Check country coverage\n",
    "    if 'ipc_country' in df_filtered.columns:\n",
    "        countries = df_filtered['ipc_country'].nunique()\n",
    "        print(f\"   Countries in filtered data: {countries}\")\n",
    "\n",
    "    return df_filtered, district_col\n",
    "\n",
    "\n",
    "def compute_ratio_features(df):\n",
    "    \"\"\"\n",
    "    Compute ratio-based features.\n",
    "\n",
    "    Ratios = category_count / total_articles\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing ratio features...\")\n",
    "\n",
    "    # Identify category columns\n",
    "    category_cols = [col for col in MACRO_CATEGORIES if col in df.columns]\n",
    "    print(f\"   Found {len(category_cols)} category columns\")\n",
    "\n",
    "    # Compute total articles (sum of all categories)\n",
    "    df['total_category_articles'] = df[category_cols].sum(axis=1)\n",
    "\n",
    "    # Compute ratios\n",
    "    for col in category_cols:\n",
    "        ratio_col = col.replace('_category', '_ratio')\n",
    "        df[ratio_col] = df[col] / df['total_category_articles'].replace(0, np.nan)\n",
    "        df[ratio_col] = df[ratio_col].fillna(0)\n",
    "\n",
    "    print(f\"   Created {len(category_cols)} ratio features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_concentration_metrics(df):\n",
    "    \"\"\"\n",
    "    Compute concentration metrics.\n",
    "\n",
    "    - HHI (Herfindahl-Hirschman Index)\n",
    "    - Entropy (diversity measure)\n",
    "    - Dominant category\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing concentration metrics...\")\n",
    "\n",
    "    ratio_cols = [col for col in df.columns if col.endswith('_ratio')]\n",
    "\n",
    "    # HHI: sum of squared ratios\n",
    "    df['hhi_category_concentration'] = df[ratio_cols].apply(\n",
    "        lambda x: (x ** 2).sum(), axis=1\n",
    "    )\n",
    "\n",
    "    # Normalized HHI (0 to 1 scale)\n",
    "    n_categories = len(ratio_cols)\n",
    "    min_hhi = 1 / n_categories  # Perfect equality\n",
    "    df['hhi_normalized'] = (df['hhi_category_concentration'] - min_hhi) / (1 - min_hhi)\n",
    "\n",
    "    # Entropy: -sum(p * log(p))\n",
    "    def compute_entropy(row):\n",
    "        p = row[row > 0]  # Only positive values\n",
    "        if len(p) == 0:\n",
    "            return 0\n",
    "        return -np.sum(p * np.log(p + 1e-10))\n",
    "\n",
    "    df['category_entropy'] = df[ratio_cols].apply(compute_entropy, axis=1)\n",
    "\n",
    "    # Dominant category\n",
    "    df['dominant_category'] = df[ratio_cols].idxmax(axis=1)\n",
    "    df['dominant_category_ratio'] = df[ratio_cols].max(axis=1)\n",
    "\n",
    "    print(\"   Created: hhi_category_concentration, hhi_normalized, category_entropy\")\n",
    "    print(\"   Created: dominant_category, dominant_category_ratio\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_temporal_trends(df, district_col):\n",
    "    \"\"\"\n",
    "    Compute temporal trends using rolling windows.\n",
    "\n",
    "    - 3-month trend (slope of linear regression)\n",
    "    - 6-month trend\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing temporal trends...\")\n",
    "\n",
    "    ratio_cols = [col for col in df.columns if col.endswith('_ratio')]\n",
    "\n",
    "    # Sort by district and time\n",
    "    df = df.sort_values([district_col, 'year_month'])\n",
    "\n",
    "    # 3-month trends\n",
    "    def compute_slope(x):\n",
    "        if len(x) < 3:\n",
    "            return np.nan\n",
    "        t = np.arange(len(x))\n",
    "        try:\n",
    "            slope = np.polyfit(t, x.values, 1)[0]\n",
    "            return slope\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    print(\"   Computing 3-month trends...\")\n",
    "    for col in ratio_cols:\n",
    "        trend_col = col.replace('_ratio', '_trend_3m')\n",
    "        df[trend_col] = df.groupby(district_col)[col].transform(\n",
    "            lambda x: x.rolling(3, min_periods=2).apply(compute_slope, raw=False)\n",
    "        )\n",
    "\n",
    "    print(\"   Computing 6-month trends...\")\n",
    "    for col in ratio_cols:\n",
    "        trend_col = col.replace('_ratio', '_trend_6m')\n",
    "        df[trend_col] = df.groupby(district_col)[col].transform(\n",
    "            lambda x: x.rolling(6, min_periods=3).apply(compute_slope, raw=False)\n",
    "        )\n",
    "\n",
    "    print(f\"   Created {len(ratio_cols) * 2} trend features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_momentum_features(df, district_col):\n",
    "    \"\"\"\n",
    "    Compute momentum (delta) features.\n",
    "\n",
    "    Delta = current - previous\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Computing momentum features...\")\n",
    "\n",
    "    ratio_cols = [col for col in df.columns if col.endswith('_ratio')]\n",
    "\n",
    "    # Sort by district and time\n",
    "    df = df.sort_values([district_col, 'year_month'])\n",
    "\n",
    "    for col in ratio_cols:\n",
    "        delta_col = col.replace('_ratio', '_delta')\n",
    "        df[delta_col] = df.groupby(district_col)[col].diff()\n",
    "\n",
    "    # Concentration delta\n",
    "    df['hhi_delta'] = df.groupby(district_col)['hhi_category_concentration'].diff()\n",
    "    df['entropy_delta'] = df.groupby(district_col)['category_entropy'].diff()\n",
    "\n",
    "    print(f\"   Created {len(ratio_cols) + 2} momentum features\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_merge_ar_predictions(df, district_col):\n",
    "    \"\"\"\n",
    "    Load AR predictions and merge with features using FORWARD-FILL expansion.\n",
    "\n",
    "    Uses FULL predictions file (all AR predictions, not just failures) to maximize coverage.\n",
    "\n",
    "    AR predictions were made for IPC assessment periods (not individual months).\n",
    "    We expand each AR prediction to ALL months within its assessment period,\n",
    "    matching the same logic used for IPC classifications.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Loading AR predictions...\")\n",
    "\n",
    "    if not AR_PREDICTIONS.exists():\n",
    "        print(f\"   Warning: AR predictions file not found: {AR_PREDICTIONS}\")\n",
    "        print(\"   AR prediction columns will not be available\")\n",
    "        return df\n",
    "\n",
    "    # Load full AR predictions (CSV format)\n",
    "    ar_df = pd.read_csv(AR_PREDICTIONS)\n",
    "    print(f\"   Loaded AR predictions: {len(ar_df):,} IPC assessment periods\")\n",
    "\n",
    "    # Strip whitespace from district identifier for alignment\n",
    "    # (Source data has leading tabs that need to be removed)\n",
    "    ar_df['ipc_geographic_unit_full'] = ar_df['ipc_geographic_unit_full'].str.strip()\n",
    "    df[district_col] = df[district_col].str.strip()\n",
    "\n",
    "    # Convert dates\n",
    "    ar_df['ipc_period_start'] = pd.to_datetime(ar_df['ipc_period_start'])\n",
    "    ar_df['ipc_period_end'] = pd.to_datetime(ar_df['ipc_period_end'])\n",
    "\n",
    "    # FORWARD-FILL EXPANSION: Each IPC period is ~1 month, expand to exact month\n",
    "    # This matches the IPC forward-fill logic for temporal consistency\n",
    "    print(\"   Expanding AR predictions to monthly observations...\")\n",
    "\n",
    "    expanded_rows = []\n",
    "    for _, row in ar_df.iterrows():\n",
    "        # IPC periods are ~1 month (28-31 days)\n",
    "        # Use period start month as the representative month\n",
    "        month = row['ipc_period_start'].replace(day=1)\n",
    "        expanded_rows.append({\n",
    "            district_col: row['ipc_geographic_unit_full'],\n",
    "            'year_month': month.strftime('%Y-%m'),\n",
    "            'ar_pred_optimal': row['y_pred_optimal'],\n",
    "            'ar_prob': row['pred_prob'],  # Column name from Stage 1 output\n",
    "            'ar_pred_binary': row['y_pred'],\n",
    "            'ar_period_start': row['ipc_period_start'],\n",
    "            'ar_period_end': row['ipc_period_end']\n",
    "        })\n",
    "\n",
    "    ar_expanded = pd.DataFrame(expanded_rows)\n",
    "    print(f\"   Expanded to {len(ar_expanded):,} district-month observations\")\n",
    "\n",
    "    # Aggregate if multiple predictions per district-month (take max)\n",
    "    ar_lookup = ar_expanded.groupby([district_col, 'year_month']).agg({\n",
    "        'ar_pred_optimal': 'max',\n",
    "        'ar_prob': 'max',\n",
    "        'ar_pred_binary': 'max',\n",
    "        'ar_period_start': 'min',\n",
    "        'ar_period_end': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "    print(f\"   Created AR lookup: {len(ar_lookup):,} unique district-month observations\")\n",
    "\n",
    "    # Rename columns to _filled suffix to indicate forward-fill methodology\n",
    "    ar_lookup = ar_lookup.rename(columns={\n",
    "        'ar_pred_optimal': 'ar_pred_optimal_filled',\n",
    "        'ar_prob': 'ar_prob_filled',\n",
    "        'ar_pred_binary': 'ar_pred_binary_filled'\n",
    "    })\n",
    "\n",
    "    # Merge with feature data\n",
    "    merge_cols = [district_col, 'year_month']\n",
    "    df = df.merge(ar_lookup, on=merge_cols, how='left', suffixes=('', '_ar'))\n",
    "\n",
    "    # Report coverage\n",
    "    ar_coverage = df['ar_pred_optimal_filled'].notna().sum()\n",
    "    ar_coverage_pct = ar_coverage / len(df) * 100\n",
    "    print(f\"   AR predictions available: {ar_coverage:,} / {len(df):,} observations ({ar_coverage_pct:.1f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_target_variable(df, district_col):\n",
    "    \"\"\"\n",
    "    Create target variable: ipc_future_crisis at t+h (h=8 months ahead).\n",
    "\n",
    "    REPRODUCIBLE SEQUENTIAL PROCESS:\n",
    "    1. Load IPC reference data\n",
    "    2. Create district-month level IPC values\n",
    "    3. Create future crisis variable by shifting IPC values forward\n",
    "    4. Merge with feature data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Creating target variable...\")\n",
    "\n",
    "    HORIZON = 8  # Prediction horizon in months\n",
    "\n",
    "    # If ipc_future_crisis already exists, use it\n",
    "    if 'ipc_future_crisis' in df.columns and df['ipc_future_crisis'].notna().sum() > 0:\n",
    "        print(\"   Using existing ipc_future_crisis column\")\n",
    "    else:\n",
    "        # Step 1: Load IPC reference data\n",
    "        print(f\"   Loading IPC reference data from: {IPC_REFERENCE}\")\n",
    "\n",
    "        if not IPC_REFERENCE.exists():\n",
    "            print(f\"   ERROR: IPC reference file not found: {IPC_REFERENCE}\")\n",
    "            print(\"   Target variable cannot be created\")\n",
    "            return df\n",
    "\n",
    "        ipc_df = pd.read_parquet(IPC_REFERENCE)\n",
    "        print(f\"   Loaded {len(ipc_df):,} IPC observations\")\n",
    "\n",
    "        # Step 2: Create district identifier matching the feature data\n",
    "        ipc_district_col = 'geographic_unit_full_name' if 'geographic_unit_full_name' in ipc_df.columns else 'district'\n",
    "\n",
    "        # Convert dates\n",
    "        ipc_df['projection_start'] = pd.to_datetime(ipc_df['projection_start'])\n",
    "        ipc_df['projection_end'] = pd.to_datetime(ipc_df['projection_end'])\n",
    "\n",
    "        # Create binary crisis indicator (IPC >= 3)\n",
    "        ipc_df['ipc_binary_crisis'] = (ipc_df['ipc_value'] >= 3).astype(int)\n",
    "\n",
    "        # Step 3: Expand IPC observations to ALL months covered by projection period\n",
    "        # This is critical for monthly data - each IPC projection covers multiple months\n",
    "        print(\"   Expanding IPC observations to all covered months...\")\n",
    "\n",
    "        expanded_rows = []\n",
    "        for _, row in ipc_df.iterrows():\n",
    "            # Generate all months from projection_start to projection_end\n",
    "            months = pd.date_range(\n",
    "                start=row['projection_start'].replace(day=1),\n",
    "                end=row['projection_end'].replace(day=1),\n",
    "                freq='MS'  # Month Start frequency\n",
    "            )\n",
    "            for month in months:\n",
    "                expanded_rows.append({\n",
    "                    ipc_district_col: row[ipc_district_col],\n",
    "                    'year_month': month.strftime('%Y-%m'),\n",
    "                    'ipc_value': row['ipc_value'],\n",
    "                    'ipc_binary_crisis': row['ipc_binary_crisis'],\n",
    "                    'ipc_period_start': row['projection_start'],\n",
    "                    'ipc_period_end': row['projection_end']\n",
    "                })\n",
    "\n",
    "        ipc_expanded = pd.DataFrame(expanded_rows)\n",
    "        print(f\"   Expanded to {len(ipc_expanded):,} district-month observations\")\n",
    "\n",
    "        # Step 4: Create IPC lookup at district-month level\n",
    "        # Take the maximum IPC value if multiple per district-month (overlapping projections)\n",
    "        ipc_lookup = ipc_expanded.groupby([ipc_district_col, 'year_month']).agg({\n",
    "            'ipc_value': 'max',\n",
    "            'ipc_binary_crisis': 'max',\n",
    "            'ipc_period_start': 'min',\n",
    "            'ipc_period_end': 'max'\n",
    "        }).reset_index()\n",
    "\n",
    "        print(f\"   Created IPC lookup: {len(ipc_lookup):,} unique district-month observations\")\n",
    "\n",
    "        # Step 5: Create complete timeline with forward-filling\n",
    "        # Generate full timeline for all district-months in feature data\n",
    "        print(\"   Creating complete timeline with forward-filling...\")\n",
    "\n",
    "        # Get unique districts and months from feature data\n",
    "        feature_districts = df[district_col].unique()\n",
    "        feature_months = sorted(df['year_month'].unique())\n",
    "\n",
    "        # Create full grid of district-months from features\n",
    "        full_grid = pd.DataFrame([\n",
    "            {district_col: d, 'year_month': m}\n",
    "            for d in feature_districts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64373bc6",
   "metadata": {},
   "source": [
    "## Create Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef64fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "            for m in feature_months\n",
    "        ])\n",
    "\n",
    "        # Rename IPC district column if needed\n",
    "        if ipc_district_col != district_col:\n",
    "            ipc_lookup = ipc_lookup.rename(columns={ipc_district_col: district_col})\n",
    "\n",
    "        # Merge IPC lookup with full grid\n",
    "        full_ipc = full_grid.merge(\n",
    "            ipc_lookup[[district_col, 'year_month', 'ipc_value', 'ipc_binary_crisis', 'ipc_period_start', 'ipc_period_end']],\n",
    "            on=[district_col, 'year_month'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Sort by district and time for forward filling\n",
    "        full_ipc = full_ipc.sort_values([district_col, 'year_month'])\n",
    "\n",
    "        # FIX ISSUE #2: TEMPORAL TARGET LEAKAGE\n",
    "        # CRITICAL: Shift BEFORE forward-fill to prevent future values bleeding into past\n",
    "        # Original (WRONG): ffill -> shift (future IPC values contaminate past)\n",
    "        # Fixed (CORRECT): shift -> ffill (only observed values used for filling)\n",
    "\n",
    "        # Step 6: Create future crisis variable by shifting FIRST\n",
    "        # Shift crisis indicator backward (future value becomes current target)\n",
    "        # This uses ONLY actually observed IPC values (no forward-filling yet)\n",
    "        full_ipc['ipc_future_crisis'] = full_ipc.groupby(district_col)['ipc_binary_crisis'].shift(-HORIZON)\n",
    "\n",
    "        # NOW forward-fill the shifted target (fills gaps with last OBSERVED future value)\n",
    "        # This is safe because we're filling the target variable, not the predictor\n",
    "        full_ipc['ipc_future_crisis'] = full_ipc.groupby(district_col)['ipc_future_crisis'].ffill()\n",
    "\n",
    "        # Fill remaining NaN targets with 0 for:\n",
    "        # 1. Last HORIZON months (no future to predict)\n",
    "        # 2. Districts without any IPC data\n",
    "        full_ipc['ipc_future_crisis'] = full_ipc['ipc_future_crisis'].fillna(0)\n",
    "\n",
    "        # FIX ISSUE #8: FORWARD-FILLED IPC FOR TARGETS\n",
    "        # Still create filled versions for current IPC status (used as features, not targets)\n",
    "        # But document clearly these are CURRENT status, not future\n",
    "        full_ipc['ipc_value_filled'] = full_ipc.groupby(district_col)['ipc_value'].ffill()\n",
    "        full_ipc['ipc_binary_crisis_filled'] = full_ipc.groupby(district_col)['ipc_binary_crisis'].ffill()\n",
    "        full_ipc['ipc_binary_crisis_filled'] = full_ipc['ipc_binary_crisis_filled'].fillna(0)\n",
    "\n",
    "        print(f\"   Created future crisis variable (h={HORIZON} months) - SHIFT THEN FILL\")\n",
    "        print(f\"   Target coverage: {full_ipc['ipc_future_crisis'].notna().sum():,} / {len(full_ipc):,}\")\n",
    "        print(f\"   Forward-filled IPC values: {full_ipc['ipc_value_filled'].notna().sum():,} / {len(full_ipc):,}\")\n",
    "\n",
    "        # Prepare IPC columns for merge\n",
    "        ipc_lookup = full_ipc[[district_col, 'year_month', 'ipc_value', 'ipc_value_filled',\n",
    "                               'ipc_binary_crisis', 'ipc_binary_crisis_filled',\n",
    "                               'ipc_future_crisis', 'ipc_period_start', 'ipc_period_end']]\n",
    "\n",
    "        # Step 7: Merge with feature data\n",
    "        merge_cols = ['year_month', 'ipc_value', 'ipc_value_filled', 'ipc_binary_crisis',\n",
    "                     'ipc_binary_crisis_filled', 'ipc_future_crisis', 'ipc_period_start', 'ipc_period_end']\n",
    "\n",
    "        # Rename district column for merge if needed\n",
    "        if ipc_district_col != district_col:\n",
    "            ipc_lookup = ipc_lookup.rename(columns={ipc_district_col: district_col})\n",
    "\n",
    "        df = df.merge(\n",
    "            ipc_lookup[[district_col] + merge_cols].drop_duplicates(),\n",
    "            on=[district_col, 'year_month'],\n",
    "            how='left',\n",
    "            suffixes=('', '_ipc')\n",
    "        )\n",
    "\n",
    "        print(f\"   Merged IPC data with features\")\n",
    "        print(f\"   Target coverage: {df['ipc_future_crisis'].notna().sum():,} / {len(df):,} rows\")\n",
    "\n",
    "    # Compute class balance\n",
    "    if 'ipc_future_crisis' in df.columns:\n",
    "        n_crisis = df['ipc_future_crisis'].sum()\n",
    "        n_total = df['ipc_future_crisis'].notna().sum()\n",
    "        prevalence = n_crisis / n_total if n_total > 0 else 0\n",
    "        print(f\"   Class balance: {n_crisis:,} crises / {n_total:,} total ({prevalence:.1%})\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preserve_metadata(df):\n",
    "    \"\"\"Ensure all metadata columns are preserved.\"\"\"\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Preserving metadata columns...\")\n",
    "\n",
    "    available = [col for col in METADATA_COLUMNS if col in df.columns]\n",
    "    missing = [col for col in METADATA_COLUMNS if col not in df.columns]\n",
    "\n",
    "    print(f\"   Available metadata columns: {len(available)}\")\n",
    "    if missing:\n",
    "        print(f\"   Missing metadata columns: {missing[:5]}...\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    ensure_directories()\n",
    "\n",
    "    # Step 1: Load valid districts from Phase 1\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 1: Loading valid districts...\")\n",
    "    valid_districts = load_valid_districts()\n",
    "\n",
    "    # Step 2: Load and filter data\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 2: Loading and filtering data...\")\n",
    "    df, district_col = load_and_filter_data(valid_districts)\n",
    "\n",
    "    # Step 3: Compute ratio features\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 3: Computing ratio features...\")\n",
    "    df = compute_ratio_features(df)\n",
    "\n",
    "    # Step 4: Compute concentration metrics\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 4: Computing concentration metrics...\")\n",
    "    df = compute_concentration_metrics(df)\n",
    "\n",
    "    # Step 5: Compute temporal trends\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 5: Computing temporal trends...\")\n",
    "    df = compute_temporal_trends(df, district_col)\n",
    "\n",
    "    # Step 6: Compute momentum features\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 6: Computing momentum features...\")\n",
    "    df = compute_momentum_features(df, district_col)\n",
    "\n",
    "    # Step 7: Load AR predictions and spatial CV folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1b0bc",
   "metadata": {},
   "source": [
    "## Validation and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 7: Loading AR predictions and CV folds...\")\n",
    "    df = load_and_merge_ar_predictions(df, district_col)\n",
    "\n",
    "    # Step 8: Create spatial folds for ALL observations\n",
    "    # Step 8: Create target variable (must be done BEFORE spatial folds)\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 8: Creating target variable...\")\n",
    "    df = create_target_variable(df, district_col)\n",
    "\n",
    "    # Step 9: Creating spatial CV folds for ALL observations\n",
    "    # NOTE: Must be done AFTER IPC data is merged (which happens in create_target_variable)\n",
    "    # Spatial folds are created fresh to ensure ALL observations have assignments,\n",
    "    # not just AR failure cases. This is critical for proper train/test splits.\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 9: Creating spatial CV folds for ALL observations...\")\n",
    "    if 'fold' in df.columns:\n",
    "        df = df.drop(columns=['fold'])  # Remove any partial fold assignments\n",
    "    df['fold'] = create_spatial_folds(df, 'ipc_geographic_unit_full', n_folds=N_FOLDS, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Step 10: Preserve metadata\n",
    "    df = preserve_metadata(df)\n",
    "\n",
    "    # Step 11: Deduplicate observations (Stage 1 methodology)\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Step 11: Deduplicating observations...\")\n",
    "    print(f\"   Before deduplication: {len(df):,} rows\")\n",
    "\n",
    "    # Following Stage 1 methodology: unique observation = (ipc_geographic_unit_full, year_month)\n",
    "    df['observation_key'] = (\n",
    "        df['ipc_geographic_unit_full'].astype(str) + '_' +\n",
    "        df['year_month'].astype(str)\n",
    "    )\n",
    "\n",
    "    # Check for duplicates\n",
    "    dupes = df.duplicated(subset=['observation_key']).sum()\n",
    "    if dupes > 0:\n",
    "        print(f\"   Found {dupes:,} duplicate observations - aggregating...\")\n",
    "\n",
    "        # Identify columns to aggregate\n",
    "        # Count columns: sum\n",
    "        count_cols = ['article_count', 'location_mention_count', 'unique_sources']\n",
    "        count_cols = [col for col in count_cols if col in df.columns]\n",
    "\n",
    "        # Geographic coordinates: weighted mean by location_mention_count\n",
    "        geo_cols = ['avg_latitude', 'avg_longitude', 'latitude_std', 'longitude_std']\n",
    "        geo_cols = [col for col in geo_cols if col in df.columns]\n",
    "\n",
    "        # All ratio features: mean\n",
    "        ratio_cols = [col for col in df.columns if col.endswith('_ratio')]\n",
    "        concentration_cols = [col for col in df.columns if col.endswith(('_hhi', '_entropy'))]\n",
    "        trend_cols = [col for col in df.columns if 'trend' in col or 'slope' in col]\n",
    "        delta_cols = [col for col in df.columns if 'delta' in col]\n",
    "\n",
    "        # Metadata: first (should be identical)\n",
    "        metadata_cols = ['ipc_geographic_unit_full', 'ipc_district', 'ipc_country',\n",
    "                        'ipc_country_code', 'year_month', 'fold', 'ipc_future_crisis',\n",
    "                        'ipc_value', 'ipc_value_filled', 'ipc_binary_crisis', 'ipc_binary_crisis_filled',\n",
    "                        'ipc_period_start', 'ipc_period_end',\n",
    "                        'ar_pred_optimal_filled', 'ar_prob_filled', 'ar_pred_binary_filled',\n",
    "                        'ar_period_start', 'ar_period_end']\n",
    "        metadata_cols = [col for col in metadata_cols if col in df.columns]\n",
    "\n",
    "        # Build aggregation dict\n",
    "        agg_dict = {}\n",
    "        for col in count_cols:\n",
    "            agg_dict[col] = 'sum'\n",
    "        for col in geo_cols:\n",
    "            if col in df.columns and 'location_mention_count' in df.columns:\n",
    "                # Weighted mean\n",
    "                agg_dict[col] = lambda x: np.average(\n",
    "                    x,\n",
    "                    weights=df.loc[x.index, 'location_mention_count']\n",
    "                )\n",
    "            else:\n",
    "                agg_dict[col] = 'mean'\n",
    "        for col in ratio_cols + concentration_cols + trend_cols + delta_cols:\n",
    "            if col in df.columns:\n",
    "                agg_dict[col] = 'mean'\n",
    "        for col in metadata_cols:\n",
    "            if col in df.columns:\n",
    "                agg_dict[col] = 'first'\n",
    "\n",
    "        # Aggregate\n",
    "        df = df.groupby('observation_key').agg(agg_dict).reset_index()\n",
    "        df = df.drop(columns=['observation_key'])\n",
    "\n",
    "        print(f\"   After deduplication: {len(df):,} rows\")\n",
    "    else:\n",
    "        df = df.drop(columns=['observation_key'])\n",
    "        print(f\"   No duplicates found\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # SAVE OUTPUT\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Saving output...\")\n",
    "\n",
    "    output_path = PHASE2_RESULTS / OUTPUT_FILES['ratio_features']\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print(f\"   Saved: {output_path}\")\n",
    "\n",
    "    # Also save CSV for easier inspection\n",
    "    csv_path = PHASE2_RESULTS / 'ratio_features_h8.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"   Saved: {csv_path}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # SUMMARY\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2 STEP 1 COMPLETE: Ratio Feature Engineering\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Count feature types\n",
    "    ratio_cols = [col for col in df.columns if col.endswith('_ratio')]\n",
    "    trend_cols = [col for col in df.columns if '_trend_' in col]\n",
    "    delta_cols = [col for col in df.columns if col.endswith('_delta')]\n",
    "\n",
    "    print(f\"\\n   Total observations: {len(df):,}\")\n",
    "    print(f\"   Unique districts: {df[district_col].nunique():,}\")\n",
    "    print(f\"   Unique countries: {df['ipc_country'].nunique() if 'ipc_country' in df.columns else 'N/A'}\")\n",
    "    print(f\"\\n   Features created:\")\n",
    "    print(f\"      Ratio features: {len(ratio_cols)}\")\n",
    "    print(f\"      Trend features: {len(trend_cols)}\")\n",
    "    print(f\"      Momentum features: {len(delta_cols)}\")\n",
    "    print(f\"      Concentration metrics: 4\")\n",
    "    print(f\"      Total features: {len(df.columns)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
