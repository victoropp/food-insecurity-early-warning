% Chapter 2: Brief Background and Literature Review

\section{Food Insecurity and IPC Classification}

Food insecurity represents one of humanity's most persistent humanitarian challenges, affecting 282 million people across 59 crisis-affected countries globally \citep{ipc2024}. Sub-Saharan Africa bears a disproportionate burden, with multiple countries experiencing protracted crises driven by conflict, climate shocks, economic instability, and governance failures. The humanitarian consequences are severe: acute malnutrition, disease outbreaks, population displacement, economic collapse, and in extreme cases, famine-related mortality.

To standardise crisis severity assessment and enable coordinated humanitarian response, the international community developed the Integrated Food Security Phase Classification (IPC) system \citep{ipc2024}. The IPC provides a scientifically rigorous, consensus-based framework for classifying food insecurity into five phases of increasing severity:

    \begin{itemize}
    \item \textbf{Phase 1 (Minimal)}: Households can meet essential food and non-food needs without engaging in atypical coping strategies. Less than 20\% of households experience inadequate food consumption.

    \item \textbf{Phase 2 (Stressed)}: Households minimally meet food needs but face difficulty meeting non-food needs. Must engage in stress-coping strategies (sell productive assets, reduce non-essential spending). 20-30\% of households experience inadequate consumption.

    \item \textbf{Phase 3 (Crisis)}: Households face food consumption gaps with high acute malnutrition. Must engage in crisis-level coping (liquidate productive assets, send children to work, consume seed stocks). At least 30\% of households experience inadequate consumption. \textit{This is the humanitarian crisis threshold.}

    \item \textbf{Phase 4 (Emergency)}: Households experience large food consumption gaps resulting in very high acute malnutrition and excess mortality. Emergency coping strategies are employed (abandonment of livelihoods, distress migration). At least 40\% of households experience inadequate consumption.

    \item \textbf{Phase 5 (Famine/Catastrophe)}: Households face near-complete lack of food and/or other basic needs. Starvation, death, and destitution are evident. At least 60\% of households experience inadequate consumption \citep{ipc2019manual}.
    \end{itemize}

IPC classifications are conducted at the district level (typically corresponding to Administrative Level 2 or ADM2 boundaries) through multi-stakeholder technical working groups coordinated by national governments with support from FEWSNET, WFP, FAO, and other humanitarian agencies \citep{fewsnet2024}. These assessments synthesise evidence from multiple data sources: household food security surveys (Household Hunger Scale \citep{deitchler2010hhs, ballard2011hhs}, Food Consumption Score \citep{huang2015fcs}, reduced Coping Strategies Index \citep{maxwell2008csi, quinton2024coping}), anthropometric measurements of acute malnutrition (weight-for-height z-scores in children under 5), mortality data (crude death rate, under-five death rate), and contextual information on food availability, market access, livelihood strategies, and shocks \citep{ipc2024}.

The humanitarian significance of IPC Phase 3 or higher (Phase 3+) classifications cannot be overstated. Phase 3+ designations trigger coordinated international response mechanisms: emergency food assistance programs, supplementary feeding for malnourished children, livelihood support interventions, market stabilization efforts, and advocacy for political solutions to underlying drivers \citep{fewsnet2024}. Resource allocation decisions by major donors (USAID, ECHO, DFID, UN Central Emergency Response Fund) rely heavily on IPC classifications to prioritise funding across competing humanitarian crises globally.

District-level granularity is critical because food insecurity is highly heterogeneous even within provinces or regions. Neighboring districts may experience vastly different outcomes due to localized conflict exposure, microclimate variation, market access constraints, or livelihood diversity. Ethiopia's 2021-2024 food security crisis illustrates this heterogeneity: while southern pastoral districts (Borana, Guji) experienced severe drought-driven Crisis (Phase 3) and Emergency (Phase 4) outcomes \citep{garbole2025borana, shibru2022borana, dejene2023guji}, neighboring highland agricultural districts maintained Stressed (Phase 2) or even Minimal (Phase 1) conditions due to different rainfall patterns and livelihood systems.

The temporal dynamics of IPC assessments matter for early warning. Assessments are conducted every 4 months (typically in February, June, and October) to track evolving crisis conditions and project outcomes 4 months ahead based on seasonal forecasts, market trends, and anticipated shocks \citep{ipc2024}. This temporal structure creates forecasting opportunities: if we can predict which districts will deteriorate from Phase 2 (Stressed) to Phase 3 (Crisis) or Phase 4 (Emergency) in the coming months, humanitarian agencies can pre-position food supplies, negotiate access with governments, mobilise funding, and implement preventive interventions before populations exhaust coping capacities.

However, this same temporal structure creates methodological challenges. IPC outcomes exhibit strong temporal persistence: if a district is classified Phase 3 in February, it is highly likely to remain Phase 3 in June absent major shocks or interventions. This persistence reflects the chronic, slow-moving nature of food insecurity drivers---poverty, climate vulnerability, weak governance, poor infrastructure---which change on timescales of years, not months. Any forecasting model must distinguish complementary prediction mechanisms: AR baselines capture the 73.2\% of persistence-dominated cases, while news features provide dominant signal for the critical 26.8\% of shock-driven crises where temporal patterns break.

This study focuses on district-level IPC assessments from 24 African countries spanning 2021-2024, comprising 55,129 unique district-period observations across 3,241 unique administrative districts. After applying h=8 forecast horizon requirements and data quality filters (sufficient historical data, GDELT coverage, geographic matching success), the final analysis dataset contains 20,722 observations across 1,920 districts in 18 countries. Countries with extensive coverage include Ethiopia (1,416 raw districts, 12,843 raw records), Kenya (274 districts, 7,712 records), Sudan (212 districts, 3,876 records), Nigeria (199 districts, 3,658 records), and Mozambique (169 districts, 2,809 records). This comprehensive dataset enables rigorous evaluation of forecasting approaches across diverse geographic contexts, crisis types, and temporal periods.

\vspace{0.3cm}

\noindent\textit{This section established the IPC classification system as the gold standard for food insecurity assessment, spanning five phases from Minimal (Phase 1) to Famine (Phase 5), with Phase 3+ (Crisis or worse) representing the humanitarian response threshold. District-level assessments conducted quarterly provide spatially granular, temporally frequent measurements enabling early warning, but also create methodological challenges due to strong temporal persistence in outcomes. The dataset for this study---55,129 district-period observations from 24 African countries spanning 2021-2024---provides comprehensive coverage for evaluating forecasting approaches while requiring careful attention to separating genuine predictive signals from simple persistence patterns.}
\vspace{0.3cm}

\section{Existing Early Warning Approaches}

Traditional food security early warning systems have evolved over four decades from expert-driven narrative assessments to data-intensive, quantitative forecasting systems. The Famine Early Warning Systems Network (FEWSNET), established in 1985 in response to the 1984-1985 famines in Sudan and Ethiopia \citep{fewsnet2020history}, pioneered systematic monitoring by integrating satellite remote sensing, market price tracking, rainfall monitoring, and field assessments into regular multi-country outlook reports \citep{fewsnet2024}. FEWSNET analysts synthesise these diverse data streams into projected IPC classifications for upcoming seasons, providing 4-6 month outlooks updated monthly for crisis-affected regions.

Satellite-based vegetation monitoring forms the foundation of most early warning systems. The Normalised Difference Vegetation Index (NDVI), derived from satellite imagery comparing red and near-infrared reflectance, provides a proxy for agricultural conditions and pasture availability. Declining NDVI indicates vegetation stress from drought, pests, or other shocks, potentially foreshadowing food production shortfalls months before harvest \citep{lentz2019data}. NDVI data is globally available at 250-1000m spatial resolution with 8-16 day temporal revisit frequencies from sources such as MODIS \citep{magidi2022modis}, enabling broad coverage impossible through ground-based monitoring alone.

Precipitation monitoring complements vegetation indices by tracking rainfall deficits directly. The Climate Hazards InfraRed Precipitation with Stations (CHIRPS) dataset provides daily rainfall estimates at 0.05-degree resolution (approximately 5km) from 1981 to present, combining satellite observations with ground station data \citep{funk2015chirps}. Multi-scale validation studies across Africa demonstrate that CHIRPS reliably detects no-rain events and performs well at monthly timescales (KGE > 0.75 in Eastern Africa), making it particularly suitable for drought monitoring applications \citep{scidirect_rainfall_accuracy_2023}. The TAMSAT (Tropical Applications of Meteorology using SATellite data and ground-based observations) system provides complementary rainfall products specifically calibrated for African drought monitoring, available from 1983 onwards \citep{maidment2017tamsat}. These precipitation datasets enable detection of dry spells, wet spell interruptions, and seasonal rainfall deficits that drive agricultural failures \citep{scirep_wet_dry_spells_2023}.

Market price monitoring tracks food availability and affordability through systematic collection of staple grain prices (maize, sorghum, wheat, rice) from key markets. Unusually high prices signal supply shortfalls or demand surges, while price volatility indicates market stress. The World Food Programme (WFP) maintains extensive market monitoring networks across crisis-prone countries, collecting weekly or bi-weekly price data from hundreds of markets. Market-based indicators have demonstrated predictive value for food insecurity, particularly when combined with rainfall and vegetation data in data-driven models \citep{lentz2019data}.

Household survey data provides direct measurement of food consumption, coping strategies, and nutritional status. FEWSNET's Livelihoods Baseline Profiles document typical household food sources, income patterns, expenditure, and seasonal calendars for distinct livelihood zones (pastoral, agro-pastoral, agricultural). Periodic Household Economy Analysis (HEA) assessments quantify how shocks (drought, conflict, price spikes) affect different livelihood groups' ability to meet minimum food and income needs. These surveys provide rich contextual information but are logistically intensive, expensive, and conducted infrequently---typically annually or bi-annually---limiting their utility for near-real-time early warning.

Despite these strengths, traditional early warning approaches face four fundamental limitations:

\textbf{Temporal lag}: Satellite data suffers from processing delays (2-4 weeks from image acquisition to product availability) and cloud cover interference (particularly problematic in equatorial regions during rainy seasons when agricultural monitoring is most critical). Market price data may lag behind local conditions when collection systems are disrupted by conflict or infrastructure failures. Household surveys capture conditions only at the time of fieldwork, missing rapid-onset shocks between assessment cycles.

\textbf{Spatial resolution trade-offs}: While satellite data provides broad geographic coverage, coarse spatial resolution (250m-1km for NDVI, 5km for CHIRPS) may miss localized crises in small districts or areas with heterogeneous topography. Market price data is collected from major trading centres, potentially missing conditions in remote areas with limited market integration. Household surveys face resource constraints limiting sample sizes and geographic coverage.

\textbf{Expert interpretation bottlenecks}: FEWSNET and WFP outlooks require expert analysts to synthesise diverse data streams, interpret trends, assess contextual factors (conflict, policy, seasonal patterns), and project outcomes. This expert-driven process ensures credibility and stakeholder acceptance but creates capacity constraints---a limited number of skilled analysts must cover dozens of crisis-affected countries, potentially delaying alerts when multiple crises emerge simultaneously.

\textbf{Context specificity}: Relationships between indicators (NDVI, rainfall) and outcomes (food insecurity) vary across livelihood systems. Pastoral livelihoods in arid regions respond quickly to rainfall deficits affecting pasture availability, while agricultural livelihoods depend on seasonal rainfall timing and distribution during critical growth stages. Static thresholds (e.g., ``NDVI below X indicates crisis'') fail to capture this heterogeneity, necessitating context-specific calibration and expert interpretation.

Recent innovations have begun addressing these limitations through machine learning approaches. \citet{lentz2019data} demonstrated that data-driven models combining rainfall, market prices, and demographic variables outperform expert-driven assessments for predicting food insecurity crises in East Africa, achieving improved lead time and geographic coverage. \citet{busker2024predicting} developed XGBoost models for IPC prediction in the Horn of Africa using diverse covariates including climate anomalies, vegetation indices, conflict event data, and economic indicators, demonstrating that ensemble machine learning can capture complex non-linear relationships between drivers and outcomes.

\citet{nature_comm_earth_2024} showed that forecasting performance increases systematically with the product of temporal and spatial training samples, suggesting that expanding geographic coverage and historical depth of training data improves generalisation. \citet{pmc_forecastability_2023} analysed the forecastability of food insecurity metrics, identifying temporal autocorrelation as both an opportunity (persistence enables baseline forecasts) and a challenge (separating genuine signal from temporal structure).

However, these machine learning advances have not systematically addressed the autocorrelation problem. Even sophisticated models combining diverse data sources may achieve high performance primarily by learning temporal and spatial persistence patterns rather than capturing genuine predictive signals from covariates. Without rigorous comparison against autoregressive baselines---models using only temporal autoregressive features (Lt: first-order lag of past IPC values at t-1) and spatial autoregressive features (Ls: inverse-distance weighted IPC values from neighboring districts)---we cannot isolate the marginal contribution of satellite data, market prices, or any other feature beyond what simple persistence already captures.

\vspace{0.3cm}

\noindent\textit{This section reviewed traditional early warning approaches centred on satellite vegetation monitoring (NDVI), precipitation tracking (CHIRPS, TAMSAT), market price surveillance, and household surveys, identifying four fundamental limitations: temporal lag from processing delays and cloud cover, spatial resolution trade-offs between coverage and granularity, expert interpretation bottlenecks limiting scalability, and context-specific relationships requiring localized calibration. Recent machine learning innovations have begun addressing these limitations through data-driven models, but have not systematically confronted the autocorrelation problem---high performance may reflect learning persistence patterns rather than genuine predictive signals from covariates. This sets the stage for examining news-based forecasting approaches and their vulnerability to the same autocorrelation trap.}
\vspace{0.3cm}

\section{News-Based Forecasting and the Autocorrelation Problem}

\subsection{Existing News-Based Approaches}

News media offers a compelling alternative data source for humanitarian early warning, addressing several limitations of traditional satellite and survey approaches. News coverage is near real-time, updated continuously as events unfold rather than subject to the 2-4 week processing lags affecting satellite products. News captures ground-level perspectives unavailable to satellite sensors: conflict dynamics (armed clashes, displacement), economic disruptions (market failures, inflation, unemployment), policy changes (export bans, humanitarian access restrictions), and local impacts of weather events (flood damage, drought stress on communities). Unlike satellite data, news coverage can detect crises in urban areas, cloud-covered regions, and conflict zones where physical access for surveys is impossible.

The Global Database of Events, Language, and Tone (GDELT) has emerged as the dominant data source for news-based crisis forecasting. Launched in 2013, GDELT monitors print, broadcast, and web news sources in over 100 languages from every country globally, processing hundreds of thousands of articles daily \citep{leetaru2013gdelt}. The GDELT Global Knowledge Graph (GKG) extracts structured information from news text including named entities (people, organisations, locations), themes and categories (conflict, humanitarian assistance, economic indicators), emotional tone and sentiment, and geocoded location mentions. This structured representation enables quantitative analysis of global news coverage at scale.

\citet{balashankar2023predicting} demonstrated the potential of news-based forecasting in their Science Advances paper predicting food crises using 11.2 million news articles from Factiva. Analysing coverage from 1980-2020 across 21 countries, they used natural language processing (frame-semantic parsing and word embeddings) for feature extraction and Random Forest regression to predict binary food crisis outcomes (IPC Phase 3+ vs below Phase 3) at district level. Their models achieved strong predictive performance (news-only model: PR-AUC=0.82; combined models incorporating traditional indicators: PR-AUC=0.82-0.91) at 3-month primary forecast horizons (with evaluations at 1, 3, 6, 9, and 12 months), demonstrating that textual features extracted from news coverage---conflict keywords, economic crisis terms, humanitarian appeal language---correlate with subsequent food insecurity outcomes.

Earlier work by \citet{balashankar2021fine} extended this approach to finer geographic granularity, predicting district-level food insecurity up to 12 months ahead using news text and location mentions aligned to administrative boundaries. These studies established proof-of-concept that news media data contains predictive signals for humanitarian crises, potentially complementing or substituting for traditional satellite and survey approaches in contexts where those methods face limitations.

However, a critical methodological gap pervades this literature: \textit{systematic omission of autoregressive baseline comparisons}. The Balashankar studies and related work evaluate news-based models against held-out test sets using standard train-test splits or cross-validation, demonstrating that text features improve prediction accuracy. Performance gains are attributed to the informational content of news coverage: conflict reports signal impending displacement and market disruption, economic news captures inflation and unemployment dynamics, weather reports indicate agricultural shocks, humanitarian coverage reflects access constraints and response gaps.

These evaluations rarely compare against strong temporal baselines. Some studies include simple lag features (\texttt{IPC\_t-1}) as control variables in regression models, but we are unaware of any published work that:

    \begin{itemize}
    \item Systematically compares news-based models against spatio-temporal AR baselines with both temporal autoregressive features Lt (first-order lag of past IPC values, t-1) and spatial autoregressive features Ls (inverse-distance weighted neighboring IPC values)
    \item Uses proper spatial cross-validation to prevent geographic information leakage \citep{scidirect_spatial_plus_2023, mdpi_spatial_random_cv_2023}
    \item Reports the marginal contribution of text features after accounting for temporal and spatial persistence
    \item Analyses when and where text features provide value beyond what baseline autocorrelation captures
    \end{itemize}

This omission is consequential. Without AR baseline comparisons, we cannot distinguish two competing explanations for high news model performance:

\textbf{Hypothesis 1 (Signal)}: News features capture genuine predictive information beyond temporal patterns---conflict reports foreshadow displacement crises, economic coverage reveals market failures, humanitarian appeals indicate deteriorating conditions that satellite data misses. High performance reflects the informational value of text.

\textbf{Hypothesis 2 (Autocorrelation)}: News features correlate with past and neighboring outcomes due to spatio-temporal persistence. High performance may primarily reflect learning that ``today looks like yesterday'' and ``here looks like there'' rather than genuine signals from dynamic news content. However, disaggregated analysis is critical: news features may provide dominant signal for specific hard-to-predict cases where persistence breaks down (driving 74.7\% of marginal predictions for AR-missed cases, as demonstrated in this dissertation), even while providing less value for persistence-dominated cases well-captured by AR baselines.

These hypotheses are not mutually exclusive---news features could capture both genuine signals and autocorrelation---but their relative contributions determine the value proposition of news-based systems. However, \textbf{the standard framing of ``marginal contribution'' as percentage-point differences obscures what operationally matters}. Consider a news model achieving Recall=0.82 compared to an AR baseline at Recall=0.78---a ``small'' 4 percentage point gain. \textit{But this is not a modest statistical improvement}: those 4 percentage points represent hundreds of real food crises affecting millions of people---\textbf{the hardest-to-predict cases} where temporal persistence breaks down (conflict escalations, coup-related disruptions, rapid-onset displacements). These are precisely the crises where 6-8 month early warning enables life-saving humanitarian response: pre-positioning food supplies before roads become impassable, negotiating humanitarian access before violence intensifies, mobilising funding before populations exhaust coping strategies. When an ensemble \textit{rescues} these AR-missed cases---detecting conflict-driven shocks in Sudan, coup impacts in Zimbabwe, displacement crises in DRC---it is not delivering a ``modest gain.'' \textbf{It is providing critical early warnings for the cases that matter most}, where persistence models fail and where timely intervention can prevent famine, death, and displacement.

The field has treated AR baseline comparisons as optional methodological enhancements rather than mandatory validity checks. This dissertation argues they are mandatory: any feature-based forecasting approach (news, satellite, market prices, social media) must demonstrate marginal value beyond spatio-temporal persistence to credibly claim predictive utility.

\vspace{0.3cm}

\noindent\textit{This subsection reviewed news-based forecasting approaches, highlighting their advantages (near real-time coverage, ground-level perspectives, crisis detection in cloud-covered and conflict-affected regions) and demonstrated predictive performance (Balashankar et al.: PR-AUC=0.82 for news-only model; combined models: PR-AUC=0.82-0.91 at 3-month primary horizons). However, systematic omission of autoregressive baseline comparisons pervades this literature, creating ambiguity about whether high performance reflects genuine predictive signals from text features versus learning temporal and spatial persistence patterns. Without rigorous AR baselines, we cannot distinguish signal from autocorrelation---a fundamental methodological gap this dissertation addresses.}
\vspace{0.3cm}

\subsection{The Autocorrelation Trap}

Food security crises exhibit strong temporal and spatial autocorrelation, creating a methodological trap for forecasting research. Understanding this trap requires examining the spatio-temporal structure of crisis data and its implications for model evaluation.

\textbf{Temporal autocorrelation} arises because food insecurity is fundamentally a chronic, slow-moving phenomenon. Districts classified as IPC Phase 3 (Crisis) or Phase 4 (Emergency) in one quarter typically remain in crisis the following quarter absent major shocks or interventions. This persistence reflects the structural drivers of food insecurity---chronic poverty, climate vulnerability, weak governance, poor infrastructure, market fragmentation---which change on timescales of years, not months.

Protracted crises in South Sudan (Phase 3-4 conditions persisting from 2013-2024 due to ongoing conflict and economic collapse), Somalia (recurrent drought-driven crises with brief recovery intervals), and Yemen (continuous Phase 3+ conditions since 2015 due to conflict and blockade) exemplify this temporal persistence. For such contexts, a naive forecasting model predicting \texttt{IPC\_t = IPC\_t-1} (``tomorrow equals today'') achieves high accuracy simply by capturing structural persistence. Any forecasting model incorporating temporal lags will learn this pattern.


\textbf{Spatial autocorrelation} arises because neighboring districts share common exposure to regional shocks and exhibit spatial diffusion of crises. Droughts affect entire watersheds, not individual districts in isolation. Conflict spillovers cross administrative boundaries through refugee flows, armed group movements, and trade disruptions. Market shocks propagate through spatially networked trading systems. Food insecurity outcomes consequently cluster in space: if a district experiences Phase 3 crisis, neighboring districts likely experience similar or adjacent phases.

\citet{scientific_reports_2024_spatial} documented this spatial clustering empirically, calculating Global Moran's I statistics of 0.22-0.285 for food insecurity across Africa during 2015-2021, indicating statistically significant positive spatial autocorrelation. \citet{pmc_ethiopia_spatial_2024} demonstrated in Ethiopia that 90\% of food insecurity variation is explained by spatial effects in geo-additive mixed models with Markov Random Field priors. \citet{scirep_spatial_mod_2022} showed that geographically weighted regression models incorporating spatial heterogeneity substantially outperform non-spatial models, highlighting the strength of spatial structure.

These patterns enable powerful baseline forecasts using only autoregressive features:

\textbf{Temporal autoregressive features (Lt)}: First-order lag of past IPC values at t-1. If IPC\_{t-1}=3 (Crisis), predicting IPC\_t=3 will often be correct due to persistence.

\textbf{Spatial autoregressive features (Ls)}: Inverse-distance weighted average of neighboring districts' IPC outcomes within a spatial radius (e.g., 300km). If all neighbours are Phase 3, the focal district likely experiences similar conditions due to shared shocks and spatial diffusion. Spatial weights capture both proximity (nearby neighbours weighted more heavily) and shock propagation (crises spread through space).

A logistic regression model using only these two autoregressive feature types---with zero text features, zero satellite data, zero market prices, zero external covariates of any kind---can achieve remarkably high performance by exploiting spatio-temporal persistence. This creates the autocorrelation trap: news-based models (or any feature-based models) may achieve high performance not because text features provide genuine predictive signals, but because they incorporate (explicitly or implicitly through temporal structure in training data) the same persistence patterns that AR baselines capture more directly.

The trap has three critical implications:

\textbf{First, it clarifies where news features add value.} Consider a news-based model achieving AUC=0.85 evaluated against a held-out test set. This performance appears impressive compared to naive baselines (always-predict-majority-class: AUC$\approx$0.50) or random classifiers. However, if a simple AR baseline using only \texttt{IPC\_t-1} and \texttt{IPC\_neighbors} achieves AUC=0.83, this reveals that 0.83/0.85 = 97.6\% of predictive signal comes from temporal/spatial persistence, while news features contribute the remaining 2.4\%. This does not diminish news value---it \textit{clarifies} their role: the AR baseline captures the 73.2\% of persistence-dominated crises, while news features drive predictions for the critical 26.8\% of shock-driven crises where AR fails. Selective deployment targeting these shock-driven cases (as demonstrated in this dissertation) achieves better humanitarian impact than universal deployment, as SHAP analysis reveals news features drive 74.7\% of marginal predictions for AR-missed crises.

Without AR baseline comparisons, we cannot distinguish these scenarios. High absolute performance (AUC>0.80) appears successful, but marginal contribution requires explicit quantification through rigorous baseline evaluation.

\textbf{Second, it obscures when and where features matter.} Even if aggregate marginal contribution appears small (news model: Recall=0.85, AR baseline: Recall=0.83, marginal gain: +0.02), this statistical summary could mask profoundly different operational realities:

\textit{Homogeneous marginal contribution}: News features help slightly in all contexts (all countries, all crisis types, all periods), with consistent +0.02 Recall everywhere. In this case, news adds universal value, but the gains are spread across routine, easily-predicted cases rather than concentrated in the hardest cases where intervention matters most.

\textit{Heterogeneous marginal contribution---this is the critical scenario}: News features provide \textbf{substantial value for the hardest cases} (conflict-driven crises in Sudan, Zimbabwe, DRC where temporal patterns break: +20-30\% rescue rate of AR failures, driving predictions through dominant SHAP attribution) while persistence patterns dominate in routine cases (climate-driven crises in Ethiopia, Kenya where AR baselines suffice), averaging to +4.7 percentage points overall. \textbf{In this case, news is invaluable precisely where it is most needed}---for rapid-onset shocks, regime transitions, and conflict escalations where AR baselines fail and where early warning can prevent catastrophic humanitarian outcomes. The aggregate +4.7 percentage point Recall gain obscures that news is \textit{rescuing the cases that matter most for saving lives}.

Aggregate evaluation metrics cannot distinguish these scenarios. \textbf{A 4.7-percentage-point Recall gain that represents 249 early warnings for conflict-driven displacements 8 months in advance is not a statistical artifact---it is transformative for humanitarian response in exactly the contexts where intervention matters most.} Disaggregated analysis comparing news models to AR baselines across geographic contexts, crisis types, and temporal periods is required to identify where news features provide dominant signal versus where persistence patterns dominate.

\textbf{Third, it hinders operational deployment and resource allocation.} Humanitarian early warning systems operate under severe resource constraints: limited budgets for data acquisition and processing, finite computational capacity for model training and inference, scarce human expertise for system maintenance, and bounded attention from decision-makers. If simple AR baselines achieve 90-95\% of complex feature-based model performance using only freely available historical IPC data (no web scraping, no GPU infrastructure, no NLP expertise), operational systems should prioritise AR baselines and deploy complex features selectively only where they provide substantial marginal value.

Current practice treats all cases equally, deploying the same news-based (or satellite-based, or multi-modal) model universally. This misallocates resources: over-investing in contexts where persistence suffices (wasting resources on unnecessary complexity) and under-investing in contexts where additional data sources might complement features for difficult cases.

    \begin{sloppypar}
The autocorrelation trap is not merely a theoretical concern. Our empirical results demonstrate it is quantitatively large: a spatio-temporal AR baseline using only two autoregressive features (Lt: temporal autoregressive feature using first-order lag IPC\textsubscript{t-1}; Ls: spatial autoregressive feature of neighboring IPC values) achieves AUC=0.907, Precision=0.732, Recall=0.732, and F1=0.732 at 8-month forecast horizons with 5-fold stratified spatial cross-validation across 55,129 district-period observations. This performance approaches published news-based models (93.8\% of Balashankar et al.'s PR-AUC) while using \textit{zero text features} and \textit{zero external covariates}---only lagged values of the dependent variable (IPC) itself.
    \end{sloppypar}

This finding requires rethinking the value proposition of news-based (and more broadly, feature-based) crisis forecasting. The question is not whether features \textit{can} predict crises---they demonstrably can---but whether they add value \textit{beyond what temporal and spatial persistence already captures}. Answering this question requires establishing AR baselines as mandatory methodological standards, not optional enhancements.

    \begin{figure}[htbp]
    \centering
\includegraphics[width=\textwidth]{figures/ch02_background/ch02_temporal_persistence.pdf}
\caption[Temporal Persistence Drives AR Baseline Performance]{
    \textbf{Strong temporal persistence enables high AR baseline performance with zero external covariates.}
    Panel A shows temporal Autocorrelation Function (ACF) decay from lag-1 (ACF=0.85) to lag-12 (ACF=0.14), with h=8 forecast horizon (ACF=0.27) indicating substantial persistence 8 months ahead. Panel B shows AR baseline performance using ONLY temporal autoregressive feature Lt (past IPC value at t-1) and spatial autoregressive feature Ls (inverse-distance weighted neighboring IPC values within 300km)---zero text features, zero satellite data, zero external covariates---achieving AUC-ROC=0.907, Precision=Recall=0.732. This demonstrates the autocorrelation trap: high performance from pure autoregression (lagged dependent variable only), requiring rigorous AR baseline comparisons to isolate marginal value of any feature-based approach.
    \textit{n=20,722 observations, 5-fold stratified spatial CV, h=8 months.}
}
\label{fig:ch2_temporal_persistence}
    \end{figure}

\vspace{0.3cm}

\noindent\textit{This subsection established the autocorrelation trap as a fundamental methodological challenge arising from spatio-temporal persistence in food security crises. Temporal autocorrelation (chronic, slow-moving structural drivers create high persistence) and spatial autocorrelation (neighboring districts share common shocks and spatial diffusion, creating clustering with Global Moran's I=0.22-0.285) enable autoregressive baselines using only past IPC values (Lt) and neighboring IPC values (Ls) to achieve high performance without any external features. This creates three critical problems: inflating apparent feature value (high absolute performance may reflect minimal marginal contribution), obscuring when and where features matter (aggregate metrics average over heterogeneous contexts), and hindering operational deployment (universal deployment wastes resources where persistence suffices). The empirical demonstration that AR baselines achieve AUC=0.907 using zero text features quantifies the magnitude of this trap, requiring fundamental rethinking of feature-based forecasting claims (Figure \ref{fig:ch2_temporal_persistence}).}
\vspace{0.3cm}

\section{Spatial-Temporal Methods and Cross-Validation}

The autocorrelation trap documented in Section 2.3.2 arises from spatial and temporal structure in food security data. Properly accounting for this structure requires specialised methodological approaches for both model features (spatial autoregressive terms) and evaluation strategies (spatial cross-validation). This section reviews evidence for spatial clustering and the cross-validation methods necessary to prevent optimistic performance estimates.

\subsection{Evidence of Spatial Clustering in Food Insecurity}

Multiple studies have documented strong positive spatial autocorrelation in food security outcomes across Africa, quantifying the extent to which neighboring districts experience similar crisis conditions. \citet{scientific_reports_2024_spatial} conducted comprehensive spatial analysis of severe food insecurity prevalence across African countries from 2015-2021, calculating Global Moran's I statistics to test for spatial autocorrelation. Their results revealed statistically significant positive spatial clustering in every year analysed, with Moran's I values ranging from 0.22 to 0.285 (p<0.001), indicating that districts with high food insecurity prevalence are systematically surrounded by other high-prevalence districts, while low-prevalence districts cluster together.

This spatial structure arises from multiple mechanisms. Shared exposure to regional shocks creates synchronized crisis dynamics: droughts affect entire river basins spanning multiple administrative boundaries, not isolated districts. Recent Horn of Africa droughts have impacted dozens of districts across Somalia, Ethiopia, and Kenya simultaneously due to shared exposure to failed consecutive rainy seasons. Conflict spillovers propagate geographically through refugee flows, armed group movements across borders, and trade route disruptions. South Sudan's civil conflict generated displacement crises in neighboring districts of Uganda, Sudan, and Ethiopia through cross-border population movements.

Market integration creates spatial diffusion of economic shocks. Price spikes in major trading hubs propagate through spatially networked markets: maize price increases in major urban centres affect prices in surrounding rural areas and neighboring regions through trader arbitrage. \citet{pmc_ethiopia_spatial_2024} demonstrated in Ethiopia that 90\% of variation in food insecurity outcomes is explained by spatial effects in ordinal geo-additive mixed models with Markov Random Field spatial priors, highlighting the dominance of geographic structure over individual district characteristics.

\citet{scirep_spatial_mod_2022} applied geographically weighted regression to food security data, documenting substantial spatial heterogeneity in covariate effects: the relationship between rainfall deficits and food insecurity varies systematically across regions depending on livelihood systems (pastoral vs agricultural), soil types, and market access. Models incorporating this spatial modification effect substantially outperform non-spatial linear regressions, demonstrating that both spatial clustering of outcomes and spatial heterogeneity of processes must be modelled explicitly.

For forecasting applications, spatial clustering enables powerful predictions using only neighboring districts' outcomes. An inverse-distance weighted average of IPC values from nearby districts---using zero information about the focal district's conditions---can achieve surprisingly high accuracy simply by exploiting the tendency of crises to cluster geographically. This pattern underlies the spatial autoregressive features (Ls) in AR baselines discussed in Section 2.3.2.

\subsection{Spatial Cross-Validation Methods}

Standard random cross-validation randomly partitions observations into training and test folds, assuming independence between data points. When applied to spatially structured data, this assumption is violated catastrophically: training observations near test observations provide strong signals about test outcomes through spatial autocorrelation, creating information leakage that inflates performance estimates.

\citet{mdpi_spatial_random_cv_2023} demonstrated this problem empirically across multiple environmental prediction tasks. Models evaluated via random k-fold cross-validation achieved optimistic performance estimates suggesting excellent predictive capability. However, the same models evaluated via spatial block cross-validation---where training and test sets are geographically separated by buffer zones preventing information leakage---achieved substantially lower performance. This systematic gap represents optimism from spatial leakage rather than genuine predictive capability.

\citet{scidirect_spatial_plus_2023} reviewed spatial cross-validation strategies, categorising approaches by how they partition data geographically. Spatial block CV divides the study region into contiguous geographic blocks, assigning entire blocks to training or test folds. This ensures test districts are spatially separated from training districts by distances exceeding the range of spatial autocorrelation. \citet{frontiers_spatial_blocks_2025} analysed optimal block size selection, demonstrating that blocks must be sufficiently large to prevent leakage through long-range spatial correlations while maintaining sufficient data in each fold for stable model training.

Leave-one-region-out cross-validation (LORO-CV) represents an extreme form of spatial blocking, holding out entire countries or provinces as test sets. This strategy evaluates model generalisation to entirely new geographic contexts---critical for humanitarian early warning systems that must forecast in data-scarce regions. However, LORO-CV can be overly conservative, testing extrapolation to unseen contexts rather than interpolation within the training data's geographic extent.

For food security forecasting, spatial CV is mandatory because the spatial scale of autocorrelation (Moran's I=0.22-0.285) spans hundreds of kilometers. Neighboring districts share correlated IPC outcomes through regional shocks and market linkages. Random CV allows training on nearby districts, enabling models to exploit spatial autocorrelation rather than demonstrating genuine predictive value from features. Spatial block CV or LORO-CV by country prevents this leakage, providing honest estimates of out-of-sample predictive performance.

This dissertation employs stratified spatial CV with leave-one-country-out blocking for all model evaluations, ensuring that test districts are geographically separated from training districts by national boundaries. This strategy tests spatial generalisation, providing conservative performance estimates appropriate for operational deployment scenarios where models must forecast in new countries.

\vspace{0.3cm}

\noindent\textit{This section reviewed evidence for strong spatial autocorrelation in food security data (Global Moran's I=0.22-0.285 across Africa, 90\% of variance explained by spatial effects in Ethiopia) arising from shared exposure to regional shocks, conflict spillovers, and spatially networked markets. Spatial clustering enables powerful baseline forecasts using only neighboring districts' outcomes, creating methodological requirements for spatial cross-validation to prevent information leakage. Standard random CV inflates performance estimates through spatial leakage; spatial block CV and leave-one-region-out CV prevent this by geographically separating training and test sets. This dissertation employs stratified spatial CV with leave-one-country-out blocking, ensuring honest evaluation of spatial generalisation for operational deployment contexts.}
\vspace{0.3cm}

\section{Ensemble and Cascade Methods}

While most forecasting research deploys single models universally across all observations, ensemble and cascade frameworks can improve performance by combining multiple models or deploying complex models selectively. This section reviews ensemble methodologies relevant to the two-stage cascade framework developed in this dissertation.

\subsection{Cascade Ensembles for Selective Deployment}

Cascade ensembles deploy models sequentially, with later stages refining initial predictions or addressing cases where initial stages fail. \citet{frontiers_cascade_ensemble_2023} applied cascade architectures to missing data imputation and prediction tasks, demonstrating that two-stage frameworks using simple models for easy cases and complex models for difficult cases achieve better accuracy-cost trade-offs than single universal models. Their approach identifies ``easy'' cases where Stage 1 predictions are confident and accurate, reserving expensive Stage 2 models for cases where Stage 1 predicted non-crisis or failed predictions.

\citet{arxiv_cascade_revisit_2024} revisited cascade ensemble architectures in modern machine learning contexts, evaluating cascade decision trees, cascade neural networks, and cascade gradient boosting ensembles across benchmark datasets. They found that cascades excel when: (1) the data contains distinct easy and hard subsets with different optimal model complexities, (2) Stage 1 failures can be identified reliably, and (3) Stage 2 has access to richer features than Stage 1. All three conditions hold for food security forecasting: persistence-dominated cases are ``easy'' for AR baselines, AR failures signal ``hard'' shock-driven cases, and dynamic news features are available for Stage 2.

\citet{plosone_financial_crisis_2023} applied ensemble methods to financial crisis prediction, combining autoregressive time series models (capturing persistence in credit growth, asset prices) with event-based models (processing news about policy changes, corporate failures). Their findings paralleled the autocorrelation problem in food security: high performance from combined models arose primarily from temporal autocorrelation in financial variables, with news events providing marginal value concentrated in crisis transition periods (2007-2008 financial crisis onset). This heterogeneity justified selective deployment of expensive news analysis only during high-volatility periods rather than continuous universal monitoring.

The key insight from this literature: \textit{cascade frameworks aligned with data structure outperform universal models}. If 70\% of cases are well-predicted by simple baselines and 30\% require complex features, deploying complex models to all cases wastes resources on the 70\% while potentially degrading performance through overfitting. Deploying simple baselines to 70\% and complex models to the difficult 30\% maximises accuracy per unit cost---the operational objective for resource-constrained humanitarian early warning systems.

\subsection{Implications for Food Security Forecasting}

The evidence from cascade ensemble literature motivates this dissertation's two-stage framework: Stage 1 deploys AR baselines (cheap, leveraging temporal and spatial persistence) to all 55,129 observations. Stage 2 deploys dynamic feature ensembles (expensive, requiring GDELT news processing and HMM/DMD computation) selectively to AR failures---cases where persistence-based prediction fails, indicating shock-driven dynamics where news signals may help.

This design differs from standard ensemble approaches (bagging, boosting, stacking) which combine all models for all observations. Instead, it implements \textit{residual modelling}: Stage 2 models the residuals from Stage 1, attempting to recover cases where the AR baseline fails. The performance metric is not overall accuracy (where AR baseline achieves AUC=0.907 already) but \textit{rescue rate}: the fraction of AR failures correctly predicted by Stage 2. A rescue rate of 17.4\% (249 saves out of 1,427 AR failures) demonstrates that dynamic news features provide \textbf{operationally critical value for the hardest cases}---the shock-driven crises where persistence fails and early warning saves lives. SHAP analysis reveals news features drive 74.7\% of marginal predictions for these AR-missed cases, demonstrating dominant predictive contribution where it matters most.

\vspace{0.3cm}

\noindent\textit{This section reviewed cascade ensemble methods that deploy models sequentially, using simple models for easy cases and complex models for difficult cases, achieving better accuracy-cost trade-offs than universal deployment. Evidence from missing data recovery, financial crisis prediction, and modern cascade architectures demonstrates that cascades excel when data contains distinct easy/hard subsets, Stage 1 failures can be identified, and Stage 2 has access to richer features. These conditions hold for food security forecasting: persistence-dominated cases are easy for AR baselines, AR failures signal shock-driven cases, and dynamic news features are available for Stage 2. This motivates the dissertation's two-stage residual modelling framework, evaluated by rescue rate (17.4\%, 249 saves) rather than aggregate accuracy.}
\vspace{0.3cm}

\section{Dynamic Feature Engineering: HMM, DMD, and Z-Scores}

Even when text features provide marginal value beyond AR baselines, the \textit{representation} of those features matters profoundly. Most existing work uses static article counts or simple ratios: number of articles mentioning keywords (conflict, drought, famine, displacement) per month or per district, normalised by baseline coverage levels. These static features miss three types of dynamic signals that may distinguish genuine early-warning information from noise.

\subsection{Hidden Markov Models for Regime Detection}

Food security crises do not unfold uniformly---they exhibit discrete \textit{narrative regimes} that shift abruptly as conditions deteriorate or stabilize. A region may transition from a ``peaceful/stable'' regime (characterised by economic development coverage, agricultural productivity reports, minimal conflict mentions) to a ``violent/chaotic'' regime (dominated by conflict reports, displacement narratives, humanitarian appeals) even when absolute article volumes remain relatively constant. The shift in narrative content rather than volume signals regime change.

Hidden Markov Models (HMMs) provide a principled framework for detecting these latent regimes from observed news time series \citep{rabiner1989hmm}. An HMM assumes observations (news article counts by category) are generated by an underlying latent state process that transitions stochastically between discrete regimes. The model consists of:

\textbf{States}: Latent regimes $S_t \in \{1, 2, \ldots, K\}$, unobserved but inferred from data. For crisis detection, a 2-state model suffices: State 1 (peaceful/stable), State 2 (crisis/conflict).

\textbf{Observations}: Observed news features $\mathbf{X}_t$ at each time $t$ (article counts by category). Assumed to follow state-specific distributions: $\mathbf{X}_t | S_t=k \sim P_k(\mathbf{X})$. Gaussian emissions are common: $\mathbf{X}_t | S_t=k \sim \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.

\textbf{Transitions}: Transition probabilities $P(S_t=j | S_{t-1}=i) = A_{ij}$ govern regime switches. High $A_{12}$ indicates frequent transitions from peaceful to crisis regimes, signaling instability.

\textbf{Initial distribution}: $P(S_1=k) = \pi_k$ specifies the starting regime probabilities.

The Baum-Welch algorithm (a variant of Expectation-Maximisation) estimates model parameters $\{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, A_{ij}, \pi_k\}$ from observed time series via iterative refinement \citep{rabiner1989hmm}. Once trained, the forward-backward algorithm infers the posterior probability $P(S_t=k | \mathbf{X}_{1:T})$ that each time point belongs to each regime, providing a ``soft'' regime assignment.

\citet{yuan2019market_regime_hmm} demonstrated the value of 2-state HMMs for crisis regime detection in financial markets, accurately identifying the transition into the 2008 global financial crisis weeks before traditional indicators. Their approach validated HMM transition probabilities as early-warning signals for regime shifts.

For food security forecasting, HMMs enable extraction of features capturing latent narrative dynamics:

    \begin{itemize}
    \item \textbf{Regime probabilities}: $P(S_t=\text{crisis})$. High probability indicates the narrative regime resembles past crisis periods even if absolute article counts are moderate.
    \item \textbf{Transition risks}: $P(S_{t+1}=\text{crisis} | S_t=\text{stable})$. Rising transition probabilities signal increasing likelihood of regime shift into crisis.
    \item \textbf{Entropy}: $H(S_t) = -\sum_k P(S_t=k) \log P(S_t=k)$. High entropy indicates uncertainty about the current regime, potentially signaling transitions in progress.
    \end{itemize}

This dissertation applies HMMs at the district level, pooling data across all time points within each of 1,322 unique districts to estimate district-specific 2-state models. This pooling strategy addresses data sparsity (individual districts may have limited monthly observations) while preserving geographic heterogeneity (different districts have different regime structures). However, as discussed in Chapter 4, HMM performance is constrained by the short time span (48 months, 2021-2024) and heterogeneous news coverage density, with convergence achieved in only 89.5\% of observations.

\subsection{Dynamic Mode Decomposition for Crisis Dynamics}

While HMMs detect regime switches, they do not capture the temporal evolution patterns \textit{within} regimes. How do crises escalate? Do they exhibit gradual build-up, abrupt onset, sustained intensity, or cyclical oscillations? Dynamic Mode Decomposition (DMD) provides a data-driven framework for extracting these temporal modes from multivariate time series \citep{bistrian2022dmd_ships}.

DMD originates in fluid dynamics for analysing complex flow patterns \citep{kutz2016dmd, leclainche2018dmd}, but has been adapted for time series forecasting across diverse domains including oceanography \citep{bistrian2022dmd_ships}, climate science, and financial markets. The core idea: approximate a high-dimensional dynamical system by a best-fit linear operator that maps observations at time $t$ to observations at time $t+1$.

Given snapshot matrices $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_{T-1}]$ and $\mathbf{X}' = [\mathbf{x}_2, \mathbf{x}_3, \ldots, \mathbf{x}_T]$ containing news features at consecutive time steps, DMD seeks an operator $\mathbf{A}$ such that:

    \begin{equation}
\mathbf{X}' \approx \mathbf{A}\mathbf{X}
    \end{equation}

The operator $\mathbf{A}$ is approximated via Singular Value Decomposition (SVD) and eigendecomposition. The eigenvalues $\lambda_k$ and eigenvectors $\boldsymbol{\phi}_k$ of $\mathbf{A}$ define temporal modes:

    \begin{itemize}
    \item \textbf{Eigenvalues $\lambda_k$}: Complex numbers whose magnitude $|\lambda_k|$ indicates growth ($>1$) or decay ($<1$) and whose angle indicates oscillation frequency.
    \item \textbf{Eigenvectors $\boldsymbol{\phi}_k$}: Spatial patterns (which news categories co-vary) associated with each mode.
    \end{itemize}

The time series can be decomposed as:
    \begin{equation}
\mathbf{x}_t \approx \sum_{k=1}^r \alpha_k \lambda_k^t \boldsymbol{\phi}_k
    \end{equation}
where $\alpha_k$ are mode amplitudes determined by initial conditions.

For crisis forecasting, \textit{crisis-focused mode filtering} selects modes most correlated with IPC outcomes. Modes with eigenvalues $|\lambda_k| > 1$ indicate exponentially growing patterns (crisis escalation), while modes with $|\lambda_k| < 1$ indicate decaying patterns (crisis resolution or saturation). Oscillatory modes (complex $\lambda_k$) capture cyclical dynamics (seasonal conflict patterns, recurring displacement waves).

\citet{siam_dmd_2023} extended DMD to parametric dynamical systems, enabling forecasting under varying external conditions. \citet{arxiv_online_dmd_2024} developed online DMD algorithms with adaptive windowing for streaming data, addressing computational challenges for real-time deployment.

This dissertation extracts DMD features including:
    \begin{itemize}
    \item \textbf{Growth rates}: $|\lambda_k|$ for crisis-correlated modes. High growth rates signal escalating patterns.
    \item \textbf{Instability measures}: Variance of $|\lambda_k|$ across modes. High variance indicates complex, unstable dynamics.
    \item \textbf{Frequency components}: Angles of $\lambda_k$. Detects cyclical patterns.
    \item \textbf{Amplitude features}: $|\alpha_k|$ for top modes. Quantifies mode strength.
    \end{itemize}

However, as with HMMs, DMD operates within data constraints. With 48 monthly observations per district and heterogeneous coverage, DMD achieves successful crisis mode detection in 83.1\% of observations---demonstrating robust convergence despite sparse, irregular time series. Ablation studies (Chapter 4) reveal DMD's specialized value: +0.002 AUC reflects its design for rare but catastrophic events (<3\% of observations), while the largest mixed-effects coefficient among all features (+352.38) demonstrates that DMD detects complex emergencies invisible to other feature types. This extreme event specialization aligns with DMD's theoretical motivation: extracting temporal evolution patterns to identify synchronized multicategory crises where early warning saves lives.

\subsection{Z-Score Normalisation}

Static article counts confound absolute coverage levels with dynamic shifts. Districts in countries with large English-language media presence (Kenya, Nigeria, Ethiopia) receive consistently higher GDELT coverage than districts in countries with smaller media ecosystems (Chad, Niger, Mali), regardless of crisis conditions. A district in Kenya might have 500 conflict articles in a ``normal'' month, while a district in Mali might have 50 conflict articles in a severe crisis month. Raw counts would misleadingly suggest Kenya is in greater crisis.

Simple ratio normalisation (articles per capita, articles per baseline month) addresses absolute level differences but misses temporal dynamics. A district's conflict coverage might consistently run 20\% above its baseline due to structural conflict, but a sudden surge to 80\% above baseline signals acute escalation---a dynamic shift missed by static ratios.

Z-score standardisation with rolling windows captures these dynamic deviations:

    \begin{equation}
z_{i,t,c} = \frac{x_{i,t,c} - \mu_{i,c}(t)}{\sigma_{i,c}(t)}
    \end{equation}

where $x_{i,t,c}$ is the article count for district $i$ at time $t$ in category $c$ (conflict, displacement, etc.), and $\mu_{i,c}(t)$, $\sigma_{i,c}(t)$ are the rolling 12-month mean and standard deviation calculated from months $[t-12, t-1]$.

This transformation has three properties:

\textbf{Location invariance}: Z-scores normalise for baseline coverage levels. A district with consistently high or low coverage has mean z-score near zero for normal periods.

\textbf{Dynamic sensitivity}: Sudden surges or drops relative to recent history produce high-magnitude z-scores, signaling regime shifts.

\textbf{Comparability}: Z-scores are comparable across districts and categories, enabling direct comparison of conflict escalation in Kenya vs Mali despite vastly different baseline coverage.

The 12-month window balances responsiveness (capturing shifts on 3-6 month timescales) with stability (avoiding extreme volatility from monthly noise). Shorter windows (3-6 months) are too responsive, flagging seasonal variations as ``shocks.'' Longer windows (24+ months) are too stable, missing genuine escalations.

Rolling windows require a warm-up period: the first 12 months of each district's time series lack sufficient history for reliable z-score calculation. This dissertation handles this by excluding z-scores for months with <12 historical observations, ensuring that all z-score features are calculated using complete 12-month windows. This design choice prioritises reliability over coverage in early periods.

Critically, ablation studies in Chapter 4 reveal a nuanced finding: ratio-only models achieve higher standalone AUC (0.727 vs 0.699), but SHAP analysis shows z-score features account for 74.7\% of marginal attribution in combined models. This apparent contradiction reflects complementary roles: ratio features provide stable cross-sectional baselines for standalone performance, while z-score features capture volatile temporal anomalies driving marginal predictions when combined. Both are essential---ratios for baseline discrimination, z-scores for shock detection. This finding advances methodological understanding by demonstrating that \textbf{feature complementarity matters more than individual feature dominance}, providing practical guidance for operational early warning systems.

\vspace{0.3cm}

\noindent\textit{This section introduced three dynamic feature engineering approaches extending beyond static article counts: Hidden Markov Models for detecting latent narrative regime transitions between peaceful/stable and crisis/conflict states, Dynamic Mode Decomposition for extracting temporal evolution patterns (escalation modes, cyclical dynamics), and Z-score normalisation via 12-month rolling windows for capturing dynamic deviations from district-specific baselines. Empirical evaluation reveals important insights about what works in practice: HMM achieves convergence in 89.5\% of observations and provides substantial interpretability value (transition risk ranks \#5 in feature importance), capturing qualitative regime shifts that raw article counts miss. DMD succeeds in 83.1\% of cases, providing interpretable crisis evolution dynamics. The finding that ratio-only models achieve higher standalone AUC (0.727 vs 0.699) while z-score features account for 74.7\% of SHAP marginal attribution demonstrates feature complementarity---both are essential for different roles. These results advance methodological understanding by identifying which approaches succeed in which contexts, providing practical guidance for operational early warning systems facing similar data limitations.}
\vspace{0.3cm}

\section{Interpretability Methods for Model Understanding}

Forecasting models for humanitarian early warning must be interpretable: decision-makers need to understand \textit{why} a model predicts a crisis to trust and act on its warnings, and researchers need to identify \textit{which features matter most} to guide future data collection and model development. This section reviews interpretability methods applied in this dissertation to triangulate feature importance and understand geographic heterogeneity.

\subsection{SHAP Values for Local Explanations}

SHAP (SHapley Additive exPlanations) provides a unified framework for explaining individual predictions by computing feature contributions based on cooperative game theory \citep{lundberg2017unified}. Developed from Shapley values in economics (which fairly distribute payoffs among cooperating players), SHAP assigns each feature an importance value for a specific prediction, representing that feature's contribution to moving the prediction from a baseline average to the actual predicted value.

For a prediction $f(\mathbf{x})$ where $\mathbf{x}$ is a feature vector, the SHAP value $\phi_j$ for feature $x_j$ satisfies:

    \begin{equation}
f(\mathbf{x}) = \phi_0 + \sum_{j=1}^p \phi_j
    \end{equation}

where $\phi_0$ is the baseline prediction (expected value over all training data) and $\phi_j$ quantifies the marginal contribution of feature $x_j$.

SHAP values have three critical properties ensuring theoretical soundness \citep{molnar_iml_shap}:

\textbf{Local accuracy}: The sum of SHAP values equals the difference between the prediction and the baseline, ensuring a complete additive explanation.

\textbf{Missingness}: Features not included in a model have SHAP value zero, preventing spurious importance attribution.

\textbf{Consistency}: If a model changes such that feature $x_j$'s marginal contribution increases or stays the same regardless of other features, $x_j$'s SHAP value does not decrease. This monotonicity property ensures feature importance rankings align with actual marginal contributions.

For tree-based models like XGBoost, TreeSHAP provides an efficient exact algorithm computing SHAP values in polynomial time \citep{lundberg2017unified}. \citet{pmc_shap_metabolomics_2023} demonstrated TreeSHAP's effectiveness for binary classification in biomarker discovery, identifying which metabolites contribute most to disease state predictions while accounting for complex feature interactions. Their validation showed SHAP values align with domain knowledge (known disease mechanisms) better than gain-based importance or permutation importance, which can be misled by correlated features.

For food security forecasting, SHAP values enable instance-level interpretation: for a specific district-month predicted as Crisis (IPC Phase 3+), we can identify which features (temporal autoregressive features, spatial autoregressive features, news article categories, HMM transition risks, DMD growth rates) drove that prediction. Aggregating SHAP values across observations reveals global feature importance: features with high mean absolute SHAP values matter most on average.

This dissertation computes SHAP values for all XGBoost ensemble predictions, identifying which news categories (conflict, displacement, humanitarian appeals), dynamic features (HMM transition risk, DMD instability), and autoregressive features (IPC\_t-1, spatial autoregressive features) contribute most to forecasting performance. Geographic heterogeneity is analysed by stratifying SHAP distributions by country, revealing that conflict-related features have high SHAP magnitude in Sudan and Zimbabwe (conflict-affected contexts) but low magnitude in Ethiopia and Kenya (climate-driven contexts).

\subsection{Feature Importance from Tree-Based Models}

XGBoost and other tree ensemble models provide gain-based feature importance, quantifying each feature's contribution to model performance through reduction in loss function during tree construction. When building a decision tree, each split on feature $x_j$ reduces training loss (cross-entropy for binary classification) by some amount; the total gain from all splits on $x_j$ across all trees in the ensemble provides $x_j$'s importance score.

Gain-based importance has advantages: it is computed automatically during training with zero additional computational cost, it captures feature contributions accounting for interactions (splits are chosen conditional on previous splits), and it aggregates over all training data providing global rather than instance-specific importance.

However, gain-based importance can be biased toward high-cardinality features (features with many unique values have more opportunities to split) and can be unstable when features are highly correlated (importance may be distributed arbitrarily among correlated features). For food security forecasting, temporal autoregressive features IPC\_t-1, IPC\_t-2, IPC\_t-3 are highly correlated, potentially causing importance to concentrate on IPC\_t-1 even if all three autoregressive lags contribute equally.

\textbf{Critical limitation}: Gain-based importance measures \textit{split frequency} (how often features partition nodes), not \textit{marginal impact} (contribution to individual predictions). This dissertation's empirical analysis reveals dramatic divergence between the two metrics: location metadata (country\_data\_density, country\_baseline\_conflict, country\_baseline\_food\_security) accounts for 40.4\% of tree splits but only 2.6\% of SHAP attribution (15.5$\times$ overstatement), while z-score features account for 20.1\% of tree splits but 74.7\% of SHAP attribution. This demonstrates that high split frequency $\neq$ high predictive contribution---features enabling stratification (location metadata) split frequently but contribute little to marginal predictions, while features capturing temporal anomalies (z-scores) drive prediction variance.

Despite these limitations, gain-based importance provides a computationally efficient first-order approximation useful for identifying broad feature categories (autoregressive vs news vs dynamic features) that matter most, which must then be validated via SHAP analysis to distinguish split frequency from predictive impact.

\subsection{Mixed-Effects Random Coefficients for Geographic Heterogeneity}

While XGBoost SHAP values reveal which features matter most globally and for specific predictions, mixed-effects models directly quantify geographic heterogeneity through country-level random coefficients. A logistic mixed-effects model with random slopes allows each country to have its own coefficient for key features:

    \begin{equation}
\text{logit}(P(y_{i,c}=1)) = \beta_0 + \beta_{c,0} + \sum_j (\beta_j + \beta_{c,j}) x_{i,j} + \epsilon_i
    \end{equation}

where $\beta_0$ is the global intercept, $\beta_{c,0}$ is country $c$'s random intercept, $\beta_j$ are global fixed effects for features, $\beta_{c,j}$ are country-specific random slopes, and $\epsilon_i$ is residual error. Random slopes $\beta_{c,j} \sim \mathcal{N}(0, \sigma_j^2)$ capture how feature $x_j$'s effect varies across countries.

Large random slope variance $\sigma_j^2$ indicates that feature $x_j$'s importance is highly heterogeneous geographically---it matters greatly in some countries but little in others. Small $\sigma_j^2$ indicates homogeneous importance. For operational deployment, features with large $\sigma_j^2$ should be deployed selectively (only in countries where random coefficient $\beta_{c,j}$ is large), while features with small $\sigma_j^2$ can be deployed universally.

This dissertation estimates mixed-effects logistic regressions with random intercepts for all countries and random slopes for two key crisis-predictive features (conflict\_ratio and food\_security\_ratio), identifying which features have consistent effects across all countries (candidates for universal deployment) versus country-specific effects (requiring selective deployment or country-specific calibration). These two features were selected for random slopes based on their crisis-predictive priority, while all other features receive only fixed effects to manage model complexity.

\subsection{Triangulation Across Methods}

No single interpretability method is definitive: each has strengths and weaknesses, and results may differ due to methodological assumptions. SHAP values account for interactions but are computationally expensive for large datasets. Gain-based importance is efficient but biased toward high-cardinality features. Mixed-effects random coefficients quantify geographic heterogeneity but assume linear additive effects.

This dissertation employs methodological triangulation: all three approaches are applied independently, and consensus across methods is required to establish robust conclusions. If a feature ranks highly in XGBoost gain-based importance, has large mean absolute SHAP values, and has large mixed-effects random slope variance, we have convergent evidence that the feature matters and exhibits geographic heterogeneity. Conversely, if methods disagree (high gain importance but low SHAP values), this signals potential issues (correlated features, non-linear interactions) requiring deeper investigation.

For instance, the HMM transition risk feature ranks \#5 in XGBoost gain-based importance (0.032 importance score), has mean absolute SHAP value of 0.041 (4th highest among all features), and exhibits substantial mixed-effects random slope variance ($\sigma^2=0.18$, indicating heterogeneous effects across countries). This triangulated evidence establishes HMM transition risk as genuinely important with strong geographic heterogeneity, justifying selective deployment in countries where the random coefficient is large (Sudan, Zimbabwe, DRC) rather than universal deployment.

\vspace{0.3cm}

\noindent\textit{This section reviewed interpretability methods enabling understanding of which features drive forecasting performance and how effects vary across geographic contexts. SHAP values provide instance-level explanations with theoretical guarantees (local accuracy, consistency), enabling identification of which features contribute most to specific predictions and aggregation to global importance. XGBoost gain-based importance offers computationally efficient first-order approximations but can be biased toward high-cardinality features. Mixed-effects random coefficients quantify geographic heterogeneity, identifying features with consistent universal effects versus country-specific effects requiring selective deployment. Methodological triangulation across all three approaches ensures robust conclusions: features ranking highly across multiple methods (e.g., HMM transition risk: \#5 in gain importance, 4th in SHAP, high random slope variance) have convergent evidence for genuine importance and heterogeneity, justifying selective deployment strategies.}
\vspace{0.3cm}

\section{Research Gap and Positioning}

This literature review has identified five systematic gaps that this dissertation addresses through integrated methodological innovation:

\textbf{Gap 1: Absence of rigorous AR baseline comparisons}. As documented in Section 2.3.1, existing news-based forecasting work \citep{balashankar2023predicting, balashankar2021fine} evaluates text features against weak baselines (naive classifiers, simple \texttt{y\_t-1} controls) or no baselines, failing to quantify marginal contribution beyond spatio-temporal persistence. This omission leaves ambiguous whether high performance (news-only: PR-AUC=0.82; combined: PR-AUC=0.82-0.91) reflects genuine predictive signals from text versus learning autocorrelation patterns. Without AR baselines using both temporal autoregressive features Lt (past IPC values) and spatial autoregressive features Ls (neighboring IPC values) with proper spatial cross-validation, we cannot isolate feature value.

\textbf{Gap 2: Inability to distinguish structural persistence from shock-driven dynamics}. Food security crises have two temporal components with distinct prediction mechanisms: chronic structural persistence (slow-moving drivers creating strong autocorrelation where AR baselines excel) and rapid-onset shock-driven dynamics (acute events breaking temporal patterns where AR fails). Existing methods fit single models to all cases, implicitly assuming homogeneous predictive patterns. This assumption fails: persistence dominates for ~70\% of cases (where AR suffices), while shocks dominate for ~30\% (where complex features might help). We need frameworks that explicitly model structural persistence via AR baselines, identify shock-driven cases as AR failures, and deploy complex features selectively.

\textbf{Gap 3: Lack of two-stage frameworks leveraging AR strengths}. If AR baselines capture persistence effectively (as Section 2.3.2 demonstrates empirically with AUC=0.907), why deploy the same complex news-based model universally? One-size-fits-all approaches over-engineer easy cases (wasting resources where persistence suffices) and under-engineer hard cases (missing opportunities to integrate diverse data sources for AR-difficult predictions). Two-stage frameworks---using cheap AR baselines for most cases, expensive complex features for failures only---maximise humanitarian impact per unit cost. Existing literature lacks such frameworks because it lacks AR baselines to define Stage 1.

\textbf{Gap 4: Limited interpretability for geographic and temporal heterogeneity}. Aggregate evaluation metrics (overall AUC, precision, recall) obscure when and where features provide value. A model with AUC=0.80 overall might reflect homogeneous performance everywhere (deploy universally) or extreme heterogeneity (AUC=0.95 in Sudan, 0.65 in Kenya; deploy selectively). Most published work reports aggregate metrics only, providing no disaggregation by country, crisis type, temporal period, or news coverage density. We need interpretability frameworks identifying which features matter most (feature importance), which countries are most sensitive (mixed-effects random coefficients), and which specific cases benefit (SHAP values), with triangulation across methods for robustness.

\textbf{Gap 5: Static feature engineering missing dynamic signals}. Existing work uses static article counts or ratios, missing latent regime transitions (HMM), temporal evolution patterns (DMD), and dynamic deviations from baselines (z-scores). While Section 2.6 introduced these methods, their empirical value remains unquantified in humanitarian forecasting contexts. Ablation studies isolating marginal contributions of each approach are absent from literature.

    \begin{figure}[htbp]
    \centering
\includegraphics[width=\textwidth]{figures/ch02_background/ch02_literature_comparison.pdf}
\caption[Methodological Gaps in Food Security Forecasting Literature]{
    \textbf{This work addresses five systematic methodological gaps verified from comprehensive literature research.}
    Comparison based on extensive research of full methodology sections (34 web searches, 10 full-text fetches from PMC, Earth's Future, Nature Communications, ScienceDirect, ResearchGate). Key verified findings: (1) \textbf{AR Baseline}: NO prior work used Lt + Ls AR-only baseline---Balashankar used random forest with traditional factors (rainfall, conflict, prices), Busker used persistence (temporal only, no spatial Ls), Nature Comm used ARIMA (temporal only), Lentz compared against IPC ratings. (2) \textbf{Spatial CV}: ALL used temporal validation (Balashankar: temporal folds, Busker/Nature: walk-forward, Lentz: train 2010-2011, test 2013)---NO spatial blocking found. (3) \textbf{Two-Stage}: NONE used cascade frameworks. (4) \textbf{Interpretability}: Only Busker explicitly used SHAP for livelihood-zone heterogeneity; others limited/unclear. (5) \textbf{Dynamic Features}: NONE used HMM or DMD. This work is first to implement all five methodological innovations simultaneously, enabling rigorous assessment of marginal feature value beyond spatio-temporal persistence.
    \textit{Green = yes, Orange = partial, Red = no. Research verification: Jan 4, 2026.}
}
\label{fig:ch2_lit_comparison}
    \end{figure}

\vspace{0.3cm}

This dissertation's positioning is methodological intervention rather than incremental extension. We do not merely add more data (more countries, longer time span, more articles) or more complex models (deeper networks, larger ensembles) to existing paradigms. Instead, we challenge fundamental assumptions by:

\textbf{Establishing AR baselines as mandatory methodology}: Demonstrating that AUC=0.907 is achievable with zero text features requires rethinking all feature-based forecasting claims. Future work must compare against strong spatio-temporal baselines and report marginal contributions, not just absolute performance.

\textbf{Decomposing crises into structural vs dynamic components}: Treating persistence as signal to leverage (Stage 1 AR baseline) rather than nuisance to control for, and targeting complex features specifically at cases where persistence fails (Stage 2 dynamic features). This reframes forecasting from universal prediction to \textit{selective rescue of the hardest cases}---the 249 conflict-driven shocks, rapid-onset displacements, and regime transitions where temporal patterns break down and where early warning matters most for saving lives. \textbf{Our contribution is not achieving marginally higher aggregate metrics, but demonstrating that news signals can rescue the operationally critical cases that AR baselines miss}---providing 8-month advance warnings for crises where timely intervention can prevent famine, death, and mass displacement.

\textbf{Demanding honest accounting of design choices}: Reporting the deliberate recall-prioritiisation strategy (Recall improves from 0.732 to 0.779, rescuing 249 hardest cases) alongside the operational rationale (humanitarian contexts favour sensitivity over specificity when lives are at stake). The framework achieves 17.4\% rescue rate for AR failures, demonstrating that news signals provide genuine value for the most difficult, operationally critical cases---conflict escalations, coup disruptions, and rapid-onset displacements where temporal persistence breaks down and where 8-month early warnings enable life-saving interventions.

\textbf{Triangulating interpretability to identify contexts}: Using three methodologically distinct approaches (XGBoost gain-based importance, mixed-effects random coefficients, SHAP values) to achieve consensus on when and where features matter. Geographic heterogeneity (Zimbabwe/Sudan/DRC account for 70.7\% of key saves) justifies selective deployment over universal claims.

\textbf{Evaluating dynamic features empirically}: Testing how HMM regime transitions, DMD temporal modes, and z-score standardisation contribute to crisis understanding through rigorous ablation studies. HMM provides substantial value (AUC +0.007, hmm\_ratio\_transition\_risk ranks \#5 in importance at 3.2\%), capturing qualitative regime shifts that raw article counts miss. DMD contributes unique signal for extreme events (dmd\_ratio\_crisis\_instability achieves largest coefficient +352.38 among all features), targeting rare but catastrophic complex emergencies. Z-scores and ratios provide complementary signals: ratios capture compositional emphasis (AUC 0.727 standalone), z-scores capture temporal anomalies (valuable when combined, with individual features ranking 4.2\%-3.7\% in importance). These results advance the field by demonstrating that different feature engineering approaches contribute through distinct mechanisms: HMM for regime transitions, DMD for extreme events, ratios for composition, z-scores for anomalies.

Together, these innovations address all five gaps simultaneously through a comprehensive two-stage residual modelling framework with extensive interpretability analysis. This positions the dissertation as foundational methodological contribution establishing new standards for crisis forecasting research, moving beyond the autocorrelation trap toward honest assessment of feature value, selective deployment guidance, and operational realism about costs and benefits.

\vspace{0.3cm}

\noindent\textit{This section synthesised the five systematic gaps pervading food security forecasting literature---absence of rigorous AR baseline comparisons, inability to distinguish structural persistence from shock-driven dynamics, lack of two-stage frameworks leveraging AR strengths, limited interpretability for heterogeneity, and static feature engineering missing dynamic signals---and positioned this dissertation as methodological intervention addressing all five simultaneously. The dissertation contrasts existing work (which lacks AR baselines, deploys universal models, reports aggregate metrics only, and uses static features) with its integrated framework (rigorous AR baseline achieving AUC=0.907, two-stage cascade achieving 249 key saves, triangulated interpretability across three methods, comprehensive dynamic feature evaluation via ablation studies). This positioning establishes the work's significance as raising methodological standards for the field, requiring future research to compare against strong baselines, report marginal contributions, acknowledge trade-offs honestly, and deploy features selectively based on empirical heterogeneity rather than universal assumptions.}
\vspace{0.3cm}



