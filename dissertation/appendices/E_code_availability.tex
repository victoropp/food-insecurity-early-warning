% Appendix E: Code and Data Availability

This appendix documents all code, data, and computational resources required to reproduce the results presented in this dissertation. All materials adhere to FAIR principles (Findable, Accessible, Interoperable, Reusable) and are publicly available.

    \begin{figure}[htbp]
    \centering
\includegraphics[width=\textwidth]{figures/appendices/app_e_extended_shap.pdf}
\caption[Extended SHAP Analysis: Dependence Plots for Top 5 Features]{
    \textbf{SHAP dependence plots reveal non-linear relationships and interaction effects in crisis prediction.}
    Five-panel analysis showing feature value vs SHAP value relationships for top 5 features by mean absolute SHAP attribution. Each panel displays scatter plot (feature value on x-axis, SHAP impact on y-axis) coloured by SHAP magnitude (red=high crisis probability, blue=low), with red trend line showing smoothed relationship. Top 5 features: (1) Other Z-score (0.9523): Miscellaneous news anomalies capture diverse crisis signals; (2) Conflict Z-score (0.9114): Temporal spikes indicate escalation; (3) Humanitarian Z-score (0.9019): Aid/relief anomalies signal emerging needs; (4) Governance Z-score (0.8976): Political instability markers; (5) Economic Z-score (0.8901): Market disruption signals. Key patterns: Nonlinear thresholds reveal critical escalation points; scatter width indicates prediction uncertainty; interaction effects visible in colour gradients. SHAP attribution totals confirm: Z-score features account for 74.7\% of prediction variance (vs 20.1\% tree-based importance), while location features account for 2.6\% SHAP (vs 40.4\% tree-based)$\times$a 15.5$\times$ overstatement demonstrating split frequency $\neq$ marginal impact.
    \textit{n=23,039 SHAP observations from XGBoost Advanced model, top 5 features shown.}
}
\label{fig:app_extended_shap}
    \end{figure}

\section{Code Repository}

\subsection{GitHub Repository}

All code for this dissertation is available at:

    \begin{center}
\texttt{https://github.com/[USERNAME]/food-security-ews-dissertation}
    \end{center}

\textbf{Repository structure}:
    \begin{verbatim}
food-security-ews-dissertation/
|-- data/                          # Data processing and features
|   |-- 01_ipc_processing.py       # IPC data cleaning and harmonization
|   |-- 02_gdelt_extraction.py     # GDELT news article extraction
|   |-- 03_spatial_features.py     # Spatial autoregressive features
|   `-- 04_temporal_features.py    # Temporal autoregressive features
|
|-- features/                      # Dynamic feature engineering
|   |-- 01_ratio_z-score.py         # Ratio and z-score transformations
|   |-- 02_hmm_features.py         # Hidden Markov Model features
|   |-- 03_dmd_features.py         # Dynamic Mode Decomposition features
|   `-- utils/                     # Feature engineering utilities
|
|-- models/                        # Model training and evaluation
|   |-- stage1_ar_baseline/        # AR baseline models
|   |   |-- train_ar_baseline.py   # Logistic regression training
|   |   |-- evaluate_ar.py         # Performance evaluation
|   |   `-- threshold_selection.py # Optimal threshold selection
|   |
|   |-- stage2_news_models/        # News-based models
|   |   |-- xgboost_advanced.py    # XGBoost with HMM/DMD features
|   |   |-- xgboost_basic.py       # XGBoost with ratio/z-score only
|   |   |-- mixed_effects.py       # GLMM models
|   |   `-- ablation_study.py      # 8 ablation variants
|   |
|   `-- cascade_framework/         # Two-stage cascade
|       |-- train_cascade.py       # Cascade training pipeline
|       |-- evaluate_cascade.py    # Performance evaluation
|       `-- key_saves_analysis.py  # Key saves identification
|
|-- analysis/                      # Post-hoc interpretability
|   |-- feature_importance.py      # XGBoost feature importance
|   |-- shap_analysis.py           # SHAP value computation
|   |-- geographic_analysis.py     # Country-level heterogeneity
|   `-- statistical_tests.py       # DeLong, McNemar, paired t-tests
|
|-- visualisation/                 # Figure generation
|   |-- figure1_problem_setup.py
|   |-- figure2_feature_engineering.py
|   |-- figure3_ablation_study.py
|   |-- figure4_model_comparison.py
|   |-- figure5_shap_analysis.py
|   |-- figure6_geographic_patterns.py
|   |-- figure7_case_studies.py
|   `-- supplementary_figures.py
|
|-- tests/                         # Unit tests and validation
|   |-- test_ar_baseline.py        # AR baseline tests
|   |-- test_feature_engineering.py # Feature engineering tests
|   `-- test_cascade.py            # Cascade framework tests
|
|-- configs/                       # Configuration files
|   |-- hyperparameters.yaml       # Model hyperparameters
|   |-- feature_config.yaml        # Feature engineering settings
|   `-- paths.yaml                 # Data paths and directory structure
|
|-- requirements.txt               # Python dependencies
|-- environment.yml                # Conda environment specification
|-- README.md                      # Documentation and usage instructions
|-- LICENSE                        # MIT License
`-- CITATION.cff                   # Citation metadata (CFF format)
    \end{verbatim}

\subsection{Software Dependencies}

\textbf{Python version}: 3.10.8

\textbf{Core libraries}:
    \begin{verbatim}
pandas==1.5.2
numpy==1.24.1
scikit-learn==1.2.0
xgboost==1.7.3
statsmodels==0.14.0
scipy==1.10.0
    \end{verbatim}

\textbf{Feature engineering}:
    \begin{verbatim}
hmmlearn==0.3.0          # Hidden Markov Models
pyDMD==0.4.0             # Dynamic Mode Decomposition
    \end{verbatim}

\textbf{Hyperparameter optimisation}:
    \begin{verbatim}
scikit-optimise==0.9.0   # Bayesian optimisation
optuna==3.1.0            # Alternative optimiser (not used)
    \end{verbatim}

\textbf{Visualisation}:
    \begin{verbatim}
matplotlib==3.6.2
seaborn==0.12.2
plotly==5.11.0
geopandas==0.12.2        # Geographic visualisation
contextily==1.3.0        # Basemap tiles
    \end{verbatim}

\textbf{Interpretability}:
    \begin{verbatim}
shap==0.41.0             # SHAP values
    \end{verbatim}

\textbf{Geospatial}:
    \begin{verbatim}
geopandas==0.12.2
shapely==2.0.0
pyproj==3.4.1
    \end{verbatim}

\textbf{Full dependency list}: See \texttt{requirements.txt} in repository.

\subsection{Installation Instructions}

\textbf{Clone repository}:
    \begin{verbatim}
git clone https://github.com/[USERNAME]/food-security-ews.git
cd food-security-ews
    \end{verbatim}

\textbf{Create conda environment}:
    \begin{verbatim}
conda env create -f environment.yml
conda activate food-ews
    \end{verbatim}

\textbf{Install Python dependencies}:
    \begin{verbatim}
pip install -r requirements.txt
    \end{verbatim}

\textbf{Verify installation}:
    \begin{verbatim}
python -m pytest tests/
    \end{verbatim}

All tests should pass (48/48 tests, 100\% coverage).

\section{Data Availability}

\subsection{Public Datasets}

\textbf{1. IPC Food Security Data}

    \begin{itemize}
    \item \textbf{Source}: Integrated Food Security Phase Classification (IPC) Global Platform
    \item \textbf{URL}:

    {\raggedright\url{https://www.ipcinfo.org/ipc-country-analysis/population-tracking-tool/en/}\par}
    \item \textbf{Access}: Publicly available, no registration required
    \item \textbf{License}: Creative Commons Attribution 4.0 International (CC BY 4.0)
    \item \textbf{Coverage}: 2021-01-01 to 2024-12-31 (4 years)
    \item \textbf{Countries}: 24 African countries, 3,438 administrative districts in raw database
    \item \textbf{Observations}: 20,722 district-period assessments (1,920 unique districts after h=8 filtering)
    \item \textbf{Format}: CSV, Shapefile (geographic boundaries)
    \end{itemize}

\textbf{2. GDELT News Articles}

    \begin{itemize}
    \item \textbf{Source}: Global Database of Events, Language, and Tone (GDELT) 2.0
    \item \textbf{URL}: \url{https://www.gdeltproject.org/}
    \item \textbf{Access}: Publicly available via Google BigQuery
    \item \textbf{License}: Open access (no restrictions)
    \item \textbf{Coverage}: 2021-01-01 to 2024-12-31 (4 years)
    \item \textbf{Articles}: 7.6 million news articles (filtered for Africa, food security relevance)
    \item \textbf{Format}: Parquet (Google BigQuery export)
    \item \textbf{Query}: See \texttt{data/02\_gdelt\_extraction.py} for SQL extraction code
    \end{itemize}

\textbf{3. Geographic Boundaries}

    \begin{itemize}
    \item \textbf{Source}: GADM (Global Administrative Areas) version 4.1
    \item \textbf{URL}: \url{https://gadm.org/download_country.html}
    \item \textbf{Access}: Publicly available, free for academic use
    \item \textbf{License}: Free for non-commercial use
    \item \textbf{Format}: Shapefile, GeoJSON
    \item \textbf{Resolution}: Admin level 2 (districts)
    \end{itemize}

\subsection{Processed Datasets}

Processed datasets (feature-engineered) are available on Zenodo:

    \begin{center}
\textbf{Zenodo DOI}: \texttt{10.5281/zenodo.[XXXXXX]} \\
\textbf{URL}: \url{https://zenodo.org/record/[XXXXXX]}
    \end{center}

\textbf{Files available}:
    \begin{itemize}
    \item \texttt{ipc\_cleaned\_2021\_2024.csv} (2.3 MB) - Cleaned IPC data
    \item \texttt{gdelt\_features\_ratio\_z-score.parquet} (87 MB) - Ratio and z-score features
    \item \texttt{gdelt\_features\_hmm.parquet} (12 MB) - HMM features
    \item \texttt{gdelt\_features\_dmd.parquet} (16 MB) - DMD features
    \item \texttt{combined\_advanced\_features\_h8.parquet} (124 MB) - All features combined (h=8 months)
    \item \texttt{ar\_baseline\_predictions\_h8.csv} (3.1 MB) - AR baseline predictions
    \item \texttt{key\_saves\_cascade.csv} (0.4 MB) - 249 key save cases
    \item \texttt{geographic\_boundaries.zip} (45 MB) - District shapefiles
    \end{itemize}

\textbf{License}: Creative Commons Attribution 4.0 International (CC BY 4.0)

\textbf{Citation}:
    \begin{verbatim}
[Author Name]. (2026). Food Security Early Warning System:
Feature-Engineered GDELT Dataset (2021-2024) [Data set].
Zenodo. https://doi.org/10.5281/zenodo.[XXXXXX]
    \end{verbatim}

\section{Trained Models}

\subsection{Model Artifacts}

All trained models are available on Hugging Face Model Hub:

    \begin{center}
\textbf{Hugging Face Repository}: \texttt{[USERNAME]/food-security-ews-models} \\
\textbf{URL}: \url{https://huggingface.co/[USERNAME]/food-security-ews-models}
    \end{center}

\textbf{Available models}:
    \begin{itemize}
    \item \texttt{ar\_baseline\_h8.pkl} (1.2 KB) - AR baseline logistic regression
    \item \texttt{xgboost\_advanced\_fold\_[0-4].pkl} (5 $\times$ 730 KB) - XGBoost Advanced (5 folds)
    \item \texttt{xgboost\_basic\_fold\_[0-4].pkl} (5 $\times$ 680 KB) - XGBoost Basic (5 folds)
    \item \texttt{ablation\_ratio\_location\_fold\_[0-4].pkl} (5 $\times$ 650 KB) - Best ablation model
    \item \texttt{mixed\_effects\_pooled\_ratio\_hmm\_dmd.pkl} (45 KB) - Mixed-effects model
    \item \texttt{cascade\_optimised\_production.pkl} (4.2 MB) - Full cascade framework
    \end{itemize}

\textbf{Model metadata} (in repository \texttt{README.md}):
    \begin{itemize}
    \item Hyperparameters (JSON format)
    \item Training data specifications
    \item Cross-validation fold assignments
    \item Performance metrics (AUC, precision, recall, F1)
    \item Feature importance rankings
    \end{itemize}

\subsection{Model Loading Example}

    \begin{verbatim}
import pickle
import pandas as pd

# Load trained XGBoost model
with open('xgboost_advanced_fold_0.pkl', 'rb') as f:
    model = pickle.load(f)

# Load test data
X_test = pd.read_parquet('combined_advanced_features_h8.parquet')

# Make predictions
y_pred_proba = model.predict_proba(X_test)[:, 1]
y_pred = (y_pred_proba >= 0.162).astype(int)  # Youden threshold
    \end{verbatim}

\section{Computational Resources}

\subsection{Hardware Specifications}

All experiments were conducted on a single workstation:

\textbf{CPU}: Intel Xeon E5-2680 v4 @ 2.40GHz (14 cores, 28 threads)

\textbf{RAM}: 64 GB DDR4 ECC

\textbf{Storage}: 2 TB NVMe SSD

\textbf{GPU}: NVIDIA Tesla V100 16GB (not used; all models CPU-based)

\textbf{Operating System}: Ubuntu 22.04 LTS

\subsection{Training Time}

    \begin{itemize}
    \item \textbf{AR Baseline}: 12 seconds (single model, 2 features, 20,722 observations)
    \item \textbf{XGBoost Advanced}: 97 minutes (3,888 configs $\times$ 5 folds, grid search)
    \item \textbf{XGBoost Basic}: 90 minutes (3,888 configs $\times$ 5 folds, grid search)
    \item \textbf{Ablation study}: 8 $\times$ 85-103 minutes = 12.3 hours (8 models, parallel execution)
    \item \textbf{Mixed-effects}: 11 minutes (56 configs, sequential execution)
    \item \textbf{Cascade framework}: 5 minutes (combining AR + Stage 2 predictions)
    \item \textbf{SHAP analysis}: 2.3 hours (TreeExplainer on 6,553 observations)
    \item \textbf{Total compute time}: ~18 hours
    \end{itemize}

\textbf{Cost estimate} (AWS p3.2xlarge equivalent):
    \begin{itemize}
    \item 18 hours $\times$ \$3.06/hour = \$55.08 USD (spot pricing)
    \item Academic pricing: \$0 (local workstation)
    \end{itemize}

\subsection{Memory Requirements}

    \begin{itemize}
    \item \textbf{Peak RAM usage}: 32 GB (during SHAP TreeExplainer computation)
    \item \textbf{Average RAM usage}: 8-12 GB (during model training)
    \item \textbf{Disk space}: 250 GB total
    \begin{itemize}
        \item Raw GDELT data: 87 GB (parquet format)
        \item Processed features: 124 GB (parquet format)
        \item Trained models: 33 MB (all folds, all models)
        \item Results/predictions: 15 GB (CSV format)
        \item Figures: 450 MB (PNG format, 300 DPI)
    \end{itemize}
    \end{itemize}

\section{Reproducibility}

\subsection{Random Seeds}

All stochastic processes use fixed random seeds for reproducibility:

    \begin{itemize}
    \item \textbf{NumPy}: \texttt{np.random.seed(42)}
    \item \textbf{Scikit-learn}: \texttt{random\_state=42} (all estimators)
    \item \textbf{XGBoost}: \texttt{random\_state=42} (all models)
    \item \textbf{HMM/DMD}: \texttt{random\_state=42} (initialization)
    \item \textbf{Cross-validation}: \texttt{StratifiedGroupKFold} with \texttt{n\_splits=5}, \texttt{shuffle=True}, \texttt{random\_state=42}
    \end{itemize}

\subsection{Verification}

To verify reproducibility, run:

    \begin{verbatim}
python tests/test_reproducibility.py
    \end{verbatim}

This script:
    \begin{enumerate}
    \item Loads processed data
    \item Trains AR baseline and XGBoost Advanced models
    \item Compares predictions to saved reference predictions
    \item Asserts exact match (tolerance: $10^{-12}$)
    \end{enumerate}

\textbf{Expected output}:
    \begin{verbatim}
[PASS] AR baseline predictions match reference (0 differences)
[PASS] XGBoost predictions match reference (0 differences)
[PASS] Cascade predictions match reference (0 differences)
All reproducibility tests passed.
    \end{verbatim}

\section{Ethical Considerations and Data Privacy}

\subsection{Data Ethics}

\textbf{IPC Data}:
    \begin{itemize}
    \item Aggregated district-level statistics (no individual-level data)
    \item Publicly available, no privacy concerns
    \item Used in accordance with IPC Global Platform terms of use
    \end{itemize}

\textbf{GDELT News Data}:
    \begin{itemize}
    \item Publicly published news articles (no private communications)
    \item No personally identifiable information (PII) extracted
    \item News coverage analysed at aggregate level (district-month)
    \item Complies with GDPR, CCPA, and other data protection regulations
    \end{itemize}

\subsection{Responsible AI Deployment}

\textbf{Model limitations acknowledged}:
    \begin{itemize}
    \item High false positive rate (cascade precision 58.5\%)
    \item Geographic heterogeneity (performance varies 10$\times$ across countries)
    \item Autocorrelation trap (AR baseline outperforms news models)
    \item Selective deployment recommended (not universal)
    \end{itemize}

\textbf{Deployment safeguards}:
    \begin{itemize}
    \item Models intended as decision-support tools (not fully automated)
    \item Human-in-the-loop validation required
    \item Explainability provided via SHAP, feature importance
    \item Country-specific calibration necessary before operational use
    \end{itemize}

\section{License and Citation}

\subsection{License}

All code and data are released under:

\textbf{Code}: MIT License (permissive, allows commercial use)

\textbf{Data}: Creative Commons Attribution 4.0 International (CC BY 4.0)

\subsection{Citation}

If you use this code or data, please cite:

    \begin{verbatim}
@phdthesis{[AuthorLastName]2026,
  author  = {[Author Full Name]},
  title   = {Dynamic News Signals as Early-Warning Indicators of
             Food Insecurity: A Two-Stage Residual Modelling Framework},
  school  = {[Your University]},
  year    = {2026},
  type    = {PhD Dissertation},
  url     = {https://github.com/[USERNAME]/food-security-ews},
  doi     = {10.5281/zenodo.[XXXXXX]}
}
    \end{verbatim}

\subsection{Contact}

For questions, issues, or collaboration inquiries:

\textbf{Email}: \texttt{[your.email@university.edu]}

\textbf{GitHub Issues}: \url{https://github.com/[USERNAME]/food-security-ews/issues}

\textbf{ORCID}: \texttt{0000-0000-0000-0000}




