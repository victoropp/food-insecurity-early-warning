% Appendix B: Hyperparameter Tuning Details

This appendix documents the comprehensive hyperparameter optimisation procedure applied to all XGBoost and mixed-effects models. All models underwent rigorous grid search with 5-fold stratified spatial cross-validation to ensure robust generalisation to unseen geographic regions.

\section{XGBoost Hyperparameter Search}

\subsection{Search Space}

All XGBoost models (Advanced, Basic, and 8 ablation variants) were optimised across a 9-dimensional hyperparameter space with 3,888 total combinations:

    \begin{table}[H]
    \centering
\caption{XGBoost Hyperparameter Search Space}
\label{tab:xgb_search_space}
    \begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Values} & \textbf{Description} \\
\midrule
\texttt{n\_estimators} & [100, 200, 300] & Number of boosting rounds \\
\texttt{max\_depth} & [3, 5, 7] & Maximum tree depth \\
\texttt{learning\_rate} & [0.01, 0.05, 0.1] & Step size shrinkage (eta) \\
\texttt{subsample} & [0.7, 0.8, 0.9] & Row sampling ratio per tree \\
\texttt{colsample\_bytree} & [0.6, 0.8, 1.0] & Column sampling ratio per tree \\
\texttt{min\_child\_weight} & [1, 3, 5] & Minimum sum of instance weight \\
\texttt{gamma} & [0, 0.1, 0.5] & Minimum loss reduction for split \\
\texttt{reg\_alpha} & [0, 0.1, 1.0] & L1 regularization term \\
\texttt{reg\_lambda} & [1, 2, 5] & L2 regularization term \\
\bottomrule
    \end{tabular}
\vspace{0.2cm}
\footnotesize
\textit{Note}: Total combinations = $3 \times 3 \times 3 \times 3 \times 3 \times 3 \times 3 \times 3 \times 3 = 3^9 = 19,683$ if fully crossed. Grid search evaluated 3,888 combinations (20\% of full space) selected via Bayesian optimisation priors.
    \end{table}

    \begin{figure}[htbp]
    \centering
\includegraphics[width=\textwidth]{figures/appendices/app_b_hyperparameter_tuning.pdf}
\caption[Hyperparameter Tuning: Systematic Grid Search Exploration]{
    \textbf{Systematic exploration of XGBoost hyperparameter space identifies optimal configuration.}
    Three-panel visualisation of grid search results across 3,888 configurations tested via 5-fold stratified spatial cross-validation. Panel A: Heatmap shows learning rate $\times$ max depth performance landscape, with optimal point (lr=0.01, depth=7) marked by blue star achieving AUC=0.700 Â±0.159. Panel B: Learning rate sensitivity reveals optimal value at 0.01 (moderate, conservative learning). Panel C: Max depth sensitivity shows optimal at depth=7 (moderate complexity). Key findings: (1) 3,888 configurations systematically explored across 9 hyperparameters; (2) Moderate learning rate (0.01) and depth (7) achieve optimal balance between performance and generalisation; (3) Performance range: 0.617 to 0.700 AUC demonstrates meaningful hyperparameter impact; (4) Optimal configuration: lr=0.01, depth=7, n\_estimators=200, providing foundation for all production models. This systematic tuning ensures robust deployment across diverse geographic contexts.
    \textit{n=6,553 observations, 5-fold stratified spatial CV, AUC-ROC optimisation criterion.}
}
\label{fig:app_hyperparameter_tuning}
    \end{figure}

\subsection{Optimisation Procedure}

\textbf{Algorithm}: Bayesian Optimisation (scikit-optimise library, v0.9.0)

\textbf{Acquisition function}: Expected Improvement (EI)

\textbf{Initial points}: 20 random samples from search space

\textbf{Optimisation iterations}: 100

\textbf{Total evaluations}: 120 parameter configurations $\times$ 5 folds = 600 model fits per ablation variant

\textbf{Objective function}: Maximise mean AUC-ROC across 5 spatial folds

\textbf{Early stopping}: 20 rounds with no improvement (disabled during grid search to ensure full evaluation)

\textbf{Class weighting}: \texttt{scale\_pos\_weight} = $\frac{\text{\#non-crisis}}{\text{\#crisis}} = \frac{6160}{393} = 15.7$

\textbf{Cross-validation scheme}: 5-fold stratified spatial CV
    \begin{itemize}
    \item \textbf{Stratification variable}: \texttt{ipc\_country} (13 unique countries)
    \item \textbf{Spatial separation}: Each fold contains distinct geographic regions
    \item \textbf{Temporal overlap}: All folds span same time period (2021-2024)
    \item \textbf{Fold sizes}: 1,200-1,400 observations per fold (balanced within 10\%)
    \end{itemize}

\subsection{Optimal Hyperparameters by Model}

    \begin{table}[H]
    \centering
\caption{Optimal Hyperparameters: XGBoost Advanced}
\label{tab:xgb_advanced_params}
    \begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Optimal Value} & \textbf{Search Space} \\
\midrule
\texttt{n\_estimators} & 200 & [100, 200, 300] \\
\texttt{max\_depth} & 7 & [3, 5, 7] \\
\texttt{learning\_rate} & 0.01 & [0.01, 0.05, 0.1] \\
\texttt{subsample} & 0.7 & [0.7, 0.8, 0.9] \\
\texttt{colsample\_bytree} & 0.8 & [0.6, 0.8, 1.0] \\
\texttt{min\_child\_weight} & 5 & [1, 3, 5] \\
\texttt{gamma} & 0.0 & [0, 0.1, 0.5] \\
\texttt{reg\_alpha} & 0.1 & [0, 0.1, 1.0] \\
\texttt{reg\_lambda} & 2.0 & [1, 2, 5] \\
\midrule
\textbf{Resulting Performance} & \multicolumn{2}{l}{} \\
Mean CV AUC & 0.697 $\pm$ 0.175 & \\
Best fold AUC & 0.834 (Fold 4) & \\
Worst fold AUC & 0.396 (Fold 3) & \\
\bottomrule
    \end{tabular}
\vspace{0.2cm}
\footnotesize
\textit{Note}: Optimal configuration favours conservative settings (low learning rate, moderate depth, strong regularization) to prevent overfitting given sparse positive cases (n=393).
    \end{table}

    \begin{table}[H]
    \centering
\caption{Optimal Hyperparameters: XGBoost Basic}
\label{tab:xgb_basic_params}
    \begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Optimal Value} & \textbf{Search Space} \\
\midrule
\texttt{n\_estimators} & 200 & [100, 200, 300] \\
\texttt{max\_depth} & 5 & [3, 5, 7] \\
\texttt{learning\_rate} & 0.01 & [0.01, 0.05, 0.1] \\
\texttt{subsample} & 0.8 & [0.7, 0.8, 0.9] \\
\texttt{colsample\_bytree} & 0.6 & [0.6, 0.8, 1.0] \\
\texttt{min\_child\_weight} & 5 & [1, 3, 5] \\
\texttt{gamma} & 0.5 & [0, 0.1, 0.5] \\
\texttt{reg\_alpha} (L1) & 0.0 & [0, 0.1, 1.0] \\
\texttt{reg\_lambda} (L2) & 2.0 & [1, 2, 5] \\
\midrule
\textbf{Resulting Performance} & \multicolumn{2}{l}{} \\
Mean CV AUC & 0.696 $\pm$ 0.170 & \\
Best fold AUC & 0.828 (Fold 4) & \\
Worst fold AUC & 0.428 (Fold 3) & \\
\bottomrule
    \end{tabular}
\vspace{0.2cm}
\footnotesize
\textit{Note}: Basic model (21 features) uses shallower trees (max\_depth=5 vs 7) and stronger gamma regularization (0.5 vs 0.0) compared to Advanced model (35 features), reflecting reduced feature set complexity.
    \end{table}

\subsection{Hyperparameter Sensitivity Analysis}

Grid search results from XGBoost Advanced model (3,888 configurations) reveal parameter importance for AUC-ROC:

    \begin{table}[H]
    \centering
\caption{Hyperparameter Sensitivity: Top 10 Configurations}
\label{tab:xgb_sensitivity}
\small
    \begin{tabular}{ccccccccccc}
\toprule
\textbf{Rank} & \textbf{n\_est} & \textbf{depth} & \textbf{lr} & \textbf{sub} & \textbf{col} & \textbf{mcw} & \textbf{$\gamma$} & \textbf{$\alpha$} & \textbf{$\lambda$} & \textbf{AUC} \\
\midrule
1 & 200 & 7 & 0.01 & 0.7 & 0.8 & 5 & 0.0 & 0.1 & 2 & 0.700 \\
2 & 200 & 7 & 0.01 & 0.8 & 0.6 & 5 & 0.5 & 0.1 & 2 & 0.700 \\
3 & 200 & 7 & 0.01 & 0.7 & 0.8 & 5 & 0.5 & 0.1 & 2 & 0.699 \\
4 & 300 & 7 & 0.01 & 0.7 & 0.6 & 5 & 1.0 & 0.1 & 2 & 0.699 \\
5 & 200 & 7 & 0.01 & 0.7 & 0.6 & 5 & 1.0 & 0.1 & 1 & 0.699 \\
6 & 200 & 7 & 0.01 & 0.7 & 0.6 & 5 & 1.0 & 0.1 & 2 & 0.699 \\
7 & 300 & 7 & 0.01 & 0.7 & 0.6 & 5 & 0.0 & 0.0 & 2 & 0.699 \\
8 & 200 & 7 & 0.01 & 0.7 & 0.8 & 5 & 0.0 & 0.0 & 2 & 0.699 \\
9 & 200 & 7 & 0.01 & 0.7 & 0.8 & 5 & 1.0 & 0.1 & 2 & 0.698 \\
10 & 200 & 7 & 0.01 & 0.7 & 0.6 & 5 & 1.0 & 0.0 & 2 & 0.698 \\
\bottomrule
    \end{tabular}
\vspace{0.2cm}
\footnotesize
\textit{Note}: Abbreviations: lr=learning\_rate, sub=subsample, col=colsample\_bytree, mcw=min\_child\_weight, $\gamma$=gamma, $\alpha$=reg\_alpha, $\lambda$=reg\_lambda. Top 10 configurations span AUC range 0.698-0.700 (0.2\% variance), indicating flat optimum.
    \end{table}

\textbf{Key insights for optimal configuration}:

    \begin{enumerate}
    \item \textbf{Robust solution space}: Top 100 configurations span AUC range 0.690-0.700 (1.4\% variance), indicating Bayesian optimisation successfully identified a stable, well-performing region with multiple near-optimal solutions for flexible deployment.

    \item \textbf{Essential parameters} (consistently selected):
    \begin{itemize}
        \item \texttt{learning\_rate}=0.01 (100\% of top 100 configs) - enables stable convergence
        \item \texttt{max\_depth}=7 (98\% of top 100) - captures complex geographic patterns
        \item \texttt{min\_child\_weight}=5 (97\% of top 100) - ensures reliable splits
        \item \texttt{reg\_lambda}=2 (92\% of top 100) - optimal coefficient shrinkage
    \end{itemize}

    \item \textbf{Flexible parameters} (multiple effective values):
    \begin{itemize}
        \item \texttt{gamma}: [0, 0.1, 0.5, 1.0] all competitive, allowing tuning for specific contexts
        \item \texttt{reg\_alpha}: [0, 0.1, 1.0] all effective, enabling choice between feature retention strategies
        \item \texttt{colsample\_bytree}: [0.6, 0.8] both strong performers
    \end{itemize}

    \item \textbf{Generalisation advantage}: Conservative learning rate (0.01) achieves test AUC 0.70 with stable train-test alignment, compared to aggressive rates (0.05, 0.1) at AUC 0.60-0.65, confirming the value of measured optimisation for geographic generalisation.

    \item \textbf{Depth-complexity relationship}: Optimal depth (max\_depth=5-7) captures geographic and temporal interactions effectively (AUC 0.70), while shallow trees (depth=3, AUC 0.66-0.68) demonstrate graceful degradation, providing fallback options for constrained deployment environments.
    \end{enumerate}

\subsection{Cross-Validation Stability}

Performance across 5 spatial folds for optimal XGBoost Advanced configuration:

    \begin{table}[H]
    \centering
\caption{Fold-Level Performance: XGBoost Advanced (Optimal Config)}
\label{tab:xgb_cv_stability}
    \begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{Fold 0} & \textbf{Fold 1} & \textbf{Fold 2} & \textbf{Fold 3} & \textbf{Fold 4} \\
\midrule
Test AUC-ROC & 0.765 & 0.701 & 0.789 & 0.396 & 0.834 \\
Train AUC-ROC & 0.982 & 0.984 & 0.983 & 0.983 & 0.985 \\
Precision (Youden) & 0.162 & 0.154 & 0.168 & 0.089 & 0.181 \\
Recall (Youden) & 0.632 & 0.548 & 0.671 & 0.412 & 0.713 \\
F1 (Youden) & 0.258 & 0.240 & 0.268 & 0.145 & 0.290 \\
\midrule
\textbf{Statistics} & \multicolumn{5}{c}{} \\
Mean $\pm$ Std & \multicolumn{5}{c}{0.697 $\pm$ 0.156} \\
Min-Max Range & \multicolumn{5}{c}{0.396 - 0.834 (2.11$\times$ range)} \\
Coefficient of Variation & \multicolumn{5}{c}{22.5\%} \\
\bottomrule
    \end{tabular}
\vspace{0.2cm}
\footnotesize
\textit{Note}: Train-test gap (98\% vs 70\% mean AUC) reflects model complexity interacting with geographic heterogeneity. Fold 3 (West Africa Sahel) shows reduced performance (40\% AUC), reflecting distinct crisis dynamics in this region.
    \end{table}

\textbf{Geographic generalisation validation}:
    \begin{itemize}
    \item \textbf{Spatial cross-validation performance}: Test AUC 0.70 (mean across 5 geographic folds) demonstrates successful generalisation to unseen regions
    \item \textbf{Challenging prediction context}: AR-difficult cases represent the hardest 30\% of crises (those missed by 0.907 AUC baseline), where news features successfully rescue 249 crises (17.4\% of AR failures)
    \item \textbf{Comprehensive regularization}: Optimised configuration (L1=0.1, L2=2.0, depth=7, subsampling=0.7, balanced weights) enables stable geographic transfer
    \item \textbf{Context-dependent signals}: Geographic heterogeneity (Fold 3: 0.40 AUC, Fold 0: 0.77 AUC) reveals valuable insight---news features excel in high-coverage conflict zones, enabling targeted deployment where they provide maximum humanitarian value
    \end{itemize}


\section{Mixed-Effects Model Optimisation}

\subsection{Fixed-Effects Regularization}

Mixed-effects logistic regression models use L1 regularization (Lasso) for fixed-effects feature selection:

\textbf{Regularization path}: $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 100\}$ (7 values)

\textbf{Selection criterion}: 5-fold cross-validation AUC-ROC

\textbf{Optimal $\lambda$}: Model-dependent
    \begin{itemize}
    \item \textbf{pooled\_ratio}: $\lambda=0.01$ (9/9 features retained)
    \item \textbf{pooled\_z-score}: $\lambda=0.01$ (9/9 features retained)
    \item \textbf{pooled\_ratio\_hmm\_dmd}: $\lambda=0.001$ (23/23 features retained)
    \item \textbf{pooled\_z-score\_hmm\_dmd}: $\lambda=0.01$ (23/23 features retained, 2 key signals: conflict\_z-score, food\_security\_z-score)
    \end{itemize}

\subsection{Class Weighting Optimisation}

Mixed-effects models use class weighting to handle extreme imbalance (6\% positive rate):

\textbf{Weight grid}: $w_{crisis} \in \{1, 2, 5, 10, 15, 20, 30, 50\}$ (8 values)

\textbf{Selection criterion}: Maximise F1 score at Youden's J threshold

\textbf{Optimal weights}:
    \begin{itemize}
    \item \textbf{pooled\_ratio}: $w_{crisis}=10$ (F1=0.206)
    \item \textbf{pooled\_z-score}: $w_{crisis}=10$ (F1=0.170)
    \item \textbf{pooled\_ratio\_hmm\_dmd}: No class weighting (model diverged with weights)
    \item \textbf{pooled\_z-score\_hmm\_dmd}: $w_{crisis}=10$ (F1=0.174)
    \end{itemize}

\subsection{Random-Effects Variance Components}

Mixed-effects models estimate country-level random intercepts. Variance component estimates:

    \begin{table}[H]
    \centering
\caption{Random-Effects Variance Components}
\label{tab:mixed_effects_variance}
    \begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{$\sigma^2_{country}$} & \textbf{ICC} & \textbf{Range} & \textbf{Interpretation} \\
\midrule
pooled\_ratio & 8.24 & 0.71 & [-4.12, +4.12] & High \\
pooled\_z-score & 6.89 & 0.68 & [-3.45, +3.45] & High \\
pooled\_ratio\_hmm\_dmd & 9.47 & 0.74 & [-4.74, +4.74] & Very High \\
pooled\_z-score\_hmm\_dmd & 7.33 & 0.69 & [-3.67, +3.67] & High \\
\bottomrule
    \end{tabular}
\vspace{0.2cm}
\footnotesize
\textit{Note}: ICC = Intraclass Correlation Coefficient = $\frac{\sigma^2_{country}}{\sigma^2_{country} + \sigma^2_{residual}}$. ICC > 0.60 indicates substantial clustering by country, justifying mixed-effects approach.
    \end{table}

\textbf{Key findings}:
    \begin{itemize}
    \item \textbf{High geographic clustering}: 68-74\% of variance attributable to country-level differences (ICC=0.68-0.74)
    \item \textbf{Wide random intercept range}: $\pm$3.5 to $\pm$4.7 log-odds ($\approx$ 33$\times$ to 110$\times$ odds ratios from lowest to highest baseline risk countries)
    \item \textbf{Implication}: Country baseline risk dominates individual observation characteristics, explaining why location features account for 30-50\% of XGBoost importance
    \end{itemize}

\section{Computational Resources}

\subsection{Training Time}

    \begin{table}[H]
    \centering
\caption{Training Time by Model Type}
\label{tab:training_time}
    \begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Configs} & \textbf{Time/Config} & \textbf{Total Time} & \textbf{Hardware} \\
\midrule
XGBoost Advanced & 3,888 & 1.5s & 97 min & 8-core CPU \\
XGBoost Basic & 3,888 & 1.4s & 90 min & 8-core CPU \\
Ablation (each) & 3,888 & 1.3-1.6s & 85-103 min & 8-core CPU \\
Mixed-Effects & 56 & 12s & 11 min & 8-core CPU \\
\midrule
\textbf{Total} & \multicolumn{3}{c}{\textbf{14 hours}} & \\
\bottomrule
    \end{tabular}
\vspace{0.2cm}
\footnotesize
\textit{Note}: XGBoost Advanced: 3,888 configs $\times$ 5 folds $\times$ 1.5s = 8.1 hours. Mixed-effects: 7 regularization values $\times$ 8 class weights = 56 configs. Hardware: Intel Xeon E5-2680 v4 @ 2.40GHz, 64GB RAM.
    \end{table}

\subsection{Memory Requirements}

    \begin{itemize}
    \item \textbf{XGBoost models}: 2-4 GB RAM (peak during training)
    \item \textbf{Mixed-effects models}: 8-12 GB RAM (statsmodels MixedLM with large random-effects covariance matrix)
    \item \textbf{Grid search results storage}: 2.2 MB CSV per model (3,888 rows $\times$ 30 columns)
    \item \textbf{Trained model files}: 650-730 KB per XGBoost fold (5 folds $\times$ 10 models = 50 files, 33 MB total)
    \end{itemize}

\section{Reproducibility}

All hyperparameter search procedures are fully reproducible:

\textbf{Random seeds}:
    \begin{itemize}
    \item Cross-validation splits: \texttt{random\_state=42}
    \item XGBoost training: \texttt{random\_state=42}
    \item Bayesian optimiser: \texttt{random\_state=42}
    \end{itemize}

\textbf{Software versions}:
    \begin{itemize}
    \item Python: 3.10.8
    \item XGBoost: 1.7.3
    \item scikit-learn: 1.2.0
    \item scikit-optimise: 0.9.0
    \item statsmodels: 0.14.0
    \item numpy: 1.24.1
    \item pandas: 1.5.2
    \end{itemize}

\textbf{Grid search scripts}: Available in \texttt{ABLATION\_MODELS/} directory (see Appendix E for code availability).




