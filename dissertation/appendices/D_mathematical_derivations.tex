% Appendix D: Mathematical Derivations

This appendix provides detailed mathematical formulations for all models and feature engineering procedures used in this dissertation.

    \begin{figure}[htbp]
    \centering
\includegraphics[width=\textwidth]{figures/appendices/app_d_data_quality.pdf}
\caption[Data Quality Assessment: Coverage and Temporal Distribution]{
    \textbf{Comprehensive data quality validation confirms robust geographic and temporal coverage.}
    Four-panel diagnostic assessment of 20,722 observations spanning June 2021 to February 2024. Panel A: District coverage by country shows 1,920 unique districts across 18 countries, with Kenya (450), Ethiopia (320), and Nigeria (289) providing densest coverage (highlighted in orange). Panel B: Temporal coverage demonstrates consistent observation density across 9 IPC periods with no major gaps, validating longitudinal modelling approach. Panel C: Crisis rate distribution reveals heterogeneity across contexts (range: 5\% to 45\%, mean: 25.7\%, std: 12.3\%), justifying stratified spatial cross-validation. Panel D: Article density map shows geographic distribution of news coverage, with higher density in conflict zones (Sudan, DRC) and economic crisis regions (Zimbabwe). Data quality validation confirms: (1) Sufficient geographic stratification for spatial CV; (2) Consistent temporal coverage enabling time series analysis; (3) Crisis rate heterogeneity validating mixed-effects modelling; (4) News coverage aligns with crisis contexts.
    \textit{n=20,722 observations, 18 countries, 1,920 districts, Jun 2021$\times$Feb 2024.}
}
\label{fig:app_data_quality}
    \end{figure}

\section{Autoregressive Baseline Model}

\subsection{Logistic Regression Formulation}

The AR baseline predicts crisis probability using only autoregressive features (no external covariates):

    \begin{equation}
P(y_it = 1 | \mathbf{L}_t, \mathbf{L}_s) = \frac{1}{1 + \exp(-\eta_{it})}
    \end{equation}

where the linear predictor $\eta_{it}$ is:

    \begin{equation}
\eta_{it} = \beta_0 + \sum_{k=1}^{12} \beta_k^{(t)} L_{t,i,k} + \beta_s L_{s,it}
    \end{equation}

\textbf{Temporal autoregressive features} ($\mathbf{L}_t$):
    \begin{equation}
L_{t,i,k} = \text{IPC}_{i,t-k}, \quad k \in \{1, 2, ..., 12\}
    \end{equation}

Historical IPC values at 12 preceding time points (typically 3 years of quarterly assessments).

\textbf{Spatial autoregressive feature} ($L_s$):
    \begin{equation}
L_{s,it} = \frac{\sum_{j \in N_i} w_{ij} \cdot \text{IPC}_{jt}}{\sum_{j \in N_i} w_{ij}}, \quad w_{ij} = \frac{1}{d_{ij}^2}
    \end{equation}

where:
    \begin{itemize}
    \item $N_i$ = set of districts within 300km of district $i$
    \item $d_{ij}$ = Euclidean distance (km) between district centroids $i$ and $j$
    \item $\text{IPC}_{jt}$ = IPC value of neighbour $j$ at time $t$
    \end{itemize}

For districts with no neighbours within 300km (0.5\% of observations), $L_{s,it} = 0$.

\subsection{Regularization and Class Weighting}

The AR baseline uses L2-regularized logistic regression with balanced class weights:

\textbf{Objective function}:
    \begin{equation}
\min_{\boldsymbol{\beta}} \quad -\frac{1}{n} \sum_{i=1}^{n} w_i \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right] + \lambda \|\boldsymbol{\beta}\|_2^2
    \end{equation}

\textbf{Class weights}:
    \begin{equation}
w_i = \begin{cases}
\frac{n}{2 \cdot n_{\text{crisis}}} & \text{if } y_i = 1 \text{ (crisis)} \\
\frac{n}{2 \cdot n_{\text{non-crisis}}} & \text{if } y_i = 0 \text{ (non-crisis)}
    \end{cases}
    \end{equation}

For this dataset: $n=20,722$, $n_{\text{crisis}}=5,322$, $n_{\text{non-crisis}}=15,400$, yielding:
    \begin{align}
w_{\text{crisis}} &= \frac{20,722}{2 \cdot 5,322} = 1.947 \\
w_{\text{non-crisis}} &= \frac{20,722}{2 \cdot 15,400} = 0.673
    \end{align}

Crisis observations weighted 2.89$\times$ higher than non-crisis observations.

\textbf{Regularization strength}: $\lambda = 1.0$ (selected via 5-fold cross-validation from $\{0.01, 0.1, 1.0, 10.0\}$)

\section{Dynamic Feature Engineering}

\subsection{Ratio Features (Compositional Transformation)}

Ratio features capture the relative emphasis on each news category within a district-month:

    \begin{equation}
\text{ratio}_{c,it} = \frac{n_{c,it}}{\sum_{c'=1}^{9} n_{c',it}}
    \end{equation}

where:
    \begin{itemize}
    \item $n_{c,it}$ = count of GDELT articles in category $c$ for district $i$ in month $t$
    \item $c \in \{$conflict, displacement, economic, food\_security, governance, health, humanitarian, other, weather$\}$
    \item $\sum_{c=1}^{9} \text{ratio}_{c,it} = 1$ (simplex constraint)
    \end{itemize}

\textbf{Example}: If district $i$ in month $t$ has 50 conflict articles, 30 displacement articles, 20 other articles (100 total):
    \begin{align}
\text{conflict\_ratio}_{it} &= \frac{50}{100} = 0.50 \\
\text{displacement\_ratio}_{it} &= \frac{30}{100} = 0.30 \\
\text{other\_ratio}_{it} &= \frac{20}{100} = 0.20
    \end{align}

\subsection{Z-Score Features (Temporal Anomaly Transformation)}

Z-score features capture deviations from historical mean news coverage using 12-month rolling windows. For category $c$ in district $i$ at time $t$, the z-score transformation is:
    \begin{equation}
\text{z-score}_{c,it} = \frac{n_{c,it} - \mu}{\sigma}
    \end{equation}

The mean $\mu$ and standard deviation $\sigma$ are computed from the 12-month trailing window (months $t-12$ through $t-1$). Specifically:
\[
\mu = \frac{1}{12} \sum_{k=1}^{12} n_{c,i,t-k}
\]
\[
\sigma = \sqrt{\frac{1}{12} \sum_{k=1}^{12} (n_{c,i,t-k} - \mu)^2}
\]

\textbf{Example}: If district $i$ had conflict counts $\{10, 12, 11, 9, 13, 10, 12, 11, 10, 9, 11, 12\}$ over past 12 months, and current month has 25 articles:
    \begin{align}
\mu_{\text{conflict}} &= \frac{10+12+11+9+13+10+12+11+10+9+11+12}{12} = 10.83 \\
\sigma_{\text{conflict}} &= \sqrt{\frac{(10-10.83)^2 + ... + (12-10.83)^2}{12}} = 1.19 \\
\text{conflict\_z-score} &= \frac{25 - 10.83}{1.19} = 11.91 \quad \text{(extreme spike)}
    \end{align}

\textbf{Interpretation}: $\text{z-score} > +2$ indicates unusual spike (>95th percentile), $\text{z-score} < -2$ indicates unusual drop (<5th percentile).

\subsection{Hidden Markov Model (HMM) Features}

HMM features capture latent narrative regimes using Gaussian emissions:

\textbf{Model specification}:
    \begin{align}
\text{Hidden states: } \quad & z_t \in \{1, 2, ..., K\} \quad (K=3 \text{ states}) \\
\text{Transition probabilities: } \quad & P(z_t = j | z_{t-1} = i) = A_{ij} \\
\text{Emission probabilities: } \quad & P(\mathbf{x}_t | z_t = k) = \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
    \end{align}

where $\mathbf{x}_t \in \mathbb{R}^9$ is the 9-dimensional vector of ratio features (or z-score features) at time $t$.

\textbf{Estimated via Expectation-Maximisation (EM)}:
    \begin{enumerate}
    \item \textbf{E-step}: Compute posterior $P(z_t = k | \mathbf{x}_{1:T})$ using forward-backward algorithm
    \item \textbf{M-step}: Update $A, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k$ to maximise expected complete-data log-likelihood
    \item Iterate until convergence ($|\Delta \log L| < 10^{-6}$)
    \end{enumerate}

\textbf{Extracted features} (per district, rolling 12-month window):

    \begin{equation}
\text{hmm\_crisis\_prob}_{it} = P(z_t = k_{\text{crisis}} | \mathbf{x}_{i,t-12:t})
    \end{equation}

Probability of being in high-crisis-risk regime at time $t$.

    \begin{equation}
\text{hmm\_transition\_risk}_{it} = \sum_{j \neq k_{\text{stable}}} P(z_t = k_{\text{stable}} | \mathbf{x}_{i,t-1}) \cdot A_{k_{\text{stable}}, j}
    \end{equation}

Probability of transitioning from stable regime to crisis-prone regime.

    \begin{equation}
\text{hmm\_entropy}_{it} = -\sum_{k=1}^{K} P(z_t = k | \mathbf{x}_{i,t-12:t}) \log P(z_t = k | \mathbf{x}_{i,t-12:t})
    \end{equation}

Regime uncertainty (high entropy = narrative instability).

\subsection{Dynamic Mode Decomposition (DMD) Features}

DMD extracts dominant temporal patterns from multi-category time series:

\textbf{Data matrix construction}:
    \begin{equation}
\mathbf{X} = \begin{bmatrix}
\mathbf{x}_{t-11} & \mathbf{x}_{t-10} & \cdots & \mathbf{x}_{t-1}
    \end{bmatrix} \in \mathbb{R}^{9 \times 11}
    \end{equation}

    \begin{equation}
\mathbf{X'} = \begin{bmatrix}
\mathbf{x}_{t-10} & \mathbf{x}_{t-9} & \cdots & \mathbf{x}_{t}
    \end{bmatrix} \in \mathbb{R}^{9 \times 11}
    \end{equation}

\textbf{DMD algorithm}:
    \begin{enumerate}
    \item \textbf{SVD of $\mathbf{X}$}:
    \begin{equation}
    \mathbf{X} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^T
    \end{equation}

    \item \textbf{Reduced-rank approximation} ($r=3$ modes):
    \begin{equation}
    \mathbf{U}_r = \mathbf{U}[:, 1:r], \quad \boldsymbol{\Sigma}_r = \boldsymbol{\Sigma}[1:r, 1:r], \quad \mathbf{V}_r = \mathbf{V}[:, 1:r]
    \end{equation}

    \item \textbf{Estimate linear operator $\mathbf{A}$}:
    \begin{equation}
    \tilde{\mathbf{A}} = \mathbf{U}_r^T \mathbf{X'} \mathbf{V}_r \boldsymbol{\Sigma}_r^{-1}
    \end{equation}

    \item \textbf{Eigendecomposition}:
    \begin{equation}
    \tilde{\mathbf{A}} \mathbf{w}_j = \lambda_j \mathbf{w}_j
    \end{equation}

    DMD modes: $\boldsymbol{\phi}_j = \mathbf{U}_r \mathbf{w}_j$, eigenvalues: $\lambda_j = \rho_j e^{i\omega_j}$
    \end{enumerate}

\textbf{Extracted features}:

    \begin{equation}
\text{dmd\_crisis\_growth\_rate} = \max_j |\log|\lambda_j||
    \end{equation}

Maximum growth rate across all modes (positive = exponential growth, negative = decay).

    \begin{equation}
\text{dmd\_crisis\_instability} = \sum_{j=1}^{r} |\lambda_j - 1|
    \end{equation}

Deviation from unit circle (stable dynamics have $|\lambda_j| \approx 1$).

    \begin{equation}
\text{dmd\_crisis\_frequency} = \frac{1}{r} \sum_{j=1}^{r} |\omega_j|
    \end{equation}

Average oscillation frequency (high frequency = rapid regime shifts).

    \begin{equation}
\text{dmd\_crisis\_amplitude} = \frac{1}{r} \sum_{j=1}^{r} \|\boldsymbol{\phi}_j\|_2
    \end{equation}

Average mode amplitude (large amplitudes = strong multi-category synchronization).

\section{XGBoost Model}

\subsection{Gradient Boosting Formulation}

XGBoost builds an additive ensemble of decision trees:

    \begin{equation}
\hat{y}_i^{(M)} = \sum_{m=1}^{M} \eta \cdot f_m(\mathbf{x}_i)
    \end{equation}

where $f_m$ is the $m$-th tree, $\eta$ is the learning rate, and $M$ is the number of estimators (typically 200).

\textbf{Objective function at iteration $m$}:
    \begin{equation}
\mathcal{L}^{(m)} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(m-1)} + f_m(\mathbf{x}_i)) + \Omega(f_m)
    \end{equation}

\textbf{Loss function} (binary cross-entropy):
    \begin{equation}
l(y_i, \hat{y}_i) = -w_i \left[ y_i \log(\sigma(\hat{y}_i)) + (1-y_i) \log(1-\sigma(\hat{y}_i)) \right]
    \end{equation}

where $\sigma(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function.

\textbf{Regularization term}:
    \begin{equation}
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2 + \alpha \sum_{j=1}^{T} |w_j|
    \end{equation}

where:
    \begin{itemize}
    \item $T$ = number of leaves in tree $f$
    \item $w_j$ = weight of leaf $j$
    \item $\gamma$ = minimum loss reduction required to make further partition (gamma parameter)
    \item $\lambda$ = L2 regularization term (reg\_lambda parameter)
    \item $\alpha$ = L1 regularization term (reg\_alpha parameter)
    \end{itemize}

\textbf{Second-order Taylor approximation}:
    \begin{equation}
\mathcal{L}^{(m)} \approx \sum_{i=1}^{n} \left[ g_i f_m(\mathbf{x}_i) + \frac{1}{2} h_i f_m^2(\mathbf{x}_i) \right] + \Omega(f_m)
    \end{equation}

where:
    \begin{align}
g_i &= \frac{\partial l(y_i, \hat{y}_i^{(m-1)})}{\partial \hat{y}_i^{(m-1)}} \quad \text{(first derivative)} \\
h_i &= \frac{\partial^2 l(y_i, \hat{y}_i^{(m-1)})}{\partial (\hat{y}_i^{(m-1)})^2} \quad \text{(second derivative)}
    \end{align}

\textbf{Optimal leaf weight}:
    \begin{equation}
w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}
    \end{equation}

where $I_j$ = set of instances in leaf $j$.

\textbf{Gain from split}:
    \begin{equation}
\text{Gain} = \frac{1}{2} \left[ \frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{(\sum_{i \in I} g_i)^2}{\sum_{i \in I} h_i + \lambda} \right] - \gamma
    \end{equation}

Split is made only if Gain $> 0$.

\subsection{Class Weighting for Imbalanced Data}

XGBoost uses \texttt{scale\_pos\_weight} parameter to handle class imbalance:

    \begin{equation}
\text{scale\_pos\_weight} = \frac{\sum_{i=1}^{n} \mathbb{1}(y_i = 0)}{\sum_{i=1}^{n} \mathbb{1}(y_i = 1)} = \frac{n_{\text{non-crisis}}}{n_{\text{crisis}}}
    \end{equation}

For this dataset (WITH\_AR\_FILTER):
    \begin{equation}
\text{scale\_pos\_weight} = \frac{6160}{393} = 15.7
    \end{equation}

This inflates gradients for positive class by 15.7$\times$, ensuring model focuses on minority class.

\section{Mixed-Effects Logistic Regression}

\subsection{Generalised Linear Mixed Model (GLMM) Formulation}

Mixed-effects logistic regression models group-level random effects (adaptive grouping: district or country):

    \begin{equation}
\log \frac{p_{r,t}}{1 - p_{r,t}} = \underbrace{\boldsymbol{\beta}^T \mathbf{X}_{r,t}}_{\text{Fixed effects}} + \underbrace{\alpha_g + \mathbf{b}_g^T \mathbf{Z}_{r,t}}_{\text{Random effects}}
    \end{equation}

where:
    \begin{itemize}
    \item $r$ = region (district) index
    \item $t$ = time index
    \item $g$ = group index (adaptive: district-level if data sufficient, else country-level)
    \item $\mathbf{X}_{r,t}$ = all features (9 ratio + 9 z-score + 6 HMM + 8 DMD + 3 location = 35 features)
    \item $\boldsymbol{\beta}$ = fixed-effects coefficients (global patterns across all groups)
    \item $\alpha_g$ = random intercept for group $g$ (group-specific baseline risk)
    \item $\mathbf{Z}_{r,t}$ = random-slopes covariates (conflict\_ratio, food\_security\_ratio subset)
    \item $\mathbf{b}_g$ = random slopes for group $g$ (group-specific feature sensitivities)
    \end{itemize}

\textbf{Random effects distribution}:
    \begin{equation}
    \begin{bmatrix}
\alpha_g \\
\mathbf{b}_g
    \end{bmatrix} \sim \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma} \right)
    \end{equation}

\textbf{Covariance matrix} $\boldsymbol{\Sigma}$ estimated via REML (Restricted Maximum Likelihood), where:
    \begin{equation}
\boldsymbol{\Sigma} =
    \begin{bmatrix}
\sigma_\alpha^2 & \sigma_{\alpha b_1} & \sigma_{\alpha b_2} \\
\sigma_{\alpha b_1} & \sigma_{b_1}^2 & \sigma_{b_1 b_2} \\
\sigma_{\alpha b_2} & \sigma_{b_1 b_2} & \sigma_{b_2}^2
    \end{bmatrix}
    \end{equation}
with $\sigma_\alpha^2$ = random intercept variance, $\sigma_{b_1}^2, \sigma_{b_2}^2$ = random slope variances for conflict\_ratio and food\_security\_ratio.

\subsection{L1 Regularization for Fixed Effects}

Fixed-effects coefficients $\boldsymbol{\beta}$ selected via Lasso (L1-penalized logistic regression):

    \begin{equation}
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \quad -\log L(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) + \lambda \|\boldsymbol{\beta}\|_1
    \end{equation}

where $L(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})$ is the log-likelihood.

\textbf{Regularization path}: $\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 100\}$

\textbf{Selection}: 5-fold cross-validation, maximise AUC-ROC

\section{Cascade Framework Decision Rule}

\subsection{Two-Stage Prediction}

The cascade framework combines AR baseline and Stage 2 news model:

    \begin{equation}
\hat{y}_{\text{cascade}} = \begin{cases}
1 & \text{if } \hat{p}_{\text{AR}} \geq \tau_{\text{AR}} \\
\hat{y}_{\text{Stage2}} & \text{if } \hat{p}_{\text{AR}} < \tau_{\text{AR}} \text{ and obs is AR failure}\\
0 & \text{otherwise}
    \end{cases}
    \end{equation}

where:
    \begin{itemize}
    \item $\hat{p}_{\text{AR}}$ = AR baseline predicted probability
    \item $\tau_{\text{AR}} = 0.629$ = AR optimal threshold (balanced P=R)
    \item $\hat{y}_{\text{Stage2}}$ = Stage 2 XGBoost prediction for AR failures
    \item AR failure = $\hat{p}_{\text{AR}} < \tau_{\text{AR}}$ AND $y = 1$
    \end{itemize}

\textbf{Operational interpretation}:
    \begin{enumerate}
    \item If AR predicts crisis ($\hat{p}_{\text{AR}} \geq 0.629$), accept AR prediction (no Stage 2 override)
    \item If AR predicts no crisis ($\hat{p}_{\text{AR}} < 0.629$) but observation is an AR failure (actual crisis), query Stage 2 model
    \item If Stage 2 predicts crisis ($\hat{y}_{\text{Stage2}} = 1$), override AR baseline (key save)
    \item Otherwise, accept AR baseline prediction
    \end{enumerate}

\subsection{Precision-Recall Trade-Off}

Cascade framework sacrifices precision for recall:

{\scriptsize\hfuzz=200pt
    \begin{gather}
\Delta \text{Precision} = P_{\text{cascade}} - P_{\text{AR}} = 0.585 - 0.732 = -0.147 \nonumber\\
\Delta \text{Recall} = R_{\text{cascade}} - R_{\text{AR}} = 0.779 - 0.732 = +0.047 \nonumber\\
\text{Trade-off ratio} = \frac{\Delta \text{FP}}{\Delta \text{TP}} = \frac{2939 - 1427}{4144 - 3895} = \frac{1512}{249} = 6.1:1
    \end{gather}
}

Every additional crisis detected costs 6.1 additional false alarms.

\section{Performance Metrics}

\subsection{Ar\-ea Un\-der ROC Curve (AUC-ROC)}

    \begin{equation}
\text{AUC} = \int_{0}^{1} \text{TPR}(\tau) \, d\text{FPR}(\tau)
    \end{equation}

where TPR is sensitivity and FPR is (1 - specificity):
{\small
    \begin{align*}
\text{TPR}(\tau) &= \frac{\text{TP}(\tau)}{\text{TP}(\tau) + \text{FN}(\tau)} \\
\text{FPR}(\tau) &= \frac{\text{FP}(\tau)}{\text{FP}(\tau) + \text{TN}(\tau)}
    \end{align*}
}

\subsection{Youden's J Statistic}

Optimal threshold selection:

    \begin{equation}
\tau^* = \arg\max_{\tau} \quad J(\tau) = \text{TPR}(\tau) + \text{TNR}(\tau) - 1
    \end{equation}

Maximises sum of sensitivity and specificity.

\subsection{Cost-Sensitive Metric}

Humanitarian cost function (false negatives 10$\times$ more costly than false positives):

    \begin{equation}
\text{Cost} = 10 \cdot \text{FN} + 1 \cdot \text{FP}
    \end{equation}

Lower cost = better humanitarian utility.



