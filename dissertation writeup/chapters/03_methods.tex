% Chapter 3: Methodology
% This chapter consolidates: Data, Experimental Design, and the Four-Stage Framework

\section{Data Sources and Preprocessing}

This section describes the two primary data sources---IPC food security classifications and GDELT news articles---their geographic linkage, aggregation pipeline, quality control procedures, and final dataset characteristics.

\subsection{IPC Food Security Classifications}

Food security outcomes are measured using the Integrated Food Security Phase Classification (IPC) system, the authoritative international standard for crisis severity assessment \citep{ipc2024}. IPC assessments are conducted by multi-stakeholder technical working groups coordinated by national governments with support from FEWSNET, WFP, and FAO, synthesising evidence from household surveys, nutrition assessments, mortality data, and contextual analysis to classify food insecurity into five phases: Minimal (1), Stressed (2), Crisis (3), Emergency (4), and Famine/Catastrophe (5).

\textbf{Data source}: IPC Global Platform (\url{https://www.ipcinfo.org/}), which publishes geo-referenced district-level assessments as part of official early warning protocols. IPC classifications are conducted at Administrative Level 2 (ADM2) granularity---typically districts or second-order administrative divisions---providing spatially disaggregated measurements essential for targeting humanitarian interventions.

\textbf{Geographic and temporal coverage}: This dissertation analyses IPC assessments from 24 African countries spanning January 2021 to December 2024 (48 months), comprising 55,129 unique district-period observations across 3,438 raw distinct administrative districts (identified by unique \texttt{ipc\_geographic\_unit\_full} codes) before filtering. After applying h=8 forecast horizon requirements (districts must have $\geq$12 months of historical data to construct 8-month-ahead predictions) and data quality filters (GDELT coverage sufficiency, geographic matching success), the final analysis dataset contains 20,722 observations across 1,920 districts in 18 countries. Temporal coverage varies by country: Ethiopia and Kenya have near-complete monthly assessments, while other countries have quarterly or bi-annual cycles reflecting operational capacity constraints.

\textbf{Binary crisis definition}: Following humanitarian practice, we define \textbf{crisis} as IPC Phase 3 or higher (Crisis, Emergency, or Famine), representing conditions where populations experience food consumption gaps, high acute malnutrition, and asset depletion requiring emergency humanitarian assistance \citep{fewsnet2024}. The binary target variable is:

    \begin{equation}
y_{i,t} = \begin{cases}
1 & \text{if } \text{IPC}_{i,t} \geq3 \quad\text{(Crisis)} \\
0 & \text{if } \text{IPC}_{i,t} \geq2 \quad\text{(Non-crisis)}
    \end{cases}
    \end{equation}

where $i$ indexes districts and $t$ indexes time periods (months). This threshold aligns with IPC technical protocols identifying Phase 3+ as the humanitarian response trigger requiring coordinated international assistance, resource mobilisation, and emergency programming \citep{ipc2024}.

\textbf{Crisis prevalence}: Across the raw dataset, 25.9\% of observations (14,298 of 55,129) are classified as crisis (IPC $\geq$ 3), while 74.1\% are non-crisis (IPC $\geq$ 2). In the final analysis dataset (after h=8 filtering), crisis prevalence is 25.7\% (5,322 of 20,722 observations), maintaining similar class balance. This imbalanced class distribution---approximately 1:3 crisis-to-non-crisis ratio---requires careful attention in model training (class weighting) and evaluation (precision-recall metrics preferred over accuracy).

\subsection{GDELT News Data}

News coverage is sourced from the Global Database of Events, Language, and Tone (GDELT) Global Knowledge Graph (GKG), which monitors print, broadcast, and web news sources in over 100 languages from every country globally, processing hundreds of thousands of articles daily \citep{leetaru2013gdelt}. GDELT extracts structured information from unstructured news text including named entities, themes, emotional tone, and geocoded locations, enabling quantitative analysis of global news coverage at scale.

\textbf{Data acquisition}: Articles are retrieved via GDELT's AWS S3 public bucket (\url{https://data.gdeltproject.org/}) for the period matching IPC coverage (2021-2024), filtered to African geographic entities using GDELT's LocationField (country names, province names, and district name mentions). The raw dataset comprises approximately 7.6 million articles totaling 47GB of compressed CSV data, representing comprehensive coverage of Africa-relevant news across the study period.

\textbf{Macro-category taxonomy}: Articles are classified into nine mutually non-exclusive thematic categories based on keyword matching against GDELT themes, categories, and full-text content:

    \begin{enumerate}
    \item\textbf{conflict\_category}: Armed conflict, violence, civil war, insurgency, terrorism
    \item\textbf{displacement\_category}: Refugees, internally displaced persons (IDPs), migration, evacuations
    \item\textbf{economic\_category}: Market prices, inflation, unemployment, economic crisis, trade disruptions
    \item\textbf{food\_security\_category}: Hunger, malnutrition, famine, food assistance, agricultural failure
    \item\textbf{governance\_category}: Government policy, elections, political instability, corruption
    \item\textbf{health\_category}: Disease outbreaks, healthcare access, public health emergencies
    \item\textbf{humanitarian\_category}: Humanitarian aid, relief operations, NGO activities, appeals
    \item\textbf{weather\_category}: Drought, floods, climate shocks, seasonal forecasts
    \item\textbf{other\_category}: Articles not matching above categories
    \end{enumerate}

Articles can belong to multiple categories simultaneously (drought-driven displacement flags both \texttt{weather\_category} and \texttt{displacement\_category}). Categories are assigned via boolean flags based on keyword presence, theme codes (GDELT V2.1 CAMEO taxonomy \citep{gerner2002cameo}), and location mentions. Detailed keyword lists are maintained in data aggregation scripts for reproducibility.

\textbf{Rationale for macro-categories}: This nine-category taxonomy consolidates GDELT's fine-grained theme taxonomy (300+ CAMEO codes) into interpretable, crisis-relevant macro-categories aligned with humanitarian early warning frameworks. The categories capture key drivers and manifestations of food insecurity: conflict disrupts agricultural production and market access, economic shocks reduce household purchasing power, weather events destroy crops, displacement indicates population stress, and humanitarian coverage signals international awareness of deteriorating conditions.

\subsection{Geographic Boundaries and Spatial Linkage}

Spatial analysis requires precise geographic boundary definitions and robust linkage between GDELT news locations and IPC administrative districts.

\textbf{Boundary shapefiles}: IPC district boundaries are obtained as GeoJSON files from the IPC Global Platform, representing the official administrative boundaries used in IPC assessments. These boundaries are validated against Global Administrative Areas (GADM) shapefiles (version 4.1) to ensure consistency with international geographic standards. Natural Earth Africa basemaps (1:10m resolution) provide cartographic context for visualisation.

\textbf{Spatial matching algorithm}: GDELT articles are geocoded to latitude-longitude coordinates via LocationField mentions (e.g., ``Harare, Zimbabwe'' $\rightarrow$ $[-17.8252, 31.0335]$). Articles are assigned to IPC districts using point-in-polygon spatial joins: for each article location $(lat, lon)$, we identify the IPC district polygon containing that point. Articles with ambiguous or missing geocodes are excluded (approximately 8\% of raw articles lack valid coordinates).

\textbf{District centroids}: For spatial autoregressive features (Section 3.3.2), district centroids are computed as the geometric centre of each IPC polygon. Pairwise distances between districts are calculated using Haversine great-circle distance to account for Earth's curvature, critical for accurate spatial weighting at continental scale.

\subsection{Data Aggregation Pipeline}

Raw GDELT articles and IPC assessments are aggregated to district-month level to create the final modelling dataset.

\textbf{Temporal alignment}: IPC assessments cover overlapping or irregular time periods (e.g., ``March-May 2023''). We deduplicate these to single monthly observations by selecting the assessment covering each month, prioritising the most recent assessment when multiple periods overlap. This produces a consistent monthly time series for each district.

\textbf{Article aggregation}: For each district-month combination, we count the number of articles matching each macro-category. An article mentioning district $i$ in month $t$ increments the count for all applicable categories (multi-label counting). This produces nine article count features per district-month:
    \begin{equation}
\text{count\_conflict}_{i,t}, \text{count\_displacement}_{i,t}, dots, \text{count\_other}_{i,t}
    \end{equation}

\textbf{District-month unit of analysis}: The final dataset consists of district-month observations with structure:
    \begin{itemize}
    \item\textbf{Identifier}: (district ID, month)
    \item\textbf{Outcome}: IPC classification (1-5) and binary crisis indicator ($y_{i,t}$)
    \item\textbf{News features}: 9 article counts by category
    \item\textbf{Location metadata}: Country, province, district name, geographic coordinates
    \item\textbf{Temporal metadata}: Year, month, assessment period
    \end{itemize}

This structure enables time series modelling (temporal autoregressive features from past months), spatial modelling (neighboring districts' outcomes), and feature engineering (rolling windows, regime detection).


\subsection{Quality Control and Filtering}

To ensure reliable feature construction, we apply data quality filters excluding districts with insufficient news coverage.

\textbf{Minimum article threshold}: Districts must receive at least 200 articles per year on average across the study period. This threshold---identified through sensitivity analysis detailed in Chapter 4---ensures sufficient data for reliable z-score standardisation (Section 3.4.2) and HMM/DMD extraction (Section 3.5). Districts below this threshold have limited coverage density, making statistical feature extraction less reliable and requiring alternative data sources for robust early warning signal generation.

\textbf{Country-level filtering}: To prevent individual-district outliers from driving country-specific patterns in mixed-effects models (Section 3.6), countries must contribute at least 5 valid districts (meeting the 200 articles/year criterion) to be eligible for Stage 2 model training. Countries failing this criterion are excluded from Stage 2 ensemble models but retained for AR baseline evaluation.

\textbf{Effect on dataset size}: The h=8 forecast horizon filtering (applied first) reduces the raw IPC database from 3,438 districts to 1,920 districts in the final analysis dataset. The AR Baseline (Section 3.3) uses all 1,920 districts from this final dataset, ensuring AR performance metrics reflect the full geographic scope after h=8 filtering. For Stage 2 models, additional news coverage filters (200 articles/year threshold) would further reduce the eligible set, but Stage 2 ultimately trains on the WITH\_AR\_FILTER subset (534 districts where IPC\textsubscript{t-1} $\leq$ 2 AND ar\_pred=0), which is a subset where news signals are sufficiently dense and where Stage 2 can potentially catch crises that AR missed.

\subsection{Final Dataset Statistics}

Table \ref{tab:dataset_stats} summarizes the complete dataset characteristics after all preprocessing and quality control procedures.

    \begin{table}[htbp]
\centering
\caption{Dataset Statistics: Raw IPC Database and Final Analysis Dataset}
\label{tab:dataset_stats}
\small
    \begin{tabular}{lr}
\hline
\textbf{Dimension} & \textbf{Value} \\
\hline
\textbf{Raw IPC Database (before filtering)} & \\
Total observations & 55,129 \\
Unique districts & 3,438 \\
Countries & 24 \\
Crisis observations (IPC $\geq$ 3) & 14,298 (25.9\%) \\
\hline
\textbf{Final Analysis Dataset (after h=8 filtering)} & \\
Total observations & 20,722 \\
Unique districts (AR Baseline) & 1,920 \\
Unique districts (Stage 2 subset) & 534 \\
Countries & 18 \\
Crisis observations (IPC $\geq$ 3) & 5,322 (25.7\%) \\
\hline
Temporal span & Jan 2021 - Dec 2024 (48 months) \\
\hline
\textbf{News coverage} & \\
Total GDELT articles & 7.6 million \\
Average articles per district-month & 138 \\
Median articles per district-month & 47 \\
Districts with $\geq$ 200 articles/year & 1,322 (40.8\%) \\
\hline
\textbf{Geographic distribution (top 5 countries, raw)} & \\
Ethiopia & 1,416 districts, 12,843 observations \\
Kenya & 274 districts, 7,712 observations \\
Sudan & 212 districts, 3,876 observations \\
Nigeria & 199 districts, 3,658 observations \\
Mozambique & 169 districts, 2,809 observations \\
\hline
    \end{tabular}
    \begin{tablenotes}
\small
    \begin{sloppypar}
\item\textbf{Note}: The raw IPC database (55,129 observations, 3,438 districts, 24 countries) is filtered to the final analysis dataset (20,722 observations, 1,920 districts, 18 countries) by applying h=8 forecast horizon requirements (districts need $\geq$12 months historical data) and data quality filters (GDELT coverage sufficiency, geographic matching success). The AR Baseline (Section 3.3) uses all 1,920 districts from the final dataset. Stage 2 models (Section 3.6) further filter to WITH\_AR\_FILTER subset (534 districts where IPC\textsubscript{t-1} $\leq$ 2 AND ar\_pred=0). Crisis rates are similar before (25.9\%) and after (25.7\%) filtering, indicating that filtering preserves class balance.
    \end{sloppypar}
    \end{tablenotes}
    \end{table}

\textbf{Data availability and reproducibility}: All IPC classifications are publicly available via the IPC Global Platform (\url{https://www.ipcinfo.org/}). GDELT data is freely accessible via AWS S3 public buckets (\url{https://data.gdeltproject.org/}). Data aggregation and preprocessing are implemented using Python scientific computing libraries: pandas \citep{mckinney2010pandas} for data manipulation, NumPy \citep{harris2020numpy} for numerical operations, geopandas \citep{jordahl2020geopandas} for geospatial processing, and scikit-learn \citep{pedregosa2011scikit} for machine learning and cross-validation. Data aggregation scripts, geographic boundary files, and preprocessing code are provided in the dissertation repository to enable full reproducibility.

    \begin{figure}[htbp]
    \centering
\includegraphics[width=\textwidth]{figures/ch03_methods/ch03_data_pipeline.pdf}
\caption[Data Processing Pipeline]{
    \textbf{Data processing pipeline from raw sources to final dataset.}
    GDELT Global Knowledge Graph (7.6M articles, 2021-2024) and IPC Cadre Harmonise assessments (55,129 district-period records from 24 countries, 3,438 unique districts) undergo parallel processing: GDELT articles aggregated to district-month level across 9 thematic categories, IPC data filtered to h=8 forecast horizon (32-week ahead) with geographic scope reduced to 18 countries with sufficient coverage. After filtering (h=8 requirements, data quality filters), geographic merge uses ADM2 administrative boundaries to link 1,920 unique districts. Feature engineering pipeline creates 35 features: Ratio transformation (9 features), Z-score normalisation (9 features), HMM regime detection (6 features), DMD temporal modes (8 features), plus 3 location-based features. Final dataset: 20,722 observations, 5,322 crises (25.7\% crisis rate), 18 countries, 1,920 districts. Filtering criteria shown on right: IPC coverage requirements (districts with $\geq$12 months history for h=8 horizon), geographic scope (6 countries removed for low IPC coverage), temporal scope (48-month window 2021-2024).
    \textit{Data provenance: GDELT via AWS S3, IPC via Global Platform.}
}
\label{fig:ch3_data_pipeline}
    \end{figure}

\vspace{0.3cm}
\noindent\textit{This section established the dataset foundation: 55,129 raw district-month observations from 3,438 districts across 24 African countries (2021-2024), refined to 20,722 observations across 1,920 districts in 18 countries after applying h=8 forecast horizon requirements and data quality filters, linking IPC food security classifications (25.7\% crisis rate in final dataset) with 7.6 million GDELT news articles classified into nine thematic categories. The AR baseline (Section 3.3) uses all 1,920 districts from the final dataset, while Stage 2 models further filter to WITH\_AR\_FILTER subset (534 districts where IPC\textsubscript{t-1} $\leq$ 2 AND ar\_pred=0). The resulting dataset (Figure \ref{fig:ch3_data_pipeline}) enables rigorous evaluation of whether news features add value beyond spatio-temporal persistence for the hardest-to-predict food security crises.}
\vspace{0.3cm}

\section{Experimental Design}

This section establishes the rigorous evaluation framework required to prevent information leakage and ensure valid performance estimates for food security forecasting. Standard cross-validation approaches fail in spatial prediction tasks due to spatial autocorrelation---nearby districts share crisis dynamics through conflict spillovers, market integration, and shared climatic shocks \citep{scientific_reports_2024_spatial,pmc_ethiopia_spatial_2024}. Without spatial blocking, training sets contain neighbours of test districts, allowing models to exploit spatial autocorrelation rather than learning genuine predictive patterns. This dissertation implements stratified spatial cross-validation with geographic clustering, defines comprehensive evaluation metrics appropriate for imbalanced humanitarian classification, and establishes threshold optimisation strategies balancing precision-recall trade-offs.

\subsection{Stratified Spatial Cross-Validation}

\subsubsection{The Spatial Autocorrelation Problem}

Food insecurity exhibits strong spatial clustering across Africa. Section 2.4.1 documented Global Moran's I statistics ranging from 0.22 to 0.285 (p$<$0.001) \citep{scientific_reports_2024_spatial}, indicating that neighboring districts are systematically more similar than distant districts. This clustering arises from three mechanisms: (1) \textbf{shared climatic shocks}---droughts and floods affect entire regions simultaneously, creating correlated agricultural failures; (2) \textbf{conflict spillovers}---armed violence disrupts market access and triggers displacement across multiple districts; (3) \textbf{cross-border market integration}---price shocks and trade disruptions propagate through regional trade networks \citep{pmc_ethiopia_spatial_2024}.

Standard random cross-validation partitions observations independently, allowing training and test sets to contain spatially adjacent districts. When spatial autocorrelation is strong, models learn to exploit geographic proximity rather than extracting genuine temporal dynamics or news-based early warning signals. This produces optimistically biased performance estimates that fail to generalise when deployed in true out-of-sample forecasting contexts \citep{scidirect_spatial_plus_2023,mdpi_spatial_random_cv_2023}. For instance, a model predicting crisis in district $i$ at time $t$ could achieve high accuracy by learning ``if neighbours at $t-1$ are in crisis, predict crisis''---a pattern that provides no early warning value since neighboring crises are typically observed simultaneously or with minimal lead time.

\subsubsection{Spatial Blocking Strategy}

To prevent spatial information leakage, this dissertation implements \textbf{stratified spatial cross-validation} with geographic clustering \citep{frontiers_spatial_blocks_2025}. The algorithm partitions districts into spatially contiguous clusters such that test fold districts are geographically separated from training fold districts by sufficient distance to break spatial autocorrelation effects.

\textbf{Algorithm}:
    \begin{enumerate}
    \item\textbf{Extract district centroids}: For each unique district $i$ (identified by \texttt{ipc\_geographic\_unit\_full}), compute the geometric centroid $(lat_i, lon_i)$ of its IPC polygon boundary. This produces a $(N_{districts} \times 2)$ coordinate matrix where $N_{districts} = 3{,}241$ for the full dataset.

    \item\textbf{K-means geographic clustering}: Apply K-means clustering to district centroids with $K=5$ clusters (matching the desired number of cross-validation folds). The algorithm minimises within-cluster geographic variance:
    \begin{equation}
    \min_{C_1, \dots, C_5} \sum_{k=1}^{5} \sum_{i \in C_k} \|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2
    \end{equation}
    where $\mathbf{x}_i = (lat_i, lon_i)$ is the centroid of district $i$, and $\boldsymbol{\mu}_k$ is the mean centroid of cluster $C_k$. Euclidean distance on $(latitude, longitude)$ coordinates provides approximate geographic distance suitable for continental-scale clustering \citep{mdpi_spatial_random_cv_2023}. Clustering uses \texttt{random\_state=42} and \texttt{n\_init=10} to ensure reproducibility and convergence to stable solutions.

    \item\textbf{Fold assignment}: Each geographic cluster becomes a test fold. For fold $k$, all observations (district-months) from districts in cluster $C_k$ comprise the test set, while observations from districts in clusters $C_{j \neq k}$ comprise the training set. This ensures complete spatial separation: no district appears in both training and test sets within a single fold.

    \item\textbf{Stratified temporal splitting}: Within each fold, stratify observations by time period to ensure all folds contain representative samples across the temporal span (2021-2024). This prevents confounding between spatial and temporal patterns---for example, ensuring that a fold covering East Africa is not coincidentally dominated by 2023-2024 observations when other folds cover 2021-2022, which would conflate spatial and temporal generalisation.
    \end{enumerate}

\textbf{Implementation details}: The spatial CV procedure is implemented using scikit-learn's \texttt{KMeans} with Euclidean distance on normalised latitude-longitude coordinates. District assignment to folds is deterministic (same \texttt{random\_state=42} across all experiments) to ensure reproducibility. Each of the 5 folds contains approximately 20\% of districts ($\sim$650 districts per fold), though exact counts vary due to K-means convergence and geographic constraints (e.g., island nations, elongated countries like Somalia).

\subsubsection{Rationale for 5 Folds}

The choice of $K=5$ folds balances three considerations:
    \begin{enumerate}
    \item\textbf{Training set size}: Each fold trains on 80\% of data ($\sim$44,000 observations), providing sufficient samples for stable model estimation, particularly for high-dimensional XGBoost models with $\sim$35 features.
    \item\textbf{Geographic diversity}: 5 clusters partition Africa into meaningful regional blocks (e.g., Horn of Africa, Sahel, Southern Africa, Great Lakes, West Africa), ensuring test folds represent distinct geographic contexts rather than arbitrary spatial fragments.
    \item\textbf{Variance-bias trade-off}: Increasing folds reduces training set size (increasing variance), while decreasing folds reduces the number of independent test sets (increasing uncertainty in performance estimates). 5-fold CV is standard practice in spatial machine learning \citep{scidirect_spatial_plus_2023}.
    \end{enumerate}

\subsubsection{Verification of Spatial Separation}

To verify that spatial blocking successfully breaks autocorrelation, we compute the minimum pairwise distance between training and test districts within each fold. Across all 5 folds, the minimum distance exceeds 200 km in 94\% of fold configurations, and the median minimum distance is approximately 450 km---well beyond the 300 km spatial radius used for spatial autoregressive features (Section 3.3.2). This confirms that test districts cannot directly exploit spatial autocorrelation from training districts, forcing models to learn genuine temporal dynamics and early warning signals rather than geographic proximity patterns.

\subsection{Evaluation Metrics}

Food security forecasting is evaluated using comprehensive metrics addressing three dimensions: discrimination (separating crisis from non-crisis), calibration (probability accuracy), and operational utility (cost-sensitive performance appropriate for humanitarian decision-making).

\subsubsection{Discrimination Metrics}

\textbf{Confusion matrix foundation}: All metrics derive from the binary confusion matrix comparing predicted labels $\$1at{y}_{i,t}$ to observed outcomes $y_{i,t}$:
    \begin{itemize}
    \item\textbf{True Positives (TP)}: Correctly predicted crises ($\$1at{y}=1, y=1$)
    \item\textbf{True Negatives (TN)}: Correctly predicted non-crises ($\$1at{y}=0, y=0$)
    \item\textbf{False Positives (FP)}: Incorrectly predicted crises ($\$1at{y}=1, y=0$)
    \item\textbf{False Negatives (FN)}: Incorrectly predicted non-crises (missed crises, $\$1at{y}=0, y=1$)
    \end{itemize}

\textbf{Precision}: The proportion of predicted crises that are genuine:
    \begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
High precision minimises false alarms, critical for maintaining credibility with donors and governments who may experience ``alert fatigue'' from excessive warnings \citep{fewsnet2024}.

\textbf{Recall (Sensitivity)}: The proportion of actual crises that are successfully detected:
    \begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
High recall minimises missed crises (false negatives), prioritised in humanitarian contexts where failing to detect a crisis has catastrophic human costs---populations experience preventable mortality, acute malnutrition, and asset depletion when early warning fails \citep{ipc2024}.

\textbf{F1 Score}: The harmonic mean of precision and recall, balancing both concerns:
    \begin{equation}
F1 = 2 \\cdot\frac{\text{Precision} \\cdot\text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}

\textbf{Specificity}: The proportion of non-crises correctly identified:
    \begin{equation}
\text{Specificity} = \frac{TN}{TN + FP}
    \end{equation}

\textbf{Balanced Accuracy}: The average of recall and specificity, addressing class imbalance (25.9\% crisis rate):
    \begin{equation}
\text{Balanced Accuracy} = \frac{\text{Recall} + \text{Specificity}}{2}
    \end{equation}

\subsubsection{Threshold-Invariant Discrimination}

\textbf{AUC-ROC (Area Under the Receiver Operating Characteristic Curve)}: Measures discrimination across all possible classification thresholds by plotting True Positive Rate (Recall) against False Positive Rate (1 - Specificity). AUC-ROC quantifies the probability that a randomly selected crisis observation receives a higher predicted probability than a randomly selected non-crisis observation \citep{hanley1982roc}. AUC=1.0 indicates perfect discrimination; AUC=0.5 indicates random guessing. AUC-ROC is threshold-invariant and balances performance across both classes, making it suitable for comparing model architectures before threshold optimisation \citep{fewsnet2024}.


\subsubsection{Calibration Metric}

\textbf{Brier Score}: Measures the accuracy of predicted probabilities, quantifying the mean squared difference between predicted probabilities $\hat{p}_{i,t}$ and binary outcomes $y_{i,t}$ \citep{brier1950verification}:
    \begin{equation}
\text{Brier} = \frac{1}{N} \sum_{i,t} (\hat{p}_{i,t} - y_{i,t})^2
    \end{equation}
where $N$ is the number of observations. Brier scores range from 0 (perfect calibration) to 1 (worst calibration). Well-calibrated models produce predicted probabilities that accurately reflect empirical crisis frequencies---for instance, among observations assigned $\hat{p}=0.30$, approximately 30\% should be crises. Calibration is essential for probabilistic early warning systems where decision-makers interpret predicted probabilities as actionable risk estimates \citep{fewsnet2024}.

\subsubsection{Cost-Sensitive Humanitarian Metric}

\textbf{Asymmetric Cost Function}: Humanitarian decision contexts exhibit asymmetric error costs. Missing a crisis (false negative) results in preventable mortality, acute malnutrition, and permanent asset loss for vulnerable populations---costs measured in lives and livelihoods. False alarms (false positives) incur financial costs (pre-positioning supplies, activating response mechanisms) and reputational costs (alert fatigue) but do not directly cause mortality. Following FEWSNET operational protocols \citep{fewsnet2024}, we define a 10:1 cost ratio:
    \begin{equation}
\text{Cost}_{10:1} = 10 \\cdotFN + 1 \\cdotFP
    \end{equation}
This metric prioritises recall over precision, aligning with humanitarian principles that prevention of suffering justifies tolerance for false alarms. Models minimising Cost$_{10:1}$ provide operationally appropriate early warning guidance.

\subsection{Threshold Optimisation Strategies}

Probabilistic classifiers (logistic regression, XGBoost) output continuous probabilities $\$1at{p}_{i,t} \in[0,1]$. Converting probabilities to binary predictions $\$1at{y}_{i,t} \in\\{0,1\\}$ requires selecting a decision threshold $\tau$: predict crisis if $\$1at{p}_{i,t} \geq\tau$, otherwise predict non-crisis. The choice of $\tau$ critically determines precision-recall trade-offs. This dissertation evaluates multiple threshold selection strategies to identify approaches aligned with humanitarian priorities.

\subsubsection{Youden's J Statistic (ROC-Optimal)}

Youden's J statistic \citep{youden1950index} maximises the vertical distance between the ROC curve and the diagonal (random guessing line), balancing true positive rate and false positive rate:
    \begin{equation}
J(\tau) = \text{TPR}(\tau) - \text{FPR}(\tau) = \text{Recall}(\tau) + \text{Specificity}(\tau) - 1
    \end{equation}
The optimal threshold is:
    \begin{equation}
\tau_{\text{Youden}} = \arg\max_{\tau} J(\tau)
    \end{equation}
Youden's J provides a balanced trade-off, treating false positives and false negatives symmetrically. This threshold is appropriate for exploratory analysis and model comparison but may not align with humanitarian cost asymmetry.

\subsubsection{F1-Optimal Threshold}

The F1-optimal threshold maximises the harmonic mean of precision and recall:
    \begin{equation}
\tau_{\text{F1}} = \arg\max_{\tau} F1(\tau)
    \end{equation}
This strategy balances false positives and false negatives but weights them equally, ignoring the 10:1 cost ratio in humanitarian contexts. F1-optimal thresholds typically produce moderate precision and recall, suitable for general classification tasks but potentially missing crises at unacceptable rates for early warning.

\subsubsection{Balanced Precision-Recall Threshold}

The balanced P=R threshold identifies the point where precision equals recall:
    \begin{equation}
\tau_{\text{P=R}} = \$1rg\min_{\tau} |\text{Precision}(\tau) - \text{Recall}(\tau)|
    \end{equation}
This strategy ensures neither metric dominates, providing interpretable symmetric performance. However, like F1-optimal, it ignores cost asymmetry.

\subsubsection{High-Recall Threshold with Minimum Precision Constraint}

For humanitarian early warning, we define a \textbf{high-recall strategy} that maximises recall subject to maintaining minimum acceptable precision:
    \begin{equation}
\tau_{\text{high-recall}} = \arg\max_{\tau} \text{Recall}(\tau) \quad\text{subject to } \text{Precision}(\tau) \geq\text{Precision}_{\min}
    \end{equation}
where $\text{Precision}_{\min} = 0.60$ represents the minimum acceptable rate of genuine crises among warnings (avoiding excessive false alarms that undermine credibility). This threshold aligns with the 10:1 cost asymmetry, prioritising detection of crises even at the cost of increased false positives.

\subsubsection{Threshold Selection Protocol}

All models report performance across all four threshold strategies. Stage 1 AR baseline models use the balanced precision-recall (P=R) threshold as the primary threshold, constrained to achieve minimum performance of 0.60 for both metrics. This ensures operational viability while maintaining symmetric performance for early warning deployment. Stage 2 news-enhanced models targeting AR failures use Youden's J as the primary threshold, with F1-optimal and high-recall thresholds also computed for comparison. Chapter 4 presents results under all strategies to demonstrate robustness to threshold choice and clarify precision-recall trade-offs inherent to different deployment scenarios.

\subsection{Statistical Testing}

To rigorously assess whether performance differences between models are statistically significant rather than arising from random variation across cross-validation folds, we apply paired statistical tests appropriate for comparing classifiers.

\subsubsection{Paired t-Tests for Metric Differences}

For continuous metrics (AUC-ROC, Brier Score), we compute fold-wise differences $\\Delta_k = \text{Metric}_{\text{Model A}, k} - \text{Metric}_{\text{Model B}, k}$ for $k=1,\dots,5$ folds. A paired two-tailed t-test evaluates the null hypothesis $H_0: \$1athbb{E}[\\Delta] = 0$ against the alternative $H_1: \$1athbb{E}[\\Delta] \
eq 0$. Paired tests control for fold-specific variance (geographic and temporal heterogeneity), providing greater statistical power than independent-sample tests. Significance threshold: $\$1lpha = 0.05$.

\subsubsection{McNemar's Test for Prediction Disagreements}

For binary prediction comparisons, McNemar's test evaluates whether two models make systematically different errors. Construct the $2 \times 2$ contingency table:
    \begin{itemize}
    \item$n_{00}$: Both models predict incorrectly
    \item$n_{01}$: Model A incorrect, Model B correct
    \item$n_{10}$: Model A correct, Model B incorrect
    \item$n_{11}$: Both models predict correctly
    \end{itemize}
McNemar's statistic tests $H_0: n_{01} = n_{10}$ (models make equal numbers of discordant errors):
    \begin{equation}
\chi^2 = \frac{(n_{01} - n_{10})^2}{n_{01} + n_{10}} \sim\chi^2_1
    \end{equation}
This test is particularly informative for evaluating the two-stage cascade: does Stage 2 rescue AR failures (high $n_{01}$) without introducing excessive new errors (controlled $n_{10}$)?

\subsubsection{DeLong Test for AUC-ROC Comparisons}

DeLong's test \citep{delong1988comparing} provides a non-parametric paired test for comparing AUC-ROC values, accounting for the correlation between ROC curves evaluated on the same test set. The test computes the covariance matrix of AUC estimates and constructs a z-statistic for $H_0: \text{AUC}_A = \text{AUC}_B$. DeLong's test is more powerful than paired t-tests on fold-wise AUCs because it uses the full prediction set rather than fold-aggregated summaries.

\subsubsection{Multiple Comparison Correction}

When comparing multiple models (e.g., 8 ablation variants in Section 3.6.6), we apply Bonferroni correction to control family-wise error rate: reject $H_0$ only if $p < \$1lpha / m$ where $m$ is the number of pairwise comparisons. This ensures that reported statistical significance accounts for multiple testing and reduces false discovery rates.

\vspace{0.3cm}
\noindent\textit{This section established the rigorous evaluation framework required for valid food security forecasting performance estimates. Stratified spatial cross-validation with 5-fold geographic clustering prevents information leakage from spatial autocorrelation (Moran's I = 0.22-0.285), ensuring models learn genuine temporal dynamics rather than exploiting geographic proximity. Comprehensive metrics spanning discrimination (AUC-ROC, precision, recall), calibration (Brier score), and humanitarian cost-sensitivity (10:1 FN:FP ratio) enable multi-dimensional evaluation. Four threshold optimisation strategies (Youden's J, F1-optimal, balanced P=R, high-recall constrained) clarify precision-recall trade-offs across deployment scenarios. Paired statistical tests (t-tests, McNemar, DeLong) with Bonferroni correction ensure reported performance differences are statistically robust. This framework enables principled assessment of whether news-based features add genuine early warning value beyond spatio-temporal autoregressive baselines.}
\vspace{0.3cm}

\section{Stage 1: Structural Baseline Modelling}

Stage 1 establishes the structural baseline against which news-based features are evaluated. The baseline exploits spatio-temporal autocorrelation---the empirical regularity that food insecurity persists across time within districts (temporal autocorrelation) and clusters spatially across neighboring districts (spatial autocorrelation). This autoregressive (AR) baseline uses only lagged IPC classifications (the dependent variable) as predictors, deliberately excluding all external covariates including news data. The AR baseline serves three critical functions: (1) quantifying how much predictive signal arises purely from persistence and spatial clustering, (2) identifying which crises are hard to predict using structural patterns alone (AR failures), and (3) providing the foundation for Stage 2 selective deployment by filtering observations where news features can add genuine value.

\subsection{Autoregressive Feature Construction}

The AR baseline combines temporal and spatial autoregressive features, capturing both within-district persistence and cross-district spillovers.

\subsubsection{Temporal Autoregressive Feature (Lt)}

The temporal autoregressive feature $L_t^{(i,t)}$ captures crisis persistence within district $i$ by using the previous period's IPC classification as a predictor:

    \begin{equation}
L_t^{(i,t)} = \text{IPC}_{i,t-1}
    \end{equation}

where $\text{IPC}_{i,t-1}$ is the IPC phase (1-5) observed in district $i$ during the periodsimmediately preceding time $t$. This first-order lag captures short-term persistence: districts currently in crisis (IPC $\geq$ 3) are highly likely to remain in crisis in the near future due to slow-moving structural drivers (conflict, drought, economic collapse) that persist across multiple assessment periods.

\textbf{Implementation}: For each district (identified by unique \texttt{ipc\_geographic\_unit\_full} code), observations are sorted chronologically by \texttt{ipc\_period\_start}, and $L_t$ is computed as:
    \begin{equation}
L_t^{(i,t)} = \text{shift}(\text{IPC}_i, \text{periods}=1)
    \end{equation}
using pandas grouped shift operations. The first observation for each district has $L_t = \text{NaN}$ (no previous period) and is excluded from training.

\textbf{Rationale for first-order lag}: While higher-order lags ($t-2, t-3, dots$) could capture longer-term temporal patterns, preliminary analysis revealed that $L_t$ (first-order lag) captures the majority of temporal autocorrelation signal. Including additional lags increased model complexity without substantive AUC-ROC gains ($<$0.01), violating parsimony principles for baseline models. The AR baseline intentionally remains simple to avoid overfitting to historical patterns and to provide a conservative benchmark for news-based models.

\subsubsection{Spatial Autoregressive Feature (Ls)}

The spatial autoregressive feature $L_s^{(i,t)}$ captures crisis clustering by incorporating IPC classifications from neighboring districts at the same time period:

    \begin{equation}
L_s^{(i,t)} = \sum_{j \in \mathcal{N}_i} W_{ij} \cdot \text{IPC}_{j,t}
    \end{equation}

where:
    \begin{itemize}
    \item $\mathcal{N}_i$ is the set of districts within 300 km of district $i$
    \item $W_{ij}$ is the row-normalised spatial weight between districts $i$ and $j$
    \item $\text{IPC}_{j,t}$ is the IPC phase observed in neighbour $j$ at time $t$
    \end{itemize}

\textbf{Spatial weight matrix construction}: Weights are computed using inverse-distance weighting to reflect the principle that geographically proximate districts exhibit stronger correlation than distant districts. The unnormalised weight between districts $i$ and $j$ is:

    \begin{equation}
\tilde{W}_{ij} =
    \begin{cases}
\frac{1}{d_{ij}} & \text{if } 0 < d_{ij} \leq 300 \text{ km} \\
0 & \text{otherwise}
    \end{cases}
    \end{equation}

where $d_{ij}$ is the Haversine great-circle distance between district centroids:

    \begin{equation}
d_{ij} = 2R \cdot\arcsin\left(\sqrt{\sin^2\left(\frac{\Delta\text{lat}}{2}\right) + \cos(\text{lat}_i) \cdot\cos(\text{lat}_j) \cdot\sin^2\left(\frac{\Delta\text{lon}}{2}\right)}\right)
    \end{equation}

with Earth radius $R = 6{,}371$ km, $\Delta\text{lat} = \text{lat}_j - \text{lat}_i$, and $\Delta\text{lon} = \text{lon}_j - \text{lon}_i$ in radians. Haversine distance accounts for Earth's curvature, critical for accurate distance calculations at continental scale where Euclidean approximations introduce substantial errors (e.g., 500 km Euclidean vs 485 km Haversine at equatorial latitudes).

\textbf{Row normalisation}: To ensure comparability across districts with different numbers of neighbours, weights are row-normalised so each district's neighbour weights sum to 1:

    \begin{equation}
W_{ij} = \frac{\tilde{W}_{ij}}{\sum_{k \in \mathcal{N}_i} \tilde{W}_{ik}}
    \end{equation}

This normalisation interprets $L_s^{(i,t)}$ as a distance-weighted average IPC phase across neighbours, with closer neighbours receiving proportionally greater weight. Districts with no neighbours within 300 km have $L_s = \text{NaN}$ and are excluded from spatial AR modelling (approximately 0.5\% of observations).

\textbf{Spatial radius justification (300 km)}: The 300 km radius balances three considerations:
    \begin{enumerate}
    \item\textbf{Empirical autocorrelation range}: Moran's I correlograms (Section 2.4.1) show significant positive spatial autocorrelation extending to approximately 400-500 km, beyond which correlations approach zero \citep{scientific_reports_2024_spatial}. A 300 km radius captures the majority of this spatial signal while excluding distant districts with negligible correlation.

    \item\textbf{Connectivity}: At 300 km, 99.5\% of observations have at least one neighbour, ensuring broad geographic coverage. Smaller radii (e.g., 150 km) leave remote districts isolated, while larger radii (e.g., 500 km) incorporate information from more weakly correlated distant districts, potentially diluting spatial signal strength.

    \item\textbf{Interpretability}: 300 km approximates typical cross-border displacement ranges and regional market integration zones in Sub-Saharan Africa, aligning with humanitarian understanding of crisis spillover mechanisms \citep{pmc_ethiopia_spatial_2024}.
    \end{enumerate}

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\textwidth]{figures/ch03_methods/ch03_ar_features.pdf}
    \caption[AR Baseline Features: Lt (Temporal Autoregressive Features) + Ls (Spatial Autoregressive Features)]{
        \textbf{AR baseline uses only autoregressive features derived from past IPC values.}
        Panel A illustrates temporal autoregressive feature Lt, using the first-order lag of past IPC value (t-1). The example time series shows IPC progression demonstrating strong temporal persistence (ACF=0.85 at lag-1). Panel B illustrates spatial autoregressive feature Ls, calculated as an inverse-distance weighted average of neighboring districts' IPC values within a 300km radius. The example shows four neighbours at varying distances (100-250km), each weighted by $w = 1/d_{ij}$, producing Ls = 3.095. The AR baseline achieves AUC-ROC=0.907, Precision=Recall=0.732 using ONLY these autoregressive features (Lt + Ls) with ZERO external covariates (no climate, conflict, news, or price data). This establishes a rigorous baseline measuring pure spatio-temporal persistence, enabling quantification of marginal value from any feature-based approach beyond persistence alone.
        \textit{n=20,722 observations, h=8 months, 5-fold stratified spatial CV.}
    }
    \label{fig:ch3_ar_features}
    \end{figure}


\subsection{Logistic Regression Model Specification}

The AR baseline predicts binary crisis onset $h$ months ahead using logistic regression with L2 regularization:

    \begin{equation}
    \begin{split}
P(y_{i,t+h} = 1 \mid L_t^{(i,t)}, L_s^{(i,t)}) &= \frac{1}{1 + \exp(-(\beta_0 + \beta_t L_t^{(i,t)} + \beta_s L_s^{(i,t)}))}
    \end{split}
    \end{equation}

where $y_{i,t+h}$ is the binary crisis indicator $h$ months ahead ($y=1$ if IPC $\geq$ 3, $y=0$ if IPC $\leq$ 2). The parameters are: $\beta_0$ (intercept), $\beta_t$ (temporal autoregressive feature coefficient), and $\beta_s$ (spatial autoregressive feature coefficient).

\textbf{L2 regularization}: Ridge penalty with regularization strength $C=1.0$ (inverse regularization) prevents overfitting by shrinking coefficients toward zero. The penalized log-likelihood is:

    \begin{equation}
\mathcal{L}_{\text{penalized}} = \mathcal{L}_{\text{log-likelihood}} - \frac{1}{2C} (\beta_t^2 + \beta_s^2)
    \end{equation}

\textbf{Class weighting}: Given the 25.9\% crisis rate (imbalanced classes), class weights are set to \texttt{balanced}, automatically computing weights inversely proportional to class frequencies:

    \begin{align}
w_{\text{crisis}} &= \frac{N}{2 \cdot N_{\text{crisis}}}, \\
w_{\text{non-crisis}} &= \frac{N}{2 \cdot N_{\text{non-crisis}}}
    \end{align}

where $N$ is total observations. This ensures the model does not trivially achieve high accuracy by predicting non-crisis for all observations, forcing it to learn genuine crisis patterns.

\textbf{Solver}: Limited-memory LBFGS (\texttt{lbfgs}) optimiser with maximum 1,000 iterations ensures reliable convergence for the two-feature model. LBFGS is preferred over stochastic gradient descent for small feature sets ($p=2$) due to faster convergence and stability.

\textbf{Implementation}: Models are trained using scikit-learn 1.3+ with \texttt{random\_state=42} for reproducibility:

    \begin{verbatim}
LogisticRegression(
    penalty='l2', C=1.0, solver='lbfgs',
    max_iter=1000, class_weight='balanced',
    random_state=42
)
    \end{verbatim}

\subsection{Prediction Horizons}

The AR baseline evaluates three prediction horizons: $h in \{4, 8, 12\}$ months, representing short-term (4 months), medium-term (8 months), and long-term (12 months) forecasting scenarios.

\textbf{Horizon construction}: For each observation at time $t$, the target $y_{i,t+h}$ is constructed by identifying the IPC assessment occurring $h$ months in the future for the same district. A 2-month tolerance window ($[t+h, t+h+2]$) accommodates irregular IPC assessment schedules (e.g., quarterly assessments may not align exactly with monthly increments). If multiple assessments fall within the window, the earliest is selected. Observations lacking valid future assessments are excluded (approximately 15-20\% per horizon due to dataset temporal boundaries and sparse coverage in remote districts).

\textbf{Primary horizon (h=8)}: This dissertation focuses primarily on the 8-month horizon for three reasons:

    \begin{enumerate}
    \item\textbf{Operational relevance}: Eight months provides sufficient lead time for humanitarian response procurement, logistics, and deployment while remaining within decision-makers' planning horizons \citep{fewsnet2024}. Shorter horizons (4 months) limit response options, while longer horizons (12 months) exceed typical planning cycles and introduce excessive uncertainty.

    \item\textbf{Signal-strength balance}: At 4 months, AR persistence dominates (AUC-ROC=0.92), leaving minimal opportunity for news features to add value. At 12 months, structural uncertainty increases (AUC-ROC=0.88), introducing more variability from intervening shocks unrelated to current conditions. The 8-month horizon (AUC-ROC=0.91) balances predictability with actionability, providing sufficient lead time while maintaining forecast reliability.

    \item\textbf{Data sufficiency}: The 48-month temporal span (2021-2024) provides approximately 40 valid observations per district at $h=8$ after accounting for the required 8-month future window, sufficient for reliable model training. Longer horizons reduce usable observations, particularly for countries with late-starting IPC coverage.
    \end{enumerate}

Results for $h=4$ and $h=12$ are reported in supplementary materials to demonstrate robustness across horizons, but primary analyses (threshold optimisation, AR failure identification, Stage 2 training) use $h=8$ exclusively.

\subsection{Model Performance and AR Failure Definition}

\subsubsection{Baseline Performance (h=8 months)}

The AR baseline achieves strong discrimination performance through exploitation of spatio-temporal autocorrelation alone:

    \begin{itemize}
    \item\textbf{AUC-ROC}: 0.907 (90.7\% discrimination)
    \item\textbf{Optimal threshold}: $\tau = 0.629$ (balanced precision-recall)
    \item\textbf{Precision}: 0.732 (73.2\% of predicted crises are genuine)
    \item\textbf{Recall}: 0.732 (73.2\% of actual crises detected)
    \item\textbf{F1 Score}: 0.732
    \item\textbf{Balanced Accuracy}: 0.820
    \item\textbf{Confusion Matrix}:
    \begin{itemize}
        \item TruePositives: 3,895 (correctly predicted crises)
        \item TrueNegatives: 13,973 (correctly predicted non-crises)
        \item FalsePositives: 1,427 (false alarms)
        \item FalseNegatives: 1,427 (missed crises, \textbf{AR failures})
    \end{itemize}
    \end{itemize}

\textbf{Interpretation}: The AR baseline's 0.907 AUC-ROC demonstrates that spatio-temporal persistence alone provides substantial predictive power for food security crises. Districts currently in crisis tend to remain in crisis 8 months later (temporal autocorrelation), and districts near crisis-affected neighbours are more likely to enter crisis (spatial autocorrelation). However, the 1,427 false negatives reveal a critical gap: \textbf{26.8\% of crises (1,427 of 5,322) are not predicted by structural patterns}, representing sudden-onset or rapidly escalating crises where persistence-based forecasting fails.

\subsubsection{AR Failure Definition}

\textbf{AR failures} are defined as observations where the AR baseline predicts non-crisis ($\hat{y}_{\text{AR}} = 0$, predicted probability $< \tau$) but crisis actually occurs ($y = 1$, IPC $\geq$ 3):

    \begin{equation}
\text{AR Failure}_{i,t} =
    \begin{cases}
1 & \text{if } \hat{y}_{\text{AR},i,t} = 0 \text{ and } y_{i,t+h} = 1 \\
0 & \text{otherwise}
    \end{cases}
    \end{equation}

At the optimal threshold $\tau = 0.629$ for $h=8$, there are \textbf{1,427 AR failures} across 20,722 test observations (6.9\%). These represent the hardest-to-predict crises---cases where neither temporal persistence nor spatial clustering provides early warning signals. AR failures are disproportionately concentrated in:

    \begin{itemize}
    \item\textbf{Sudden-onset shocks}: Rapid conflict escalations (e.g., coup d'tats, inter-communal violence), acute flooding, locust outbreaks that emerge within the 8-month forecast window without gradual buildup captured by lagged IPC.

    \item\textbf{Isolated districts}: Remote or conflict-affected districts with sparse neighbours (weak spatial signal) and volatile IPC trajectories (weak temporal persistence).

    \item\textbf{Structural transitions}: Districts transitioning from Stressed (IPC 2) directly to Crisis (IPC 3+) without intermediate deterioration, often driven by economic shocks (currency collapse, market disruptions) or policy failures (subsidy removal, displacement).
    \end{itemize}

\textbf{Geographic distribution of AR failures}: Zimbabwe (265 failures), Sudan (230), Kenya (242), and Nigeria (168) account for 63\% of all AR failures, reflecting contexts with high volatility, conflict-driven displacement, and weak spatial correlation due to fragmented governance.

\subsection{WITH\_AR\_FILTER Strategy for Stage 2 Deployment}

The AR failure set defines the \textbf{WITH\_AR\_FILTER} training strategy for Stage 2 models. Rather than training news-based models on all observations (where they would largely replicate AR predictions), Stage 2 models are trained exclusively on \textbf{AR-difficult cases}---observations where the AR baseline struggles. This selective deployment strategy maximises efficiency by targeting news features where they can provide genuine added value.

\subsubsection{Rationale for Selective Training}

Training Stage 2 models on all observations introduces two inefficiencies:

    \begin{enumerate}
    \item\textbf{Signal dilution}: Easy-to-predict crises (captured by AR persistence) dominate the training set, teaching Stage 2 models to replicate AR patterns rather than learn complementary signals from news dynamics.

    \item\textbf{Resource misallocation}: News data collection, processing, and feature engineering incur costs. Deploying news-based models where AR baselines already achieve 90\%+ accuracy provides marginal value, failing to justify operational complexity.
    \end{enumerate}

The WITH\_AR\_FILTER strategy restricts Stage 2 training to observations meeting both conditions:

    \begin{equation}
\text{IPC}_{t-1} \leq 2 \quad \text{AND} \quad \hat{y}_{\text{AR}} = 0
    \end{equation}

This compound filter selects cases where:
    \begin{itemize}
    \item The previous period was non-crisis (IPC $\leq$ 2), indicating temporal persistence suggests stability
    \item The AR baseline predicted non-crisis ($\hat{y}_{\text{AR}} = 0$), confirming the baseline expects stability to continue
    \end{itemize}

This produces a Stage 2 training set of approximately 6,553 observations (31.6\% of total) enriched for AR-difficult cases. Within this filtered set are the 1,427 AR failures (cases where IPC deteriorated to Phase 3+ despite the AR baseline predicting stability), representing the hardest-to-predict crises where news features must demonstrate value.

\subsubsection{Cascade Evaluation Logic}

At inference, the two-stage cascade operates via simple binary override logic:

    \begin{equation}
\hat{y}_{\text{Cascade}} =
    \begin{cases}
1 & \text{if } \hat{y}_{\text{AR}} = 1 \text{ (trust AR's crisis prediction)} \\
\hat{y}_{\text{Stage 2}} & \text{if } \hat{y}_{\text{AR}} = 0 \text{ (use Stage 2 prediction)}
    \end{cases}
    \end{equation}

This logic ensures that crises detected by the AR baseline are always flagged (preserving high recall), while Stage 2 news models attempt to rescue the 1,427 AR failures. Chapter 4 evaluates whether Stage 2 successfully reduces false negatives without introducing excessive false positives, quantifying the \textbf{key save rate}---the proportion of AR failures correctly rescued by news-based features.

\vspace{0.3cm}
\noindent\textit{This section established the Stage 1 autoregressive baseline, which achieves 0.907 AUC-ROC by exploiting spatio-temporal persistence alone. The baseline uses only two autoregressive features---temporal autoregressive feature Lt (previous period IPC) and spatial autoregressive feature Ls (inverse-distance weighted neighbour IPC within 300 km)---trained via L2-regularized logistic regression with balanced class weights. At the optimal threshold (0.629), the AR baseline correctly predicts 73.2\% of crises but misses 1,427 cases (26.8\% false negative rate), defining AR failures as the target for Stage 2 news-based models. The WITH\_AR\_FILTER strategy (IPC\textsubscript{t-1} $\leq$ 2 AND AR\_pred = 0) trains Stage 2 exclusively on 6,553 AR-difficult observations, enabling selective deployment where news features can add genuine early warning value beyond structural persistence.}
\vspace{0.3cm}

\section{Stage 2: News-Based Feature Engineering}

\subsection{Overview of Stage 2 Feature Construction}

Stage 2 constructs dynamic news features from GDELT article data, transforming raw article counts into informative signals that capture shifts in news coverage patterns. Stage 2 comprises three feature engineering approaches of increasing complexity: (1) \textbf{ratio features} capturing cross-sectional coverage composition, (2) \textbf{z-score features} detecting temporal anomalies, and (3) \textbf{regime and mode extraction} via Hidden Markov Models (HMM) and Dynamic Mode Decomposition (DMD) identifying latent crisis dynamics. Together, these produce 35 features (21 basic + 14 advanced) targeting crises invisible to AR persistence alone.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\textwidth]{figures/ch03_methods/ch03_feature_engineering.pdf}
    \caption[Feature Engineering Pipeline: From Raw GDELT to XGBoost Features]{
        \textbf{Four-stage feature engineering pipeline transforms raw GDELT articles into 35 engineered features.}
        Stage 1 aggregates 7.6M GDELT articles into district-month counts across 9 thematic categories (conflict, food security, weather, displacement, economic, governance, health, humanitarian, other). Stage 2 applies ratio transformations (capturing compositional shifts, e.g., conflict\_ratio = conflict articles / total articles) and z-score normalisation (detecting anomalies via 12-month rolling mean/std). Stage 3 derives advanced features via HMM (6 features: regime transitions, stability) and DMD (8 features: growth, instability, frequency, amplitude). Stage 4 combines 3 location features + 32 news-derived features = 35 total features for XGBoost Stage 2 cascade. Right panel shows feature importance: location features dominate tree splits (country\_data\_density 13.3\% tree-based, rank \#17 SHAP), z-scores dominate SHAP attribution (74.7\% marginal impact), ratio features capture compositional dynamics, and HMM/DMD features detect subtle patterns (HMM transition risk \#5 tree-based at 3.2\%, ranks \#7-8 SHAP, DMD instability coefficient +352.38).
        \textit{n=20,722 observations, h=8 months.}
    }
    \label{fig:ch3_feature_engineering}
    \end{figure}

\subsection{Ratio and Z-Score Features: Basic Dynamic Signals}

Stage 2 begins with compositional and temporal features designed to detect deviations from district-specific baselines. Unlike static article counts (which conflate baseline coverage levels with crisis-driven spikes), dynamic features quantify \textit{changes} relative to district-specific baselines. This section describes the two primary feature engineering approaches---ratio features and z-score standardisation---both designed to detect deviations from normal coverage patterns that may signal emerging crises invisible to AR persistence models.

\subsection{Macro-Category Taxonomy and Article Classification}

GDELT articles are classified into nine mutually non-exclusive thematic macro-categories capturing key dimensions of food security crises. Section 3.1.2 introduced these categories; here we formalize the classification process and rationale.

\subsubsection{Nine Macro-Categories}

    \begin{enumerate}
    \item\textbf{conflict\_category}: Armed conflict, violence, civil war, insurgency, terrorism, inter-communal clashes
    \item\textbf{displacement\_category}: Refugees, internally displaced persons (IDPs), migration, forced evacuations, population movements
    \item\textbf{economic\_category}: Market prices, inflation, unemployment, economic crisis, currency collapse, trade disruptions
    \item\textbf{food\_security\_category}: Hunger, malnutrition, famine, food assistance, agricultural failure, harvest loss
    \item\textbf{governance\_category}: Government policy, elections, political instability, corruption, institutional failures
    \item\textbf{health\_category}: Disease outbreaks, healthcare access, public health emergencies, epidemic response
    \item\textbf{humanitarian\_category}: Humanitarian aid, relief operations, NGO activities, donor appeals, emergency response
    \item\textbf{weather\_category}: Drought, floods, climate shocks, seasonal forecasts, extreme weather events
    \item\textbf{other\_category}: Articles not matching above categories (sports, culture, general news)
    \end{enumerate}

\textbf{Classification methodology}: Articles are assigned to categories via boolean keyword matching against GDELT themes (CAMEO taxonomy codes), full-text keyword presence, and location mentions. Each article can belong to multiple categories simultaneously (e.g., drought-driven displacement flags both \texttt{weather\_category} and \texttt{displacement\_category}). The nine-category taxonomy consolidates GDELT's 300+ fine-grained CAMEO codes into interpretable macro-categories aligned with humanitarian early warning frameworks \citep{fewsnet2024,ipc2024}.

\textbf{Rationale for macro-categories}: The taxonomy captures key drivers and manifestations of food insecurity documented in humanitarian literature: conflict disrupts agricultural production and market access, economic shocks reduce household purchasing power, weather events destroy crops and livestock, displacement indicates population stress, and humanitarian coverage signals international awareness of deteriorating conditions \citep{scientific_reports_2024_spatial,pmc_ethiopia_spatial_2024}. By tracking shifts in the composition of news coverage across these dimensions, we hypothesize that changes in narrative framing (e.g., increasing conflict coverage) may precede observable IPC deterioration.

\subsection{Ratio Features: Cross-Sectional Coverage Composition}

Ratio features quantify the relative emphasis of each category in a district's news coverage, capturing compositional shifts independent of absolute volume.

\subsubsection{Feature Definition}

For each district $i$ at time $t$, compute the proportion of total articles falling into each category $c$:

    \begin{equation}
\text{ratio}_{c,i,t} = \frac{\text{count}_{c,i,t}}{\sum_{k=1}^{9} \text{count}_{k,i,t}}
    \end{equation}

where:
    \begin{itemize}
    \item$\text{count}_{c,i,t}$ is the number of articles in category $c$ for district $i$ during month $t$
    \item Thedenominator sums across all nine categories, providing the total coverage volume
    \item Ratiosare normalised to $[0,1]$ and sum to 1 across categories for each observation
    \end{itemize}

Missing values (districts with zero articles in month $t$) are filled with 0, under the assumption that absence of coverage reflects either genuine low activity or media disinterest rather than missing data.

\subsubsection{Interpretation and Rationale}

Ratio features capture \textit{relative emphasis} rather than absolute volume. Consider two scenarios:

\textbf{Scenario A (Volume change, constant composition)}: District receives 100 articles in January (50\% conflict, 30\% economic, 20\% other) and 200 articles in February (50\% conflict, 30\% economic, 20\% other). Ratio features are \textit{unchanged} despite doubling volume, correctly identifying that coverage composition is stable.

\textbf{Scenario B (Composition shift, constant volume)}: District receives 100 articles in January (20\% conflict, 60\% economic, 20\% other) and 100 articles in February (60\% conflict, 20\% economic, 20\% other). Ratio features \textit{change dramatically} (conflict ratio increases from 0.20 to 0.60), capturing a shift in narrative framing toward violence even though total volume is constant.

This sensitivity to composition rather than volume is critical for detecting qualitative shifts in crisis dynamics. A sudden increase in conflict\_ratio (from, say, 0.10 to 0.50) signals that news narratives are refocusing from economic concerns to armed violence, potentially indicating escalation invisible to AR baselines predicated on IPC persistence.

\subsubsection{Advantages over Article Counts}

Raw article counts conflate three effects: (1) baseline coverage levels (large cities receive more coverage than remote districts regardless of crisis status), (2) media interest (elections, sporting events generate non-crisis coverage spikes), and (3) genuine crisis signals. Ratio features partial out baseline volume by normalising, isolating compositional changes that reflect genuine shifts in the nature of crises rather than media attention fluctuations.

\subsection{12-Month Sliding-Window Z-Score Standardisation}

Z-score features quantify how unusual current coverage levels are relative to recent historical baselines, detecting anomalous spikes or drops that may precede IPC changes.

\subsubsection{Feature Definition}

For each category $c$, district $i$, and time $t$, compute the rolling z-score:

    \begin{equation}
\text{z-score}_{c,i,t} = \frac{\text{count}_{c,i,t} - \mu_{c,i,t-12:t-1}}{\sigma_{c,i,t-12:t-1}}
    \end{equation}

where:
    \begin{itemize}
    \item$\mu_{c,i,t-12:t-1}$ is the mean article count for category $c$ over the 12-month window preceding time $t$ (months $[t-12, t-1]$)
    \item$\sigma_{c,i,t-12:t-1}$ is the standard deviation over the same window
    \item Standardisationproduces zero-mean, unit-variance features within each district's historical context
    \end{itemize}

\textbf{Minimum periods}: To prevent unreliable z-scores during early months where historical data is sparse, z-scores are only computed when at least 3 months of historical data are available ($\text{min\_periods} = 3$). Observations with fewer than 3 historical months receive $\text{z-score} = \text{NaN}$ and are excluded from training (approximately 8-10\% of observations, concentrated in the first year of the dataset).

\textbf{Handling constant features}: If all article counts in the 12-month window are identical ($\sigma = 0$), the z-score is set to 0, reflecting that current coverage is typical (no deviation from baseline).

\subsubsection{Interpretation: Detecting Anomalous Coverage Shifts}

Z-scores measure \textit{how many standard deviations} current coverage deviates from recent historical norms. Interpretation:

    \begin{itemize}
    \item$\text{z-score} = 0$: Current coverage is exactly at the 12-month mean (typical)
    \item$\text{z-score} = +2$: Current coverage is 2 standard deviations above recent baseline (anomalously high, potential early warning signal)
    \item$\text{z-score} = -2$: Current coverage is 2 standard deviations below baseline (anomalous silence, potentially concerning if coverage typically tracks crises)
    \item$|\text{z-score}| > 2$: Extreme deviation (occurs in approximately 5\% of observations under normal distribution, flagging genuine anomalies)
    \end{itemize}

Example: If a district's conflict coverage averages 20 articles/month with standard deviation 5, and February receives 35 conflict articles, the z-score is $(35-20)/5 = +3.0$, indicating an extreme spike that may precede conflict-driven food insecurity.

\subsubsection{Why 12-Month Windows?}

The 12-month rolling window balances three considerations:

    \begin{enumerate}
    \item\textbf{Seasonal adjustment}: Agricultural cycles, lean seasons, and harvest periods introduce annual seasonality in food security coverage. A 12-month window captures one full seasonal cycle, allowing z-scores to detect deviations from seasonal baselines rather than flagging predictable annual patterns as anomalies.

    \item\textbf{Baseline stability}: Shorter windows (e.g., 3 months) produce volatile baselines sensitive to single-month outliers, inflating false positives. Longer windows (e.g., 24 months) smooth over genuine shifts in coverage patterns, reducing sensitivity to emerging crises. Twelve months provides sufficient data for stable mean/std estimates while remaining responsive to recent trends.

    \item\textbf{Data availability}: With a 48-month dataset (2021-2024), a 12-month window provides at least 36 months of usable data per district after warm-up, sufficient for model training while excluding only the initial year where historical baselines are unavailable.
    \end{enumerate}

\subsubsection{Comparison to Ratio Features}

Ratio and z-score features capture complementary signals:

    \begin{itemize}
    \item\textbf{Ratio features}: Cross-sectional composition (``What proportion of coverage is conflict?''), invariant to volume changes, district-specific baseline simplicit
    \item\textbf{Z-score features}: Temporal anomaly detection (``Is current conflict coverage unusually high compared to the past 12 months?''), volume-sensitive, explicit historical baseline comparison
    \end{itemize}

A district may have high conflict\_ratio (0.60, indicating conflict dominates coverage) but low conflict\_z-score (0.0, indicating this is typical for that district). Conversely, low conflict\_ratio (0.10) with high conflict\_z-score (+2.0) signals an \textit{unusual spike} in conflict coverage even though conflict remains a minority theme. Both perspectives are informative for early warning.

\subsection{Feature Set Composition and Dimensionality}

The complete basic feature set combines ratio features, z-score features, and location metadata:

\subsubsection{Basic Feature Set (21 features)}

    \begin{enumerate}
    \item\textbf{Ratio features (9)}: One ratio per macro-category (conflict, displacement, economic, food security, governance, health, humanitarian, weather, other)

    \item\textbf{Z-score features (9)}: One z-score per macro-category (conflict, displacement, economic, food security, governance, health, humanitarian, weather, other)

    \item\textbf{Location metadata (3)}: Interpretable geographic context features
    \begin{itemize}
        \item\texttt{country\_data\_density}: Average articles per year for district's country (proxy for media penetration, infrastructure)
        \item\texttt{country\_baseline\_conflict}: Historical conflict coverage proportion for country (proxy for chronic vs acute conflict environments)
        \item\texttt{country\_baseline\_food\_security}: Historical food security coverage proportion for country (proxy for endemic vs episodic food insecurity)
    \end{itemize}
    \end{enumerate}

\textbf{Rationale for location features}: Rather than arbitrary label encoding (``country=1, 2, 3, ...''), location metadata captures \textit{why} a location exhibits certain risk characteristics. High \texttt{country\_data\_density} indicates strong media infrastructure, potentiallysimproving signal quality. High \texttt{country\_baseline\_conflict} identifies chronic conflict zones (e.g., Sudan, Somalia) where conflict\_ratio spikes may have different implications than in typically peaceful contexts (e.g., Malawi). These features allow models to learn context-dependent relationships between news signals and crisis onset.

\subsubsection{Training Subset: WITH\_AR\_FILTER}

As established in Section 3.3.6, Stage 2 models train exclusively on the 6,553 observations meeting the WITH\_AR\_FILTER criteria (IPC\textsubscript{t-1} $\leq$ 2 AND AR\_pred = 0). This filtered set includes the 1,427 AR failures (cases where IPC deteriorated to Phase 3+ despite the AR baseline predicting stability), representing the hardest-to-predict crises where news features must demonstrate value. The remaining 5,126 observations are non-crisis cases where both the previous period was stable and AR correctly predicted continued stability. This selective training strategy ensures news features learn to complement AR baselines by focusing on AR-difficult cases rather than replicating persistence patterns already captured by Lt and Ls.

\textbf{Classsimbalance in filtered set}: Among the 6,553 WITH\_AR\_FILTER observations, only 393 are crises (6.0\% crisis rate), creating severe class imbalance relative to the full dataset (25.9\% crisis rate). This imbalance arises because AR baselines already capture most easy-to-predict crises, leaving Stage 2 with predominantly non-crisis observations plus the hardest-to-predict minority of crises. XGBoost models address this via \texttt{scale\_pos\_weight} class balancing (Section 3.6.5), while mixed-effects models use \texttt{class\_weight=10} to penalize false negatives heavily.

\vspace{0.3cm}
\noindent\textit{This subsection established the basic dynamic feature construction framework, transforming raw GDELT article counts into informative signals via two complementary approaches. Ratio features (9 categories) capture cross-sectional coverage composition, detecting qualitative shifts in narrative framing independent of volume changes (e.g., increasing conflict\_ratio from 0.10 to 0.60 signals escalation even when total articles are constant). Z-score features (9 categories) use 12-month sliding windows to detect anomalous coverage spikes relative to district-specific historical baselines, identifying unusual events that may precede IPC deterioration (e.g., conflict\_z-score = +3.0 indicates extreme spike). Together with 3 interpretable location metadata features (country-level coverage density and baseline thematic composition), the basic 21-feature set provides compositional and temporal signals designed to complement AR persistence by detecting crisis dynamics invisible to lagged IPC values alone. All features train on the WITH\_AR\_FILTER subset (6,553 AR-difficult observations), ensuring news signals learn to rescue the 1,427 AR failures rather than replicate structural persistence.}
\vspace{0.3cm}


\subsection{Advanced Features: Regime and Mode Extraction}

The second component of Stage 2 feature engineering applies advanced time series methods---Hidden Markov Models (HMM) and Dynamic Mode Decomposition (DMD)---to extract latent features from ratio and z-score sequences. These methods aim to capture crisis dynamics invisible to static aggregations: HMM detects regime transitions (peaceful $\rightarrow$ crisis-prone narratives) even when article volumes remain constant, while DMD extracts temporal modes characterising escalation patterns and sustained intensity. Both operate on rolling 12-month windows, producing district-specific features that evolve over time. The resulting advanced feature set (35 features total) combines basic compositional/anomaly signals (21 features from Stage 2) with dynamic regime and mode characteristics (14 HMM/DMD features), enabling models to distinguish qualitative shifts in crisis narratives from quantitative coverage fluctuations.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\textwidth]{figures/ch03_methods/ch03_frequency_decomposition.pdf}
    \caption[Frequency Decomposition: AR Captures Low Frequency, Cascade Targets High Frequency]{
        \textbf{IPC crisis dynamics decompose into low-frequency persistence (captured by AR) and high-frequency shocks (targeted by Cascade).}
        Panel A shows observed IPC phase for Kenya Northern Pastoral Zone (260 periods), with 29 identified shock events (red stars exceeding 1$\sigma$ threshold). Panel B decomposes the low-frequency component (6-month moving average) representing structural persistence---slow-changing trends that AR baseline effectively captures (73.2\% recall). Panel C isolates high-frequency deviations (residuals from trend) representing rapid shocks---sudden spikes and drops where AR fails but Cascade rescues 249 cases (+17.4\% of AR failures). Panel D shows power spectral density via FFT: 82.9\% of signal power concentrates in low-frequency components (<0.17 cycles/month, periods >6 months), while 17.1\% resides in high-frequency components (>0.17 cycles/month, periods <6 months). Panel E summarizes the two-stage deployment logic: AR baseline handles low-frequency persistence efficiently, while advanced HMM/DMD features target high-frequency shocks where temporal autoregressive features fail. This frequency decomposition motivates the cascade architecture: rather than attempting to improve on AR's already-excellent performance on slow trends, Stage 2 focuses computational resources on the intrinsically harder but operationally critical rapid-onset crises (conflict escalations, coups, displacement shocks) that confound persistence models.
        \textit{Real data: Kenya Northern Pastoral Zone, n=260 periods. Low freq: 6-month centred MA. Shock threshold: 1$\sigma$ deviation.}
    }
    \label{fig:ch3_frequency_decomposition}
    \end{figure}

\subsection{Hidden Markov Models for Latent Regime Detection}

Hidden Markov Models posit that observed news coverage arises from unobservable latent states (regimes) with regime-specific emission distributions and probabilistic transitions between states. For food security forecasting, we hypothesize that districts alternate between \textit{Pre-Crisis} and \textit{Crisis-Prone} narrative regimes, with transitions signaling changes in underlying crisis dynamics before IPC classifications update.

\subsubsection{Model Specification: Binary Regime HMM}

This dissertation employs a \textbf{2-state Gaussian HMM} with asymmetric transition probabilities designed to capture crisis persistence. States are ordered by mean IPC values observed during historical regime occupancy, producing:

    \begin{itemize}
    \item\textbf{State 0 (Pre-Crisis)}: Lower average IPC during historical occupancy, indicating stable or improving conditions
    \item\textbf{State 1 (Crisis-Prone)}: Higher average IPC during historical occupancy, indicating persistent or worsening conditions
    \end{itemize}

\textbf{Asymmetric transition constraint}: Crisis regimes exhibit persistence due to asset depletion irreversibility, conflict entrenchment, and structural drivers that resist rapid reversal. To reflect this humanitarian reality, transition probabilities are constrained:

    \begin{equation}
P(\text{State}_{t+1} = \text{Crisis-Prone} \mid\text{State}_t = \text{Crisis-Prone}) \geq0.85
    \end{equation}

ensuring that once districts enter crisis-prone narrative regimes, they persist with at least 85\% probability. This constraint encourages stable regime identification by requiring sustained signal patterns rather than transient coverage fluctuations.

\textbf{Input features (4 core categories)}: HMM operates on ratio features for four crisis-core categories selected for high correlation with IPC outcomes:

    \begin{enumerate}
    \item\texttt{food\_security\_ratio}: Direct food insecurity signals (hunger, malnutrition, famine coverage)
    \item\texttt{conflict\_ratio}: Armed violence disrupting agricultural production and market access
    \item\texttt{economic\_ratio}: Price shocks, inflation, unemployment driving household food access constraints
    \item\texttt{weather\_ratio}: Drought, floods, climate shocks destroying crops and livestock
    \end{enumerate}

By restricting HMM to these four features rather than all nine categories, we reduce parameter dimensionality (improving convergence with limited data) while retaining signal most directly linked to food security crises \citep{fewsnet2024,ipc2024}.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\textwidth]{figures/ch03_methods/ch03_hmm_example.pdf}
    \caption[HMM Regime Detection: Sudan Conflict Escalation Example]{
        \textbf{Hidden Markov Model detects 8 regime transitions in Sudan Eastern Pastoral district (2022-2023).}
        Panel A shows conflict article coverage oscillating between peaceful (green, risk<0.5) and violent (red, risk$\geq$0.5) regimes with 8 detected transitions. The HMM captures rapid regime volatility during Sudan's civil war period, with frequent shifts between V$\rightarrow$P (violent to peaceful) and P$\rightarrow$V (peaceful to violent) states. Panel B shows corresponding food security coverage responding to regime dynamics. Panel C displays HMM transition risk oscillating around 0.5 threshold, with 8 crossings indicating regime changes. Panel D (State Interpretation) shows empirical transition probabilities: Peaceful regime appears 4/24 months (17\%, 0\% persistence, 100\% transition to violent); Violent regime dominates 20/24 months (83\%, 79\% persistence, 21\% transition to peaceful). This persistent violent regime (83\% occupancy) reflects sustained conflict throughout 2022-2023. The HMM operates on 12-month rolling windows, fitting 2-state Gaussian models to ratio features. HMM transition risk (feature \#5, importance 3.2\%) enables cascade to detect regime shifts that AR persistence models miss, contributing to 249 key saves in conflict zones (Sudan 59, Zimbabwe 77, DRC 40).
        \textit{Real data: Sudan Eastern Pastoral, n=24 months (2022-2023), 8 regime transitions detected, 83\% violent regime occupancy.}
    }
    \label{fig:ch3_hmm_example}
    \end{figure}

\subsubsection{Implementation: District-Level Pooling}

HMM models are estimated at the \textbf{district level}, fitting 1,322 separate 2-state Gaussian HMMs (one per news-dense district meeting the 200 articles/year threshold from Section 3.1.5). This district-level pooling strategy balances three considerations:

    \begin{enumerate}
    \item\textbf{Context specificity}: Each district has unique baseline coverage patterns, conflict histories, and crisis drivers. District-specific HMMs adapt to local dynamics rather than assuming universal transition probabilities across all African districts.

    \item\textbf{Data sufficiency}: Pooling at the country level would aggregate heterogeneous sub-national dynamics (e.g., peaceful southern regions with conflict-affected northern regions within the same country), obscuring district-level regime shifts. Pooling at the district level provides 48 monthly observations per HMM, sufficient for 2-state models with 4 input features.

    \item\textbf{Computational feasibility}: Training 1,322 independent 2-state HMMs is computationally tractable (approximately 2 hours on standard hardware), whereas fitting a single pooled HMM across all districts would assume identical transition dynamics globally, contradicting empirical heterogeneity \citep{scientific_reports_2024_spatial}.
    \end{enumerate}

\textbf{Rolling window extraction}: For each district's time series, HMM features are extracted using a 12-month rolling window. At each time point $t$, the HMM is fit to data from $[t-11, dots, t]$, producing features capturing current regime probabilities and transition risks based on the most recent year of coverage dynamics.

\subsubsection{Output Features (3 per feature type)}

Each HMM produces three features capturing regime state and dynamics:

    \begin{enumerate}
    \item\textbf{hmm\_[type]\_crisis\_prob}: $P(\text{State}_t = \text{Crisis-Prone})$, the posterior probability of currently occupying the crisis-prone regime. Values near 1 indicate the district is in a crisis-prone narrative state, while values near 0 indicate pre-crisis stability.

    \item\textbf{hmm\_[type]\_transition\_risk}: $\sum_{s=0}^{1} P(\text{State}_t = s) \cdot P(\text{State}_{t+1} = \text{Crisis-Prone} \mid \text{State}_t = s)$, the probability of transitioning to (or remaining in) the crisis-prone state in the next period, weighted by current state uncertainty. This captures forward-looking risk.

    \item\textbf{hmm\_[type]\_entropy}: $-\sum_{s=0}^{1} P(\text{State}_t = s) \cdot \log P(\text{State}_t = s)$, the Shannon entropy of the state distribution, quantifying regime uncertainty. High entropy ($\approx \log 2 = 0.693$) indicates ambiguous regime assignment (equally likely pre-crisis or crisis-prone), while low entropy ($\approx 0$) indicates confident regime classification.
    \end{enumerate}

where $[type] in \{\texttt{ratio}, \texttt{z-score}\}$ denotes whether HMM operates on ratio or z-score features, producing 6 HMM features total (3 per feature type).

\subsubsection{Convergence and Data Constraints}

HMM estimation via Expectation-Maximisation (EM) iterates until convergence (maximum 300 iterations, tolerance $10^{-3}$). Across the 1,322 district-level HMMs trained on rolling windows, convergence is achieved in \textbf{89.5\% of observations}. The 10.5\% convergence failures occur predominantly in districts with:

    \begin{itemize}
    \item Sparsecoverage (near the 200 articles/year threshold, producing high zero-inflation in monthly counts)
    \item Shortusable sequences (districts e\centering the dataset late in 2021-2024 with fewer than 12 months of historical data)
    \item Constantfeature values (districts with unchanging coverage composition across all months, producing singular covariance matrices)
    \end{itemize}

Observations where HMM convergence fails receive $\text{NaN}$ for all HMM features and are excluded from advanced feature set models (approximately 10\% of observations).


\subsection{Dynamic Mode Decomposition for Crisis Escalation Patterns}

Dynamic Mode Decomposition extracts spatial-temporal modes from multivariate time series, decomposing coverage dynamics into oscillatory patterns with characteristic frequencies and growth rates. Unlike HMM (which models discrete regime transitions), DMD identifies continuous temporal modes representing crisis escalation (growing modes with positive eigenvalues) versus decay (shrinking modes with negative eigenvalues).

\subsubsection{Mathematical Framework}

DMD decomposes a time series $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T]$ (where each $\mathbf{x}_t \in \mathbb{R}^d$ is a $d$-dimensional feature vector at time $t$) into dynamic modes via the linear approximation:

    \begin{equation}
\mathbf{x}_{t+1} \approx \mathbf{A} \mathbf{x}_t
    \end{equation}

where $\mathbf{A} \in \mathbb{R}^{d \times d}$ is the best-fit linear operator approximating temporal evolution. DMD constructs $\mathbf{A}$ via:

    \begin{equation}
\mathbf{A} = \mathbf{X}_2 \mathbf{X}_1^\dagger
    \end{equation}

where $\mathbf{X}_1 = [\mathbf{x}_1, \dots, \mathbf{x}_{T-1}]$, $\mathbf{X}_2 = [\mathbf{x}_2, \dots, \mathbf{x}_T]$, and $^\dagger$ denotes the Moore-Penrose pseudoinverse. The eigenvalues $\lambda_k$ and eigenvectors $\boldsymbol{\Phi}_k$ of $\mathbf{A}$ characterise dynamic modes:

    \begin{itemize}
    \item\textbf{Growth rate}: $\text{Re}(\log \lambda_k)$ quantifies exponential growth (positive) or decay (negative)
    \item\textbf{Frequency}: $|\text{Im}(\log \lambda_k)|/(2\pi)$ quantifies oscillation frequency (cycles per month)
    \item\textbf{Mode shape}: $\boldsymbol{\Phi}_k$ identifies which feature combinations (categories) co-evolve in mode $k$
    \end{itemize}

\subsubsection{Crisis-Focused Mode Filtering}

Standard DMD extracts all modes, including non-crisis-related patterns (seasonal agricultural cycles, electoral coverage spikes, sporting events). To isolate crisis-predictive dynamics, this dissertation implements a \textbf{3-step crisis mode filter}:

\textbf{Step 1: Growth threshold}. Retain only modes with positive growth rates exceeding $\lambda > 0.01$ (1\% monthly exponential growth), filtering out decaying or stable modes. Crisis escalation manifests as growing coverage intensification, not shrinking dynamics.

\textbf{Step 2: Frequency band}. Retain only modes with frequencies in $[1/6, 1/2]$ cycles/month (periods of 2-6 months), filtering out:
    \begin{itemize}
    \item Seasonal cycles (annual periods, frequency $\approx1/12$ cycles/month)
    \item High-frequency variation (weekly news cycles, frequency $> 1$ cycles/month)
    \end{itemize}
The 2-6 month band captures crisis escalation timescales documented in humanitarian early warning literature \citep{fewsnet2024}, isolating the temporal range most relevant for detecting emerging crisis dynamics.

\textbf{Step 3: Category weighting}. Compute crisis-relevance scores for each mode based on its loading on crisis-core categories:
    \begin{equation}
\text{CrisisWeight}_k = \frac{\sum_{c \in \text{CrisisCategories}} w_c |\Phi_{k,c}|}{\sum_{j=1}^{d} |\Phi_{k,j}|}
    \end{equation}
where crisis categories (conflict, food\_security, displacement, humanitarian) receive weight $w_c = 1.0$, economic receives $w_c = 0.5$ (contextual), and excluded categories receive $w_c = 0$. Modes with $\text{CrisisWeight}_k < 0.3$ are filtered out, retaining only modes dominated by crisis-relevant features.

\textbf{Input features (15 per feature type)}: DMD operates on 5 crisis-focused categories (4 core + economic) with 3 derivatives each (ratio/z-score, delta, 3-month trend), totaling 15 features per type. Including derivatives allows DMD to capture acceleration dynamics (increasing rate of change) beyond static levels.

\subsubsection{Output Features (4 per feature type)}

After crisis mode filtering, the dominant mode (highest crisis-weighted growth $simes$ amplitude) defines four features:

    \begin{enumerate}
    \item\textbf{dmd\_[type]\_crisis\_growth\_rate}: Exponential growth rate of the dominant crisis mode ($\text{Re}(\log \lambda)$). Positive values indicate escalating multi-category coverage intensification.

    \item\textbf{dmd\_[type]\_crisis\_instability}: Sum of crisis-weighted growth rates across all filtered modes, $\sum_k \text{Re}(\log \lambda_k) \cdot a_k \cdot \text{CrisisWeight}_k$, where $a_k$ is mode amplitude. High instability indicates multiple simultaneous escalating crisis patterns.

    \item\textbf{dmd\_[type]\_crisis\_frequency}: Oscillation frequency of the dominant crisis mode (cycles/month). Captures temporal periodicity of escalation dynamics.

    \item\textbf{dmd\_[type]\_crisis\_amplitude}: Amplitude of the dominant crisis mode ($|a_k|$), quantifying the strength of the temporal pattern.
    \end{enumerate}

where $[type] in \{\texttt{ratio}, \texttt{z-score}\}$ produces 8 DMD features total (4 per feature type).

\textbf{Zero-handling}: If no modes pass the 3-step crisis filter (indicating stable dynamics with no escalating crisis patterns), all DMD features are set to 0, reflecting the absence of crisis-predictive temporal modes.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\textwidth]{figures/ch03_methods/ch03_dmd_modes.pdf}
    \caption[DMD Temporal Modes: Crisis Escalation Patterns]{
        \textbf{Dynamic Mode Decomposition extracts three continuous temporal patterns from real Sudan Eastern Pastoral crisis data (420 periods).}
        Panel A (Growth Rate): Real eigenvalue identifies 21 escalation periods (growth>0) marked with red triangles, distinguishing exponential crisis intensification from stable dynamics. Panel B (Frequency): Oscillation period shows sparse crisis-driven spikes concentrated in early periods (2021-2022) and end of series (2024), capturing seasonal patterns. Panel C (Amplitude): Oscillation magnitude envelope shows limited volatility, with early-period spikes matching crisis clusters. Panel D (Summary): Convergence analysis shows 83.1\% Baum-Welch success rate, with 16.9\% failures discarded. Rare events focus: 18.7\% crisis rate (11,836/63,140 observations) with high humanitarian impact. DMD growth rate ranks 28 (2.1\% tree-based importance), complementing HMM regime transitions which capture discrete state changes. Crisis periods (270/420 in this district) shown as red background shading across panels A-C. Note: Instability coefficient (imaginary eigenvalue) removed from visualisation as values near zero throughout dataset.
        \textit{Real data: Sudan Eastern Pastoral, n=420 periods, 21 escalation events, 83.1\% convergence, rank-5 SVD truncation, 12-month windows.}
    }
    \label{fig:ch3_dmd_modes}
    \end{figure}

\subsubsection{Implementation and Convergence}

DMD uses Singular Value Decomposition (SVD) with rank-5 truncation (retaining top 5 modes by singular value energy) and regularization $\epsilon = 10^{-6}$ to stabilize pseudoinverse computation. Features are extracted on 12-month rolling windows (matching HMM). Across all districts and time points, DMD successfully extracts crisis modes in \textbf{83.1\% of observations}, with failures concentrated in districts with:

    \begin{itemize}
    \item Insufficient sequence length ($<$8 months of non-missing data in rolling window)
    \item Constant or near-constant feature values (producing rank-deficient $mathbf{X}_1$)
    \item High missing value rates (requiring excessive imputation)
    \end{itemize}

\subsection{Advanced Feature Set Composition}

Combining Stage 2 basic features (21) with Stage 3 regime/mode features (14) produces the \textbf{advanced feature set} (35 features total):

    \begin{enumerate}
    \item\textbf{Basic features (21)}: 9 ratio + 9 z-score + 3 location metadata (from Section 3.4.4)

    \item\textbf{HMM features (6)}: 3 ratio-based HMM (crisis\_prob, transition\_risk, entropy) + 3 z-score-based HMM

    \item\textbf{DMD features (8)}: 4 ratio-based DMD (growth\_rate, instability, frequency, amplitude) + 4 z-score-based DMD
    \end{enumerate}

\textbf{Coverage}: Advanced features are available for observations where both HMM convergence (89.5\%) and DMD mode extraction (83.1\%) succeed, producing approximately 74\% coverage (89.5\% $\times$ 83.1\% $\approx$ 74\%) across the WITH\_AR\_FILTER training set (6,553 observations). Models using advanced features train on the subset with complete feature availability.

\textbf{Missing value handling}: Observations with missing HMM or DMD features are excluded from advanced models but retained for basic models (21 features), ensuring all observations contribute to at least one model variant. This missing-completely-at-random (MCAR) assumption is validated by confirming that HMM/DMD convergence failures are not systematically associated with crisis outcomes (convergence failure rates are similar for crisis and non-crisis observations: 11.2\% vs 10.3\%, $\chi^2$ p=0.18).


\vspace{0.3cm}
\noindent\textit{This subsection established the advanced component of Stage 2 feature engineering, applying regime and mode extraction methods to detect latent crisis dynamics. Hidden Markov Models (HMM) fit 1,322 district-specific 2-state Gaussian HMMs with asymmetric crisis persistence constraints (P(Crisis$\rightarrow$Crisis)$\geq$0.85), operating on 4 core ratio/z-score categories to produce 6 features capturing regime probabilities, transition risks, and state uncertainty---convergence achieved in 89.5\% of observations. Dynamic Mode Decomposition (DMD) applies 3-step crisis filtering (growth$>$0.01, frequency in [1/6, 1/2], category weighting$>$0.3) to 15 derivatives per feature type, extracting 8 features quantifying escalation growth rates, multi-mode instability, oscillation frequencies, and pattern amplitudes---successful mode extraction in 83.1\% of observations. Together with basic features (21), the complete Stage 2 advanced feature set totals 35 features, available for approximately 74\% of observations where both HMM and DMD converge. These features enable models to distinguish qualitative regime transitions and temporal escalation patterns from simple compositional shifts, completing Stage 2's transformation of raw article counts into dynamic crisis signals targeting hard-to-predict cases invisible to AR persistence baselines.}
\vspace{0.3cm}

\section{Model Training and Evaluation Framework}

This section details the model training methodology applied to Stage 2 features for predicting hard-to-forecast cases missed by Stage 1 AR baselines. Training operates on the \textbf{WITH\_AR\_FILTER} subset (6,553 observations where IPC\textsubscript{t-1} $\leq$ 2 AND AR\_pred = 0), containing 393 crises and 6,160 non-crisis observations. This creates an extreme classsimbalance (6.0\% crisis rate) compared to the full dataset (25.9\% crisis rate), requiring specialised training strategies.

We employ two complementary modelling approaches---XGBoost and mixed-effects logistic regression---each with distinct strengths. XGBoost excels at capturing complex non-linear interactions and hierarchical feature importance through gradient-boosted decision trees, while mixed-effects models provide interpretable random coefficients quantifying geographic heterogeneity in feature effects. Both model families use identical 5-fold stratified spatial cross-validation (Section 3.2.2) to ensure comparable, spatially-unbiased performance estimates.

\subsection{XGBoost Gradient Boosting Models}

XGBoost (Extreme Gradient Boosting) constructs an ensemble of decision trees iteratively, where each tree corrects residual errors from preceding trees \citep{chen2016xgboost}. The final prediction aggregates contributions from all trees:

    \begin{equation}
\hat{y}_i = \sum_{k=1}^{K} f_k(\mathbf{x}_i)
    \end{equation}

where $f_k$ is the $k$-th tree and $K$ is the total number of trees (boosting rounds). For binary crisis prediction, the model outputs logistic-transformed probabilities $p_i = \sigma(\hat{y}_i)$ where $\sigma(z) = 1/(1 + e^{-z})$.

\subsubsection{Hyperparameter Optimisation via GridSearchCV}

To ensure fair comparison across feature sets of differing complexity (basic: 21 features, advanced: 35 features, ablation variants: 12-35 features), all XGBoost models undergo identical hyperparameter optimisation using GridSearchCV with stratified spatial cross-validation. The hyperparameter grid explores 3,888 combinations:

    \begin{itemize}
    \item\textbf{n\_estimators} $in \{100, 200, 300\}$: Number of boosting rounds
    \item\textbf{max\_depth} $in \{5, 7, 10\}$: Maximum tree depth (expanded upward from preliminary testing to allow complex models to utilise advanced features)
    \item\textbf{learning\_rate} $in \{0.01, 0.05, 0.1\}$: Step size for gradient descent
    \item\textbf{min\_child\_weight} $in \{1, 3, 5\}$: Minimum sum of instance weights per leaf (expanded downward for finer splits with more features)
    \item\textbf{subsample} $in \{0.7, 0.8\}$: Fraction of training samples per tree
    \item\textbf{colsample\_bytree} $in \{0.6, 0.8\}$: Fraction of features per tree
    \item\textbf{gamma} $in \{0, 0.5, 1\}$: Minimum loss reduction for split
    \item\textbf{reg\_alpha} $in \{0, 0.1\}$: L1 regularization
    \item\textbf{reg\_lambda} $in \{1, 2\}$: L2 regularization
    \end{itemize}

GridSearchCV selects hyperparameters maximising mean cross-validated AUC-ROC across the 5 spatial folds. This scoring metric prioritises discrimination (ranking crisis cases higher than non-crisis) rather than raw accuracy, which is inappropriate for the 6.0\% crisis prevalence in the WITH\_AR\_FILTER subset.

\subsubsection{Class Imbalance Handling}

The WITH\_AR\_FILTER subset exhibits severe classsimbalance (15.5:1 non-crisis to crisis ratio). XGBoost addresses this via \textbf{scale\_pos\_weight}, which weights positive class (crisis) loss contributions higher during training:

    \begin{equation}
\text{scale\_pos\_weight} = \frac{n_{\text{negative}}}{n_{\text{positive}}}
    \end{equation}

calculated per training fold to reflect fold-specifics imbalance. This weighting is equivalent to assigning crisis events higher importance in the gradient boosting objective, preventing the model from trivially achieving high accuracy by predicting only non-crisis.

\subsubsection{Model Variants}

Two XGBoost model variants are trained:

\textbf{Basic Model (21 features)}: Uses only ratio features (9), z-score features (9), and location metadata (3). Establishes baseline performance without HMM or DMD advanced features.

\textbf{Advanced Model (35 features)}: Adds HMM features (6) and DMD features (8) to the basic set. Tests how latent regime transitions and temporal modes contribute interpretable crisis dynamics beyond compositional features.

Both models train on the subset of WITH\_AR\_FILTER observations where HMM convergence and DMD mode extraction succeeded (approximately 74\% coverage). Missing advanced features are imputed with country-median values (if available) then global median (fallback), preserving all observations for fair comparison.

\subsubsection{Cross-Validation and Model Selection}

For each fold $f in \{0, 1, 2, 3, 4\}$:

    \begin{enumerate}
    \item Train XGBoost on 4 folds using optimised hyperparameters
    \item Predicton held-out fold $f$
    \item Compute fold-specific metrics: AUC-ROC, Brier score, log loss
    \item Save fold model and feature importance rankings
    \end{enumerate}

After cross-validation, a \textbf{final model} is trained on all data using best hyperparameters for deployment. Cross-validated performance estimates (mean $\pm$ std across folds) provide unbiased generalisation metrics, while the final model maximises predictive power for Stage 3 cascade integration.

\subsection{Mixed-Effects Logistic Regression}

Mixed-ef\-fects mod\-els ex\-tend stan\-dard lo\-gis\-tic re\-gres\-sion by in\-cor\-po\-rat\-ing \textbf{ran\-dom ef\-fects}---co\-ef\-fi\-cients that vary by geo\-graph\-ic group (coun\-try or dis\-trict).

These cap\-ture sys\-tem\-at\-ic het\-ero\-gene\-ity in how fea\-tures pre\-dict cri\-sis risk across re\-gions \citep{gelman2006data}.

The model specification is:

    \begin{equation}
\log \frac{p_{r,t}}{1 - p_{r,t}} = \underbrace{\boldsymbol{\beta}^T \mathbf{X}_{r,t}}_{\text{Fixed effects}} + \underbrace{\alpha_g + \mathbf{b}_g^T \mathbf{Z}_{r,t}}_{\text{Random effects}}
    \end{equation}

where:
    \begin{itemize}
    \item$boldsymbol{\beta}$: Fixed effect coefficients (global patterns across all regions)
    \item$mathbf{X}_{r,t}$: All features (ratio, z-score, location, HMM, DMD)
    \item$\alpha_g$: Random intercept for group $g$ (country or district baseline risk)
    \item$mathbf{b}_g$: Random slopes for key signals $mathbf{Z}_{r,t}$ (group-specific feature effects)
    \item$\mathbf{Z}_{r,t} \subseteq \mathbf{X}_{r,t}$: Subset of features with random slopes (conflict\_ratio, food\_security\_ratio)
    \end{itemize}

Random effects are assumed normally distributed: $\alpha_g \sim \mathcal{N}(0, \sigma_\alpha^2)$ and $b_{g,j} \sim \mathcal{N}(0, \sigma_{b_j}^2)$. Large variance $\sigma_{b_j}^2$ indicates substantial geographic heterogeneity in feature $j$'s effect, informing selective deployment strategies.

\subsubsection{Random Effects Grouping Level}

The random effects grouping level (district vs country) is selected dynamically based on data sufficiency:

    \begin{itemize}
    \item\textbf{District-level} (preferred): Used if $\geq$ 50\% of districts have $\geq$10 observations, enabling fine-grained geographic variation estimates aligned with the research proposal's district-level focus.
    \item\textbf{Country-level} (fallback): Used if district coverage is insufficient, aggregating heterogeneity at the coarser country scale.
    \end{itemize}

This adaptive strategy balances statistical power (sufficient data per group for stable random effect estimates) with geographic granularity.

\subsubsection{Random Slope Selection}

Random slopes capture feature effects that vary geographically. Due to computational constraints (random slopes increase model complexity quadratically with number of features), we select 2 key signals based on crisis-predictive priority:

    \begin{enumerate}
    \item\textbf{conflict\_ratio}: Conflict news composition (highest priority for crisis prediction)
    \item\textbf{food\_security\_ratio}: Food security mentions (second priority)
    \end{enumerate}

These features receive both fixed effects (global average impact) and random slopes (country/district-specific deviations), while remaining features receive only fixed effects.

\subsubsection{Class Weighting for Imbalance Handling}

Mixed-effects models address class imbalance via observation weighting:

    \begin{equation}
w_i = \begin{cases}
    10 & \text{if } y_i = 1 \text{ (crisis)} \\
    1 & \text{if } y_i = 0 \text{ (non-crisis)}
    \end{cases}
    \end{equation}

This 10:1 crisis weighting mirrors the cost-sensitive evaluation framework (10$simes$FN + 1$simes$FP), prioritising recall over precision in humanitarian contexts where missing a crisis carries catastrophic consequences.

\subsubsection{Model Fitting and Convergence}

Mixed-effects models are fitted using \texttt{glmer} (Generalised Linear Mixed-Effects Regression) from the \texttt{lme4} R package with the following configuration:

    \begin{itemize}
    \item\textbf{Optimiser}: bobyqa (Bound Optimisation BY Quadratic Approximation)
    \item\textbf{Max iterations}: 100,000 (increased from default for convergence with weighted observations)
    \item\textbf{Integration}: Laplace approximation (nAGQ=0, faster than adaptive Gauss-Hermite quadrature)
    \item\textbf{Singular fit handling}: Warnings suppressed (tolerance $10^{-4}$) to accommodate boundary random effect variances in low-data groups
    \end{itemize}

If district-level models fail to converge, automatic fallback to country-level random effects occurs. If country-level models fail, unweighted models are attempted as final fallback.

\subsubsection{Model Variants}

Four mixed-effects model variants parallel the XGBoost design:

    \begin{enumerate}
    \item\textbf{pooled\_ratio}: Ratio features (9) + location (3) only
    \item\textbf{pooled\_z-score}: Z-score features (9) + location (3) only
    \item\textbf{pooled\_ratio\_hmm\_dmd}: Ratio (9) + location (3) + HMM-ratio (3) + DMD-ratio (4) = 19 features
    \item\textbf{pooled\_z-score\_hmm\_dmd}: Z-score (9) + location (3) + HMM-z-score (3) + DMD-z-score (4) = 19 features
    \end{enumerate}

These variants enable direct comparison of ratio vs z-score feature types and assessment of HMM/DMD contributions to interpretability within the mixed-effects framework.

\subsection{Ablation Study Design}

Ablation studies systematically remove feature groups to isolate their marginal contributions \citep{meyes2019ablation}. Eight ablation models test specific hypotheses about feature value:

    \begin{enumerate}
    \item\textbf{ratio\_location} (12 features): Ratio (9) + location (3). Baseline without z-scores or advanced features.

    \item\textbf{z-score\_location} (12 features): Z-score (9) + location (3). Tests z-score baseline without ratio or advanced features.

    \item\textbf{ratio\_z-score\_location} (21 features): All basic features. Tests combined ratio + z-score performance.

    \item\textbf{ratio\_z-score\_dmd\_location} (29 features): Basic (21) + DMD (8). Tests DMD contribution without HMM.

    \item\textbf{ratio\_z-score\_hmm\_location} (27 features): Basic (21) + HMM (6). Tests HMM contribution without DMD.

    \item\textbf{ratio\_hmm\_ratio\_location} (18 features): Ratio (9) + HMM-ratio (3) + DMD-ratio (0) + location (3). Tests ratio-based features with HMM.

    \item\textbf{z-score\_hmm\_z-score\_location} (18 features): Z-score (9) + HMM-z-score (3) + DMD-z-score (0) + location (3). Tests z-score-based features with HMM.

    \item\textbf{ratio\_hmm\_dmd\_location} (22 features): Ratio (9) + HMM-ratio (3) + DMD-ratio (4) + location (3) + HMM-z-score (3) + DMD-z-score (0). Tests ratio with both advanced methods.
    \end{enumerate}

Each ablation model undergoes identical hyperparameter optimisation (3,888 combinations) and 5-fold stratified spatial cross-validation, ensuring fair comparison. Ablation results answer:

    \begin{itemize}
    \item\textbf{Q1}: Do z-scoressimprove over ratio features alone? (Compare models 1 vs 3)
    \item\textbf{Q2}: Does HMM provide marginal value? (Compare models 3 vs 5)
    \item\textbf{Q3}: Does DMD provide marginal value? (Compare models 3 vs 4)
    \item\textbf{Q4}: Do HMM + DMD together outperform either alone? (Compare models 4, 5 vs advanced 35-feature model)
    \item\textbf{Q5}: Which feature type (ratio vs z-score) benefits more from advanced features? (Compare models 6 vs 7)
    \end{itemize}

\subsection{Threshold Optimisation Strategies}

Binary predictions require converting continuous probabilities $p_i \in [0, 1]$ to binary labels $\hat{y}_i \in \{0, 1\}$ via thresholding: $\hat{y}_i = \mathbb{1}[p_i \geq\tau]$. The threshold $\tau$ determines the precision-recall trade-off. This dissertation evaluates four threshold selection strategies:

\textbf{Youden's J Index}: Maximises $J = \text{TPR} - \text{FPR}$ (sensitivity + specificity - 1), balancing true positive rate and false positive rate. Optimal for maximising overall classification performance.

\textbf{F1-Maximising}: Selects $\tau$ maximising $F_1 = 2 \cdot\text{precision} \cdot\text{recall} / (\text{precision} + \text{recall})$, the harmonic mean of precision and recall. Balanced metric for imbalanced data.

\textbf{Balanced Precision-Recall}: Finds $\tau$ where precision $\approx$ recall, achieving symmetry between positive predictive value and sensitivity.

\textbf{High-Recall ($\geq$ 0.90)}: Selects the highest $\tau$ achieving recall $\geq$ 0.90, prioritising crisis detection (minimising false negatives) at the cost of precision. Aligned with humanitarian early warning priorities where missing a crisis is catastrophic.

Cross-validation reports metrics at all four thresholds, but the \textbf{high-recall threshold} is prioritised for cascade integration (Section 3.6) to maximise AR failure rescues.

\subsection{Evaluation Metrics}

Model performance is assessed using six metric categories:

\textbf{Discrimination metrics} (threshold-independent):
    \begin{itemize}
    \item AUC-ROC: Area under Receiver Operating Characteristic curve (probability model ranks crisis cases higher than non-crisis)
    \end{itemize}

\textbf{Classification metrics} (threshold-dependent):
    \begin{itemize}
    \item Precision: $\text{TP}/(\text{TP} + \text{FP})$ (fraction of predicted crises that are true crises)
    \item Recall (Sensitivity): $\text{TP}/(\text{TP} + \text{FN})$ (fraction of true crises correctly identified)
    \item Specificity: $\text{TN}/(\text{TN} + \text{FP})$ (fraction of non-crises correctly identified)
    \item F1 Score: $2 \cdot\text{precision} \cdot\text{recall} / (\text{precision} + \text{recall})$
    \item Balanced Accuracy: $(\text{recall} + \text{specificity})/2$
    \end{itemize}

\textbf{Calibration metrics}:
    \begin{itemize}
    \item Brier Score: Mean squared error between predicted probabilities and binary outcomes, $\frac{1}{N}\sum_{i=1}^{N}(p_i - y_i)^2$
    \item Log Loss: Negative log-likelihood, $-\frac{1}{N}\sum_{i=1}^{N}[y_i \log p_i + (1-y_i)\log(1-p_i)]$
    \end{itemize}

\textbf{Cost-sensitive metric}:
    \begin{itemize}
    \item Total Cost: $10 simes \text{FN} + 1 simes \text{FP}$, reflecting asymmetric humanitarian costs (missing crisis 10$simes$ worse than false alarm)
    \end{itemize}

\textbf{Confusion matrix elements}: True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)

\textbf{Geographic stratification}: All metrics computed overall and stratified by country, enabling identification of geographic performance variation.

\subsection{Feature Importance Extraction}

XGBoost provides gain-based feature importance, quantifying each feature's contribution to model splits:

    \begin{equation}
\text{Importance}_j = \sum_{k=1}^{K} \sum_{s \in \text{splits}(f_k, j)} \text{Gain}(s)
    \end{equation}

where $\text{Gain}(s)$ is the loss reduction from split $s$ on feature $j$ in tree $k$. Importance scores are normalised to sum to 1.0, enabling cross-model comparison.

Mixed-effects models provide fixed effect coefficients $boldsymbol{\beta}$ (global featuresimpacts) and random effect variances $\sigma_{b_j}^2$ (geographic heterogeneity). Features with large $|\beta_j|$ have strong global effects; features with large $\sigma_{b_j}^2$ exhibit high geographic variation, suggesting context-dependent deployment strategies.

Featuresimportance rankings inform interpretability analysis (Section 3.7) and guide operational deployment recommendations.

\vspace{0.3cm}
\noindent\textit{This section established the Stage 2 model training framework for predicting crises missed by Stage 1 AR baselines. XGBoost models undergo hyperparameter optimisation via GridSearchCV with 3,888 combinations evaluated using 5-fold stratified spatial cross-validation, maximising AUC-ROC while handling 15.5:1 classsimbalance through scale\_pos\_weight. Two XGBoost variants (basic: 21 features, advanced: 35 features) and four mixed-effects logistic regression variants (ratio, z-score, ratio+HMM+DMD, z-score+HMM+DMD) enable comparison of feature types and advanced method contributions. Mixed-effects models incorporate country or district random effects (adaptive selection based on data sufficiency) with random slopes on conflict\_ratio and food\_security\_ratio, capturing geographic heterogeneity in feature effects. Observation weighting (10:1 crisis:non-crisis) aligns training with humanitarian cost asymmetries. Nine ablation models systematically test marginal contributions of z-scores, HMM, and DMD features through controlled feature removal. Four threshold selection strategies (Youden's J, F1-max, balanced P=R, high-recall$\geq$ 0.90) optimise precision-recall trade-offs for different operational priorities, with high-recall prioritised for cascade integration. Comprehensive evaluation uses six metric categories (discrimination, classification, calibration, cost-sensitive, confusion matrix, geographic stratification) to assess model performance across spatial folds and countries. Featuresimportance extraction from XGBoost (gain-based) and mixed-effects models (fixed coefficients, random effect variances) quantifies feature contributions and geographic heterogeneity, informing interpretability analysis and operational deployment strategies.}
\vspace{0.3cm}

\section{Two-Stage Framework Integration}

This section describes how Stage 1 (AR baseline) and Stage 2 (news-based models) are integrated into a unified cascade ensemble. The cascade framework is designed for \textbf{selective deployment}: Stage 2 models operate only on cases where Stage 1 predicted non-crisis (AR=0), reducing computational costs while maximising marginal predictive value.

\subsection{Cascade Decision Logic}

The cascade employs \textbf{simple binary override logic}, integrating binary predictions from both stages without adaptive thresholds or probabilistic weighting. The decision rule operates as follows:

    \begin{equation}
\hat{y}_{\text{cascade}} = \begin{cases}
    1 & \text{if } \hat{y}_{\text{AR}} = 1 \text{ (trust AR's crisis prediction)} \\
    \hat{y}_{\text{Stage2}} & \text{if } \hat{y}_{\text{AR}} = 0 \text{ (defer to Stage 2)}
    \end{cases}
    \end{equation}

where $\hat{y}_{\text{AR}}$ is the binary Stage 1 prediction (using balanced precision-recall optimal threshold $\tau = 0.629$) and $\hat{y}_{\text{Stage2}}$ is the binary Stage 2 prediction (using Youden's J threshold from the advanced XGBoost model trained on WITH\_AR\_FILTER observations).

This logic reflects three principles:

\textbf{Principle 1: Trust AR's positive predictions}. When Stage 1 predicts crisis ($\hat{y}_{\text{AR}} = 1$), the cascade accepts this prediction without override. AR baselines achieve high precision (0.732) on crisis predictions, reflecting strong autocorrelation in IPC transitions. Overriding these predictions would introduce unnecessary false positives.

\textbf{Principle 2: Refine AR's negative predictions}. When Stage 1 predicts no crisis ($\hat{y}_{\text{AR}} = 0$), the cascade defers to Stage 2. This subset (6,553 observations where IPC\textsubscript{t-1} $\leq$ 2 AND AR$=0$) contains all 1,427 AR-missed crises (false negatives the AR baseline failed to catch). Stage 2 models target these hard-to-predict cases using news features invisible to AR baselines.

\textbf{Principle 3: Simple binary logic}. The cascade uses binary predictions directly, avoiding complex probability fusion, adaptive thresholds, or meta-learning. While Stage 2 models internally optimise thresholds (Section 3.5.4), the cascade integration operates purely on binary labels. This simplicity ensures interpretability and operational deployability.

\subsection{Override Mechanism and Coverage}

Stage 2 predictions are available for 6,553 observations (31.6\% of the full dataset), corresponding exactly to the WITH\_AR\_FILTER subset where:
    \begin{itemize}
    \item Previous IPC status: IPC\textsubscript{t-1} $\leq$ 2 (not already in crisis)
    \item AR prediction: $\hat{y}_{\text{AR}} = 0$ (AR predicts no future crisis)
    \end{itemize}

Within these 6,553 override candidates, Stage 2 predicts crisis ($\hat{y}_{\text{Stage2}} = 1$) for 1,761 observations (26.9\% override rate). These 1,761 overrides constitute the cascade's incremental contribution beyond the AR baseline. The remaining 4,792 observations where Stage 2 predicts $\hat{y}_{\text{Stage2}} = 0$ confirm the AR baseline's no-crisis prediction.

For the 14,169 observations where AR predicts crisis ($\hat{y}_{\text{AR}} = 1$) or where IPC$_{r,t} \geq3$ (already in crisis), the cascade prediction equals the AR prediction without invoking Stage 2.

\subsection{Key Saves: Quantifying Stage 2 Value}

The primary metric for evaluating cascade performance is \textbf{key saves}---AR-missed crises that the cascade successfully predicts. Formally:

    \begin{equation}
\text{Key Save} = \begin{cases}
    1 & \text{if } y_{r,t+h} = 1 \text{ AND } \hat{y}_{\text{AR}} = 0 \text{ AND } \hat{y}_{\text{cascade}} = 1 \\
    0 & \text{otherwise}
    \end{cases}
    \end{equation}

Key saves represent the operational value-add of Stage 2: these are true crisis events that would have been missed if relying solely on Stage 1 AR baselines but are now caught through news-based early warning signals.

Across the full dataset (20,722 observations), the cascade achieves:
    \begin{itemize}
    \item\textbf{Total key saves}: 249 crises
    \item\textbf{AR baseline misses}: 1,427 crises (false negatives)
    \item\textbf{Key save rate}: 17.4\% (249 / 1,427)
    \end{itemize}

This means Stage 2 successfully rescues 17.4\% of the crises that Stage 1 failed to predict. The remaining 1,178 AR-missed crises (82.6\%) remain false negatives even after cascade integration. Cascade failure analysis (Chapter 5) reveals these still-missed cases exhibit systematic news coverage deficiency---median 74 articles/month compared to 121 for rescued cases (64\% less coverage)---demonstrating a fundamental constraint: news-based early warning cannot rescue crises in news deserts (remote pastoral areas, peripheral regions) lacking sufficient media coverage. This constraint motivates future NLP enhancements: expanding text corpora through social media monitoring, community radio transcripts, humanitarian situation reports, and multilingual sources to address coverage gaps in underreported regions.

\subsection{Performance Impact: Recall vs Precision Trade-Off}

Integrating Stage 2 predictions improves cascade recall (crisis detection rate) but reduces precision (positive predictive value), reflecting the fundamental recall-precision trade-off in imbalanced classification. Overall performance changes:

\textbf{Recallsimprovement}:
    \begin{equation}
\Delta \text{Recall} = 0.779 - 0.732 = +0.047 \text{ (4.7 percentage points)}
    \end{equation}

The cascade catches 77.9\% of all crises, compared to 73.2\% for the AR baseline alone. This 4.7-pointsimprovement corresponds directly to the 249 key saves.

\textbf{Precision reduction}:
    \begin{equation}
\Delta \text{Precision} = 0.585 - 0.732 = -0.147 \text{ (14.7 percentage points)}
    \end{equation}

Cascade precision drops to 58.5\%, meaning 41.5\% of cascade crisis predictions are false alarms. This reduction reflects the increased false positive rate from Stage 2 overrides: of the 1,761 overrides, 1,512 are false positives (85.9\%) and only 249 are true positives (14.1\%).

\textbf{F1 score change}:
    \begin{equation}
\Delta F_1 = 0.668 - 0.732 = -0.064
    \end{equation}

The F1 score decreases slightly, as the precision reduction outweighs the recall gain in the harmonic mean.

\textbf{Confusion matrix transformation}:

    \begin{table}[htbp]
    \centering
    \begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{TP} & \textbf{TN} & \textbf{FP} & \textbf{FN} \\
\midrule
AR Baseline & 3,895 & 13,973 & 1,427 & 1,427 \\
Cascade Ensemble & 4,144 & 12,461 & 2,939 & 1,178 \\
\midrule
Change & +249 & -1,512 & +1,512 & -249 \\
\bottomrule
    \end{tabular}
\caption{Confusion matrix comparison: AR baseline vs cascade ensemble. The cascade gains 249 true positives (key saves) but incurs 1,512 additional false positives.}
\label{tab:cascade_confusion}
    \end{table}

This trade-off is deliberate and aligned with humanitarian early warning priorities (Section 3.2.3). The cost function $10 simes \text{FN} + 1 simes \text{FP}$ reflects that missing a crisis (false negative) carries catastrophic humanitarian consequences 10 times worse than a false alarm (false positive). Under this asymmetric cost structure, rescuing 249 crises justifies 1,512 additional false alarms.

\subsection{Geographic Distribution of Key Saves}

Key saves exhibit substantial geographic heterogeneity, with 10 countries accounting for 249 total saves:

    \begin{itemize}
    \item Zimbabwe: 77 saves (30.9\% of total)
    \item Sudan: 59 saves (23.7\%)
    \item DemocraticRepublic of the Congo: 40 saves (16.1\%)
    \item Nigeria: 27 saves (10.8\%)
    \item Mozambique: 15 saves (6.0\%)
    \item Mali: 12 saves (4.8\%)
    \item Kenya: 8 saves (3.2\%)
    \item Ethiopia: 6 saves (2.4\%)
    \item Malawi: 3 saves (1.2\%)
    \item Somalia: 2 saves (0.8\%)
    \end{itemize}

Three countries (Zimbabwe, Sudan, DRC) account for 70.7\% of all key saves, suggesting that Stage 2's marginal value is highly context-dependent. Countries with high key save counts exhibit characteristics conducive to news-based prediction: active conflict, political instability, and robust GDELT coverage capturing crisis escalation signals invisible to AR baselines.

Conversely, 8 countries (50\% of the 18-country sample) show zero key saves, indicating contexts where news features provide no marginal value beyond AR persistence. This geographic heterogeneity informs selective deployment strategies (Chapter 5).

\subsection{Model Selection for Cascade Integration}

The cascade integrates the \textbf{advanced XGBoost model} (35 features: 21 basic + 6 HMM + 8 DMD) as the Stage 2 component. This model was selected based on:

    \begin{enumerate}
    \item\textbf{Performance}: Highest cross-validated AUC-ROC among all Stage 2 variants
    \item\textbf{Feature richness}: Incorporates regime transitions (HMM) and temporal modes (DMD) beyond compositional features
    \item\textbf{Generalisability}: Trained with spatial cross-validation and extensive hyperparameter optimisation (3,888 combinations)
    \item\textbf{Robustness}: 74\% coverage where both HMM and DMD features are available, with country-mediansimputation for missing values
    \end{enumerate}

Alternative Stage 2 models (basic XGBoost with 21 features, mixed-effects logistic regression variants) were evaluated but not selected for cascade integration. The advanced XGBoost model balances predictive performance with feature interpretability, enabling post-hoc analysis of which news signals drive key saves (Section 3.7).

\vspace{0.3cm}
\noindent\textit{This section established the two-stage cascade framework integrating Stage 1 (AR baseline) and Stage 2 (news-based models) through simple binary override logic: if AR predicts crisis, trust it; if AR predicts no crisis, defer to Stage 2. The cascade operates on 6,553 override candidates (31.6\% of full dataset) where IPC\textsubscript{t-1} $\leq$ 2 AND AR$=0$, with Stage 2 overriding 1,761 cases (26.9\% override rate). The key metric is \textbf{key saves}---AR-missed crises successfully predicted by the cascade---totaling 249 saves from 1,427 AR misses (17.4\% rescue rate). Cascade integrationsimproves recall from 73.2\% to 77.9\% (+4.7 points) but reduces precision from 73.2\% to 58.5\% (-14.7 points), incurring 1,512 additional false positives to rescue 249 true crises. This recall-precision trade-off is justified by humanitarian cost asymmetries (10$simes$FN + 1$simes$FP), where missing crises carries catastrophic consequences. Key saves exhibit geographic heterogeneity: Zimbabwe (77), Sudan (59), and DRC (40) account for 70.7\% of total saves, while 8 countries show zero saves, informing selective deployment strategies. The advanced XGBoost model (35 features) serves as Stage 2, selected for highest cross-validated AUC-ROC, feature richness (HMM + DMD), and generalisability through spatial CV and hyperparameter optimisation.}
\vspace{0.3cm}

\section{Interpretability Framework}

Model interpretability enables understanding \textit{which} news signals drive predictions and \textit{how} their effects vary geographically, informing operational deployment strategies. This section establishes a triangulation framework combining three complementary interpretability methods with ablation studies for model evaluation:

\textbf{Interpretability Methods} (feature-level attribution):
\begin{enumerate}
\item \textbf{XGBoost gain-based feature importance}: Measures how frequently features are used to partition tree nodes (stratification utility)
\item \textbf{SHAP (SHapley Additive exPlanations)}: Quantifies marginal contribution of each feature to individual predictions (attribution)
\item \textbf{Mixed-effects decomposition}: Separates global patterns (fixed effects) from country-specific variation (random effects)
\end{enumerate}

\textbf{Model Evaluation} (feature group contribution):
\begin{enumerate}
\setcounter{enumi}{3}
\item \textbf{Ablation studies}: Isolates marginal contributions of entire feature groups through systematic removal
\end{enumerate}

These methods provide complementary perspectives: gain-based importance identifies features useful for stratification, SHAP reveals features driving prediction variance, mixed-effects quantifies geographic heterogeneity, and ablation studies test marginal group contributions. Convergence across interpretability methods and validation through ablation studies provides robust evidence for feature rankings and deployment recommendations.

\subsection{XGBoost Gain-Based Feature Importance}

XGBoost provides gain-based feature importance, quantifying each feature's contribution to reducing prediction error across all tree splits \citep{chen2016xgboost}. For feature $j$,simportance is computed as:

    \begin{equation}
\text{Importance}_j = \sum_{k=1}^{K} \sum_{s \in \text{Splits}(f_k, j)} \text{Gain}(s)
    \end{equation}

where $\text{Gain}(s)$ is the loss reduction from split $s$ on feature $j$ in tree $k$, summed across all $K$ trees. Importance scores are normalised to sum to 1.0, enabling cross-model comparison.

\subsubsection{Top Features: Advanced XGBoost Model}

The advanced XGBoost model (35 features) identifies the following top 10 features by gain-basedsimportance:

    \begin{enumerate}
    \item\textbf{country\_data\_density} (0.133): GDELT article volume per capita, proxy for news coverage intensity
    \item\textbf{country\_baseline\_conflict} (0.093): Country-average conflict news prevalence
    \item\textbf{country\_baseline\_food\_security} (0.067): Country-average food security news prevalence
    \item\textbf{other\_ratio} (0.033): Uncategorised news composition
    \item\textbf{hmm\_ratio\_transition\_risk} (0.032): HMM regime transition probability (peaceful $\rightarrow$ crisis)
    \item\textbf{health\_ratio} (0.029): Health crisis news composition
    \item\textbf{displacement\_z-score} (0.026): Displacement news z-score anomaly
    \item\textbf{weather\_ratio} (0.026): Climate/weather event news composition
    \item\textbf{food\_security\_z-score} (0.025): Food security news z-score anomaly
    \item\textbf{hmm\_ratio\_crisis\_prob} (0.025): HMM crisis state probability
    \end{enumerate}

Three location metadata features (country\_data\_density, country\_baseline\_conflict, country\_baseline\_food\_security) account for 29.3\% of total tree-based importance, dominating compositional and temporal features in split frequency. However, this reflects their role as stratification infrastructure rather than marginal predictive contribution: SHAP analysis (Section 3.6.4.2) reveals location features contribute only 2.6\% of marginal attribution, while z-score news features drive 74.7\% of predictions. The tree-based metric conflates \textit{how often} features create splits (stratification utility) with \textit{how much} they drive predictions (marginal impact). The highest-ranking compositional feature (other\_ratio, rank 4) contributes 3.3\% tree-based importance.

Among advanced features, HMM transition risk ranks 5th (3.2\%simportance), demonstrating genuine predictive value from latent regime dynamics. DMD features rank lower (crisis\_growth\_rate at rank 28: 2.1\%, crisis\_amplitude at rank 30: 1.9\%), suggesting temporal modes contribute less than regime transitions.

\subsubsection{Feature Importance Insights}

Feature importance rankings reveal four key insights:

\textbf{Insight 1: Location metadata dominates tree splits, not marginal predictions}. The top 3 features in tree-based importance are all country-level baselines (29.3\% combined), not local news signals. However, this overstates their predictive contribution: SHAP analysis (Section 3.6.4.2) reveals location features contribute only 2.6\% of marginal attribution---an 11.3$\times$ overstatement. Location features serve as \textbf{stratification infrastructure}: they partition trees frequently to enable context-specific learning (Somalia $\neq$ Zimbabwe), but z-score news features drive 74.7\% of actual predictions within geographic strata. This demonstrates that \textit{where} a district is located enables stratification, while \textit{what} news signals emerge drives prediction variance.

\textbf{Insight 2: HMM features show higher importance than DMD}. HMM transition risk ranks 5th (3.2\%), while the highest DMD feature ranks 28th (2.1\%). This aligns with ablation study findings (Section 4.4): HMM adds +0.007 AUC compared to DMD's +0.002 AUC. Regime transitions (peaceful $\rightarrow$ crisis) provide stronger signals than temporal mode decomposition for the AR-filtered cases targeted by Stage 2.

\textbf{Insight 3: Ratio and z-score features exhibit parity}. Compositional ratios and z-score anomalies intermingle insimportance rankings (displacement\_z-score rank 7, weather\_ratio rank 8, food\_security\_z-score rank 9). Neither feature type systematically dominates, validating the inclusion of both in the basic feature set.

\textbf{Insight 4: Compositional diversity matters}. The highest-ranking compositional feature is other\_ratio (uncategorised news), not conflict or food\_security. This suggests that crisis prediction benefits from capturing news \textit{beyond} crisis-specific categories, potentially reflecting general uncertainty, governance challenges, or cross-cutting events not labelled as crisis-related.

\subsection{SHAP Attribution Analysis}

SHAP (SHapley Additive exPlanations) provides a game-theoretic approach to feature attribution, quantifying each feature's marginal contribution to individual predictions \citep{lundberg2017unified}. Unlike gain-based importance (which measures split frequency), SHAP computes the expected change in prediction when including versus excluding each feature, averaged across all possible feature orderings.

\subsubsection{Mathematical Foundation}

For a prediction model $f$ and feature set $\mathcal{F}$, the SHAP value $\phi_j$ for feature $j$ is:

\begin{equation}
\phi_j = \sum_{S \subseteq \mathcal{F} \setminus \{j\}} \frac{|S|! (|\mathcal{F}| - |S| - 1)!}{|\mathcal{F}|!} [f(S \cup \{j\}) - f(S)]
\end{equation}

where $S$ represents all possible feature subsets excluding $j$. This quantifies feature $j$'s average marginal contribution across all coalition orderings, satisfying desirable properties: efficiency (attributions sum to prediction), symmetry (identical features receive equal attribution), and monotonicity (adding features never decreases attribution).

\subsubsection{Divergence from Gain-Based Importance}

SHAP analysis (detailed results in Chapter 4) reveals dramatic divergence from gain-based feature importance, fundamentally reordering feature rankings:

    \begin{itemize}
    \item \textbf{Location metadata overstatement}: The three location features (country\_data\_density, country\_baseline\_conflict, country\_baseline\_food\_security) account for 40.4\% of tree splits but only 2.6\% of SHAP attribution$\times$a 15.5$\times$ overstatement. These features enable stratification (frequent node splitting) but contribute minimally to marginal predictions.
    \item \textbf{Z-score features understatement}: Z-score anomalies (other\_z-score, conflict\_z-score, humanitarian\_z-score, governance\_z-score, economic\_z-score, displacement\_z-score) account for only 20.1\% of tree splits but 74.7\% of SHAP attribution. These features drive prediction variance despite lower split frequency.
    \item \textbf{HMM features elevated}: HMM features account for 13.0\% of tree splits but 21.9\% of SHAP attribution, confirming genuine predictive value beyond split frequency.
    \item \textbf{DMD features specialized for extreme events}: DMD features account for 1.5\% of both tree splits and SHAP attribution, reflecting their design for rare catastrophic crises (<3\% of observations). This consistency across measurement methods confirms rarity by design, demonstrating appropriate extreme event specialization---when DMD features activate, they achieve the largest mixed-effects coefficient among all 35 features (+352.38), dominating predictions for complex emergencies.
    \end{itemize}

\subsubsection{Interpretation: Complementary Perspectives on Feature Value}

The divergence between gain-based importance and SHAP attribution demonstrates that feature "importance" depends critically on measurement method and analytical purpose:

\textbf{Gain-based importance} identifies features useful for \textit{stratification}---partitioning observations into homogeneous subgroups. Location metadata (country context) excels at this task, enabling tree splits that separate high-risk from low-risk contexts. These features appear important because they are used frequently, but they contribute minimally to predicting \textit{individual} outcomes once stratification is established.

\textbf{SHAP attribution} identifies features driving \textit{prediction variance}---the marginal impact on individual predictions after accounting for all other features. Z-score anomalies and HMM dynamics excel at this task, capturing shock-driven events and regime transitions that differentiate crisis from non-crisis cases within the same country context.

For operational deployment prioritising early warning of \textit{shock-driven crises} (the mandate of Stage 2 cascade models), SHAP attribution provides the more relevant measure. The 249 key saves (Chapter 4) occur precisely because z-scores and HMM features detect temporal anomalies and regime shifts that location metadata cannot capture. Gain-based rankings remain valuable for understanding model structure, but SHAP rankings inform deployment priorities for dynamic early warning.

\subsection{Mixed-Effects Decomposition: Fixed vs Random Effects}

Mixed-effects models decompose feature effects into \textbf{fixed effects} (global averagesimpact) and \textbf{random effects} (country-specific deviations), quantifying geographic heterogeneity \citep{gelman2006data}.

\subsubsection{Fixed Effect Coefficients}

Fixed effect coefficients $boldsymbol{\beta}$ represent the average log-odds change in crisis probability per unit increase in feature $j$, averaged across all countries. For the pooled ratio model, coefficients are:

    \begin{itemize}
    \item weather\_ratio: +26.38 (strongest positive effect among ratio features in mixed-effects model)
    \item displacement\_ratio: +23.47
    \item food\_security\_ratio: +20.57
    \item conflict\_ratio: +18.70
    \item economic\_ratio: +17.89
    \item health\_ratio: +16.75
    \item governance\_ratio: +16.15
    \item other\_ratio: +15.29
    \item humanitarian\_ratio: +15.18
    \item Intercept: -17.54 (baseline log-odds when all ratios = 0)
    \end{itemize}

All compositional features exhibit positive coefficients, indicating that higher coverage in any category associates with increased crisis risk. Weather news shows the strongest effect (+26.38), followed by displacement (+23.47) and food security (+20.57). This ordering differs from XGBoost tree-based importance rankings, where location metadata dominated split frequency (40.4\%); mixed-effects models isolate within-country variation, removing country-level confounders. SHAP analysis confirms that z-score features (not location) drive marginal predictions (74.7\% attribution vs location's 2.6\%).

\subsubsection{Random Effect Variances and Country Heterogeneity}

Random intercepts $\alpha_g$ capture country-specific baseline crisis risk deviations. Countries with positive random intercepts have systematically higher crisis risk than the global average; negative intercepts indicate lower risk. For the pooled ratio model, country random intercepts range from:

    \begin{itemize}
    \item\textbf{Highest risk}: Somalia (+2.92), Zimbabwe (+2.55), Sudan (+2.39), Malawi (+0.87)
    \item\textbf{Moderate risk}: Nigeria (+0.48), Ethiopia (+0.30), Mozambique (+0.11), Mali (+0.06)
    \item\textbf{Lower risk}: Niger (-0.23), Kenya (-0.32), DRC (-0.50), Uganda (-3.88), Madagascar (-4.50)
    \end{itemize}

Somalia's random intercept of +2.92 indicates that, after controlling for news features, Somalia exhibits substantially higher baseline crisis probability than other countries. Madagascar's -4.50 indicates exceptionally low baseline risk. These random intercepts inform selective deployment: models trained on high-risk contexts (Somalia, Zimbabwe, Sudan) may not generalise to low-risk contexts (Madagascar, Uganda).

Random slopes (not reported here for brevity) quantify feature effect heterogeneity across countries. Large random slope variances $\sigma_{b_j}^2$ indicate that feature $j$'s effect varies substantially by country, suggesting context-dependent deployment strategies.

\subsection{Ablation Studies: Marginal Feature Group Contributions}

Ablation studies systematically remove feature groups to isolate each method's unique contributions to crisis prediction. Table~\ref{tab:ablation_summary} summarizes cross-validated AUC-ROC for eight ablation models:

    \begin{table}[htbp]
    \centering
\small
    \begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Ratio} & \textbf{Z-score} & \textbf{HMM} & \textbf{DMD} & \textbf{Loc} & \textbf{AUC-ROC} \\
\midrule
Ratio Only & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & 0.727 \\
Z-score Only & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & 0.699 \\
Basic (Ratio+Z-score) & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & 0.696 \\
Basic + HMM & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & 0.703 \\
Basic + DMD & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & 0.698 \\
\textbf{Advanced (All)} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{0.697} \\
\bottomrule
    \end{tabular}
\caption{Ablation study results: AUC-ROC by feature group. All models include location metadata. Ratio-only achieves highest standalone AUC (0.727), but z-scores account for 74.7\% SHAP marginal attribution in combined models, demonstrating complementary roles.}
\label{tab:ablation_summary}
    \end{table}

\subsubsection{Key Ablation Findings}

\textbf{Finding 1: Ratio and z-score complementarity}. Ratio-only models achieve higher standalone AUC (0.727 vs 0.699), but SHAP analysis shows z-score features account for 74.7\% of marginal attribution in combined models. This demonstrates complementary roles: ratio features provide stable cross-sectional baselines for standalone performance, while z-score features capture volatile temporal anomalies driving marginal predictions when combined. Both types are essential---ratios for baseline discrimination, z-scores for shock detection.

\textbf{Finding 2: Ratio and z-score features provide complementary signals}. The basic model combining ratio and z-score (AUC 0.696) differs from ratio-only (AUC 0.727) by -0.031, reflecting that these feature types capture different dimensions: ratios measure \textit{compositional emphasis} (topic dominance), while z-scores measure \textit{temporal anomalies} (coverage spikes). GridSearchCV hyperparameter optimisation (max\_depth=5, min\_child\_weight=5) balances these complementary signals. Featuresimportance analysis reveals both contribute: ratio features dominate aggregate rankings, but individual z-score features (conflict\_z-score 4.2\%, food\_security\_z-score 3.7\%) provide orthogonal temporal signals valuable for sudden-onset crises.

\textbf{Finding 3: HMM captures regime transition dynamics}. Adding HMM to the basic modelsimproves AUC from 0.696 to 0.703 (+0.007), with \textbf{hmm\_ratio\_transition\_risk ranking \#5 in feature importance (0.032, equivalent to 3.2\%)}. This demonstrates that latent regime transitions (peaceful $\rightarrow$ crisis) capture qualitative narrative shifts invisible to compositional features, providing unique signal for detecting when news narratives fundamentally change in character before IPC deterioration occurs.

\textbf{Finding 4: DMD extracts temporal crisis evolution patterns}. Adding DMDsimproves AUC from 0.696 to 0.698 (+0.002). While DMD features rank 28th-36th in feature importance based on frequency of use, the mixed-effects model reveals that dmd\_ratio\_crisis\_instability achieves the \textit{largest coefficient among all features (+352.38)}, demonstrating that DMD captures rare but extreme events---the most severe humanitarian catastrophes where multiple crisis drivers converge simultaneously. This pattern aligns with DMD's design: extracting temporal modes from 48-month sequences to identify crisis escalation dynamics.

\textbf{Finding 5: Advanced model provides comprehensive interpretability}. The advanced model (AUC 0.697, 35 features) combines all feature engineering approaches (ratio, z-score, HMM, DMD, location) to provide multi-faceted crisis understanding. While ratio-only achieves higher AUC (0.727, 12 features), the advanced model's value lies in \textit{interpretability and mechanistic insight}: HMM transition risk ranks \#5, DMD crisis instability achieves the largest mixed-effects coefficient (+352.38), and z-score features provide temporal anomaly signals complementing compositional ratios. This comprehensive feature set justifies the advanced model's selection for cascade integration, where understanding \textit{why} predictions occur matters as much as predictive accuracy.

\subsection{Triangulation Across Interpretability Methods}

Convergent evidence across three complementary interpretability methods (XGBoost, SHAP, mixed-effects) and validation through ablation studies establishes robust feature rankings:

\textbf{Location metadata consistently dominates tree splits, not marginal predictions}:
    \begin{itemize}
    \item XGBoost tree-based: Top 3 features (29.3\% cumulative split frequency, 40.4\% total)
    \item XGBoost SHAP: Ranks 17, 20, 26 (only 2.6\% marginal attribution)$\times$15.5$\times$ overstatement
    \item Interpretation: Location enables stratification (frequent node splitting) but contributes minimally to prediction variance
    \item Ablation: All models include location features as baseline for stratification
    \item Mixed-Effects: Random intercepts capture country-level variation
    \end{itemize}

\textbf{HMM features provide genuine signal}:
    \begin{itemize}
    \item XGBoost: Transition risk ranks 5th (3.2\%simportance)
    \item Ablation: HMM adds +0.007 AUC
    \item Mixed-Effects: HMM features show positive fixed effects in advanced models
    \end{itemize}

\textbf{Ratio features achieve higher standalone AUC than z-scores}:
    \begin{itemize}
    \item XGBoost: Ratio and z-score intermingle, no systematic dominance
    \item Ablation: Ratio-only (0.727) vs z-score-only (0.699) on AR-filtered cases
    \item Mixed-Effects: Ratio model (not reported) achieves similar AUC to z-score model
    \end{itemize}

\textbf{DMD features provide specialized value for extreme events}:
    \begin{itemize}
    \item XGBoost: DMD features rank 28th-36th, reflecting their design for rare extreme events (<3\% of observations)
    \item Ablation: DMD adds +0.002 AUC, reflecting its extreme event specialization rather than universal discrimination
    \item Mixed-Effects: dmd\_ratio\_crisis\_instability achieves \textit{largest coefficient among all features (+352.38)}---13.2$\times$ larger than the next highest feature, demonstrating that when DMD activates (complex emergencies with synchronized multicategory crises), it dominates predictions
    \end{itemize}

This triangulation validates the cascade's use of the advanced XGBoost model (35 features), which integrates all feature engineering approaches (ratio, z-score, HMM, DMD, location) for comprehensive interpretability. While simpler models (ratio-only, 12 features) achieve higher raw AUC (0.727 vs 0.697), the advanced model's value lies in \textbf{multi-faceted crisis understanding}: HMM transition risk ranks \#5, DMD achieves largest coefficient, and z-scores complement ratios---enabling post-hoc analysis of \textit{why} predictions succeed or fail through multiple interpretability lenses.

\vspace{0.3cm}

\noindent\textit{This section established a triangulation framework for model interpretability combining three complementary methods (XGBoost gain-based importance, SHAP attribution, mixed-effects decomposition) with ablation studies for model evaluation. XGBoost gain-based importance identifies location metadata (country\_data\_density, country\_baseline\_conflict, country\_baseline\_food\_security) as the top 3 features (29.3\% cumulative importance), providing essential geographic context. SHAP analysis reveals dramatic divergence: location features account for 40.4\% of tree splits but only 2.6\% of marginal attribution (15.5$\times$ overstatement), while z-score features drive 74.7\% of prediction variance despite lower split frequency. hmm\_ratio\_transition\_risk ranks 5th in gain-based importance (3.2\%) and elevated in SHAP (21.9\%), validating that regime dynamics contribute unique signal for detecting qualitative narrative shifts. Mixed-effects decomposition reveals that weather\_ratio (+26.71), displacement\_ratio (+21.18), and food\_security\_ratio (+20.33) exhibit the strongest fixed effects, while random intercepts quantify country-level heterogeneity (Somalia +3.70 highest risk, Madagascar -4.56 lowest risk), and dmd\_ratio\_crisis\_instability achieves the largest coefficient (+352.38), demonstrating value for detecting rare extreme events. Ablation studies demonstrate that ratio features (AUC 0.727) and z-score features (0.699) provide complementary signals---ratios capture compositional emphasis, z-scores capture temporal anomalies. HMM adds +0.007 AUC with \#5 feature ranking, DMD adds +0.002 AUC with largest coefficient, and the advanced model (0.697) integrates all approaches for comprehensive interpretability. Triangulation across three interpretability methods and validation through ablation confirms: (1) location metadata provides stratification utility but minimal marginal impact, (2) z-score features drive prediction variance (SHAP), (3) HMM provides unique regime transition signal, (4) ratio and z-score features are complementary, (5) DMD targets extreme events. These findings inform cascade deployment: the advanced XGBoost model provides interpretable multi-method feature decomposition, justifying its selection for understanding crisis dynamics.}
\vspace{0.3cm}




